diff --git a/DARMHand-v0_test.py b/DARMHand-v0_test.py
deleted file mode 100644
index 97977ac..0000000
--- a/DARMHand-v0_test.py
+++ /dev/null
@@ -1,29 +0,0 @@
-import gym
-from darm_gym_env import DARMEnv
-import time
-import random
-
-env = gym.make("darm/DarmHand-v0", render_mode="human", hand_name="hand1")
-
-done = False
-obs = env.reset()
-
-start = time.time()
-while not done:
-    ac = env.action_space.sample()
-    if random.random() > 0.5: 
-        ac[0:4] = [0.0, 0.0, 10.0, 10.0]
-    obs, rew, done, info = env.step(ac)
-    env.render()
-
-    if done:
-        env.reset()
-        done = False
-
-        print(f"Duration: {time.time() - start}")
-        start = time.time()
-        print(info)
-    # TODO: Address links going beyond their limits
-    # TODO: Add trucation signal from closing window
-
-env.close()
\ No newline at end of file
diff --git a/DARMSFHand-v0_test.py b/DARMSFHand-v0_test.py
deleted file mode 100644
index a41c01a..0000000
--- a/DARMSFHand-v0_test.py
+++ /dev/null
@@ -1,32 +0,0 @@
-import gym
-from darm_gym_env import DARMSFEnv
-import time
-
-env = gym.make("darm/DarmSFHand-v0", render_mode="human", hand_name="hand1")
-
-done = False
-obs = env.reset()
-
-start = time.time()
-episode_return = 0
-while not done:
-    ac = env.action_space.sample()
-    obs, rew, done, info = env.step(ac)
-    # obs is in m and m/s. It can be scaled before passing into model in cm and cm/s
-    episode_return += rew
-    env.render()
-
-    if done:
-        print(f"Duration: {time.time() - start}")
-        print(info)
-        print(f"Return: {episode_return}")
-        episode_return = 0
-        start = time.time()
-
-        obs = env.reset()
-        done = False
-
-    # TODO: Address links going beyond their limits
-    # TODO: Add trucation signal from closing window
-
-env.close()
\ No newline at end of file
diff --git a/build/lib/darm_gym_env/darm_gym.py b/build/lib/darm_gym_env/darm_gym.py
index 13994e6..69561d5 100644
--- a/build/lib/darm_gym_env/darm_gym.py
+++ b/build/lib/darm_gym_env/darm_gym.py
@@ -1,5 +1,6 @@
 import os
 import numpy as np
+import collections
 
 import gym
 import mujoco as mj
@@ -8,54 +9,109 @@ from mujoco.glfw import glfw
 from pathlib import Path
 
 
-TARGETS_FILE = Path(__file__).parent.parent/"darm_targets.npy REF_POSE TODO"
+DARM_XML_FILE = f"{os.getenv('DARM_MUJOCO_PATH')}/mujoco_env/darm.xml"
+SF_START_STATE_FILE = f"{os.getenv('DARM_MUJOCO_PATH')}/darm_gym_env/DARMHand_SF_start_state.npy"
+MF_START_STATE_FILE = f"{os.getenv('DARM_MUJOCO_PATH')}/darm_gym_env/DARMHand_MFNW_start_state.npy"
 
 class DARMEnv(gym.Env):
     metadata = {"render_modes": ["human"], "render_fps": 1}
 
-    def __init__(self, render_mode=None, reaction_time=0.08, hand_name="hand1") -> None:
+    def __init__(self, render_mode=None, action_time=0.08, hand_name="hand1",
+                min_th = 0.004,
+                min_target_th = 2*0.004,
+                max_target_th = 5*0.004, # 20 mm
+                target_joint_state_delta = [],
+                min_joint_vals = [],
+                max_joint_vals = [],
+                max_tendon_tension = [],
+                single_finger_env = False,
+                ignore_load_start_states = False
+                ) -> None:
         super().__init__()
-
-        # Env Parameters
         assert render_mode is None or render_mode in self.metadata["render_modes"]
+        
+
+        # ========================== Env Parameters ==========================
         self.render_mode = render_mode
         self.hand_name = hand_name
-        self.reaction_time = reaction_time
+        self.single_finger_env = single_finger_env
+        self.action_time = action_time
         self.ep_start_time = 0  # episode start time
 
-        # Load the Model
-        self._load_model("../mujoco_env/darm.xml")
+
+        # ========================== Load the Model ==========================
+        self._load_model()
         if not (self.model and self.data):
             raise "Error loading model"
         self._get_fingertip_indices()
 
-        # Load targets
-        with open(TARGETS_FILE, 'rb') as f:
-            # np.array([np.random.random((15)) for _ in range(5)])
-            self.targets = np.load(f)
-        self.targets_len = len(self.targets)
 
-        # Define Observation Space
+        # ========================== Load targets ==========================
+        if not ignore_load_start_states:
+            self._load_start_states()
+
+
+        # ========================== Mujoco Model Simulation Parameters ==========================
+        self.min_joint_vals = min_joint_vals or self._get_joint_limits("min")   # degrees
+        self.max_joint_vals = max_joint_vals or self._get_joint_limits("max")   # degress
+        # abs increament of joint state from starting state to target state
+        self.target_joint_state_delta = target_joint_state_delta or self._compute_target_joint_state_delta()   # degrees
+        self.max_tendon_tension = max_tendon_tension or self._get_actuator_ctrlrange("max")
+
+        self.min_joint_vals = self.min_joint_vals*(np.pi/180)
+        self.max_joint_vals = self.max_joint_vals*(np.pi/180)
+        self.target_joint_state_delta = self.target_joint_state_delta*(np.pi/180)
+
+        self.min_th = min_th    # norm threshold in metres at which env is solved
+        self.min_target_th = min_target_th  # min norm to target state
+        self.max_target_th = max_target_th  # max norm to target state
+
+        # Initialize target observation
+        self.target_obs = np.zeros(3*len(self.fingertip_indices))
+
+
+        # ========================== Reward Components Weights ==========================
+        self.rwd_keys_wt = dict(
+            reach = 1.0,
+            bonus = 4.0,
+            penalty = 50,
+            act_reg = 0.1,
+            # sparse = 1,
+            # solved = 1, # review - weight should not be assigned to this?
+            # done = 1 # review - weight should not be assigned to this?
+        )
+
+
+        # ========================== Get Ref Position ==========================
+        # Reference Position is at the centre of the wrist
+        # The ref pos will remain fixed since it was taken before simulation started
+        mj.mj_forward(self.model, self.data)
+        ref_body_idx = mj.mj_name2id(self.model, int(mj.mjtObj.mjOBJ_BODY), f"{self.hand_name}_rc_centre_block")
+        self.ref_pos = np.array(self.data.xpos[ref_body_idx])
+
+
+        # ========================== Define Observation and Action Space ==========================
         self.observation_space = gym.spaces.Box(low=-np.inf, high=np.inf, 
-                                                shape=(2*3*len(self.fingertip_indices),), 
-                                                dtype=float)
+                                                shape=(3*3*len(self.fingertip_indices),), 
+                                                dtype=np.float32)
         
-        # Define Action Space
+
         # NOTE: Watch out for Box upper limit if Carpal Actuators are involved
-        self.action_space = gym.spaces.Box(low=np.array([0.0]*self.model.nu), 
-                                            high=np.array([10.0]*4 + [2.0]*(self.model.nu-4)), 
-                                            shape=(self.model.nu,), dtype=float)
+        # FIXME: Fix action range in reward and step functions. Current ==> [0,2] after denorm
+        # Define a mujoco action range array used for scaling
+        self.action_space = gym.spaces.Box(low=np.array([-1.0]*self.model.nu), 
+                                            high=np.array([1.0]*self.model.nu), 
+                                            shape=(self.model.nu,), dtype=np.float32)
+
 
-        # For Human Rendering
+        # ========================== For Human Rendering ==========================
         self.window = None
         self.window_size = 1200, 900
 
-    def _load_model(self, xml_path):
-        dirname = os.path.dirname(__file__)
-        abspath = os.path.join(dirname + "/" + xml_path)
-        xml_path = abspath
-
+    def _load_model(self):
+        xml_path = DARM_XML_FILE
         self.model = mj.MjModel.from_xml_path(xml_path)
+
         if self.model: 
             print("Loaded XML file successfully") 
         else:
@@ -64,6 +120,44 @@ class DARMEnv(gym.Env):
         
         self.data = mj.MjData(self.model)
 
+    def _load_start_states(self):
+        if self.single_finger_env:
+            filename = SF_START_STATE_FILE
+        else:
+            filename = MF_START_STATE_FILE
+
+        with open(filename, 'rb') as f:
+            self.start_states = np.load(f, allow_pickle=True)
+            self.start_states_len = len(self.start_states)
+
+    def _get_joint_limits(self, type = None):
+        joint_limits = []
+        for i in range(self.model.njnt):
+            joint_limits.append(self.model.jnt_range[i]*(180/np.pi))
+        
+        joint_limits = np.asarray(joint_limits)
+        
+        if type == "min":
+            return joint_limits[:, 0]
+        if type == "max":
+            return joint_limits[:, 1]
+        
+        return joint_limits[:, 0], joint_limits[:, 1]
+
+    def _compute_target_joint_state_delta(self):
+        # return (self.max_joint_vals - self.min_joint_vals)//10  # for every range of 10 deg, have a delta of 1 deg
+        joint_state_delta =  ((self.max_joint_vals - self.min_joint_vals)//40) * 2  # for every range of 40 deg, have a delta of 2 deg
+        return np.clip(joint_state_delta, a_min=2, a_max=10)    # a minimum delta of 2 degrees, max of 10 degrees
+
+    def _get_actuator_ctrlrange(self, type = None):
+        if type == "min":
+            return np.array([self.model.actuator_ctrlrange[i][0] for i in range(self.model.nu)])
+        if type == "max":
+            return np.array([self.model.actuator_ctrlrange[i][1] for i in range(self.model.nu)])
+    
+        ctrl_range = np.array([self.model.actuator_ctrlrange[i] for i in range(self.model.nu)])
+        return ctrl_range[:, 0], ctrl_range[:, 1] # (min, max)
+
     def _init_controller(self):
         pass
 
@@ -72,12 +166,25 @@ class DARMEnv(gym.Env):
 
     def _get_fingertip_indices(self):
         # NOTE: Remember to set the mocap index properly in reset()
-        indices = ["i", "ii", "iii", "iv", "v"]
+        if self.single_finger_env:
+            indices = ["ii"]
+        else:
+            indices = ["i", "ii", "iii", "iv", "v"]
+
         self.fingertip_indices = [mj.mj_name2id(self.model, int(mj.mjtObj.mjOBJ_SITE), f"{self.hand_name}_fingertip_{i}") for i in indices]
     
-    def _get_obs(self):
-        return np.concatenate((np.array([self.data.site_xpos[i] for i in self.fingertip_indices]).flatten(),
-                             self.target_obs))
+    def _get_obs(self, prev_obs, new_obs, action_time=None):
+        if not action_time:
+            # if no action time velocity is zero. i.e. after reset
+            vel_obs = np.zeros((3*len(self.fingertip_indices),))
+        elif action_time: 
+            prev_fingertip_pos = prev_obs[:3*len(self.fingertip_indices)]
+            new_fingertip_pos = new_obs[:3*len(self.fingertip_indices)]
+            vel_obs = (new_fingertip_pos - prev_fingertip_pos)/action_time
+
+        return np.concatenate((np.array([(np.array(self.data.site_xpos[i]) - self.ref_pos) for i in self.fingertip_indices]).flatten(),
+                             self.target_obs,
+                             vel_obs))
 
     def _get_info(self):
         return {"sim_time": self.data.time - self.ep_start_time}
@@ -85,95 +192,168 @@ class DARMEnv(gym.Env):
     def _norm_to_target(self, obs):
         """
         Returns the norm of each fingertip to the target position
-        obs: an observation from the observation space [...fingertip_pos, ...target_pos]
+        obs: an observation from the observation space [...fingertip_pos, ...target_pos, ...fingertip_vel]
         """
         obs = obs.reshape((-1, 3))
-        n_fingertips = len(obs)//2
+        n_fingertips = len(self.fingertip_indices)
 
         fingertip_poses = obs[0:n_fingertips]
-        target_poses = obs[n_fingertips:]
+        target_poses = obs[n_fingertips:2*n_fingertips]
 
         return np.linalg.norm(fingertip_poses-target_poses, ord=2, axis=-1)
 
-    def _get_reward(self, state, action, new_state, time_delta):
+    def _get_reward(self, action, new_state):
         """
-        Reward function to compute reward given state, action, and new state.
-        R = R(S, a, S')
-
-        If norm to target reduces: -1 else (-1 + x) where x is a neg. number 
-                proportional to number of fingers with increased norms
-        Punish high velocity according to the eqution: -0.3 + 0.3*np.exp(-1*vel)
-        Punish high torque according to the equation: -0.2 + 0.2*np.exp(-1*action)
-        Reward reaching target with a tolerance of 4mm: 250
+        Reward function to compute reward given action, and new state.
+        R = R(a, S')
+
+        Agent is punished for being far from target
+        Agent is punished for going farther than a threshold from the target
+        Agent is punished for high action magnitude
+        Agent is rewarded for being close to the target
+        Agent is rewarded for coming close to the target beyond a threshold 
         """
 
-        # Change in Norm to Target
-        prev_norm = self._norm_to_target(state)
-        new_norm = self._norm_to_target(new_state)
-        norm_reward = -1 + sum(new_norm > prev_norm)*(-1/len(self.fingertip_indices))
+        reach_dist = self._norm_to_target(new_state)    # NOTE: Single finger
+        near_th = self.min_th
+        far_th = 2*self.max_target_th
 
-        # Velocity Correction
-        previous_pos = state[:len(state)//2].reshape((-1,3))
-        new_pos = new_state[:len(new_state)//2].reshape((-1,3))
-        vel = np.linalg.norm(new_pos-previous_pos, ord=2, axis=-1) / time_delta
-        vel_reward = (-0.3 + 0.3*np.exp(-1*vel)).mean() # scale vel term in exp beween [-1,-5]
+        # Scale action down to [0, 1] from [0, max_tendon_tension]
+        action = action / self.max_tendon_tension
         
-        # Effort Correction
-        action_reward = (-0.2 + 0.2*np.exp(-1*action)).mean()
-
-        # Reach Target Reward
-        target_reward = 250 if self._get_done(new_state) else 0
-
-        return (norm_reward + vel_reward + action_reward + target_reward)
+        # NOTE: Some of the fingers in five fingered hand have more than five actuators
+        # act_mag = np.linalg.norm(action.reshape(-1, 5)) # reshape action to (-1,5), ensure nu is ordered from mujoco
+        # TODO: Consider scaling down this act_mag to be equiv. to a single finger with nu=5
+        act_mag = np.linalg.norm(action)/np.sqrt(self.model.nu/1) # action magnitude is not measured per finger but as a whole
+        # by dividing by sqrt(nu/5), the norm is similar to when computing with nu==5. Check it out.
+        # by dividing by sqrt(nu) act_mag will have a max value in the order of the max_value of action now => 1
+        
+        rwd_dict = collections.OrderedDict((
+            # Optional Keys
+            ('reach',   -1.*reach_dist),
+            ('bonus',   1.*(reach_dist<2*near_th) + 1.*(reach_dist<near_th)),
+            ('act_reg', -1.*act_mag),
+            ('penalty', -1.*(reach_dist>far_th)),
+            # Must keys
+            ('sparse',  -1.*reach_dist),
+            ('solved',  reach_dist<near_th),
+            ('done',    reach_dist > far_th),
+        ))
+
+            # Weights:
+            # reach = 1.0,
+            # bonus = 4.0,
+            # penalty = 50,
+            # act_reg = 0.1,
+        rwd_dict['dense'] = np.sum([wt*rwd_dict[key] for key, wt in self.rwd_keys_wt.items()], axis=0)
+        return rwd_dict
 
     def _get_done(self, new_state):
-        return all(self._norm_to_target(new_state) < 0.004)
+        return all(self._norm_to_target(new_state) < self.min_th)
+
+    def _check_collision(self):
+        """Returns True if there is collision, otherwise False"""
+        return len(self.data.contact.geom1) > 0
+
+    def generate_start_state(self):
+        while True:
+            # ========================== Sample valid start_state from Joint Space ==========================
+            joint_state = np.random.uniform(low=self.min_joint_vals, high=self.max_joint_vals)
+            self.forward(joint_state)
+            if self._check_collision(): # returns True if there is collision
+                # ensure there is no collision at the start state
+                continue
+        
+            # ========================== Create a valid target ==========================
+            joint_state_delta = self.target_joint_state_delta*np.random.choice(a=[-1,1], size=(self.model.njnt,), replace=True)
+            target_joint_state = np.clip(a=joint_state + joint_state_delta, 
+                                        a_min=self.min_joint_vals, 
+                                        a_max=self.max_joint_vals)
+            self.target_obs = self.forward(target_joint_state)[:3*len(self.fingertip_indices)]
+            if self._check_collision(): # returns True if there is collision
+                # ensure there is no collision at the target state
+                continue
+            
+            # Return to start state
+            observation = self.forward(joint_state)
+
+            # Verify distance of start state to target state is within limits
+            norm = self._norm_to_target(observation)
+            if not (all(norm >= self.min_target_th) and all(norm <= self.max_target_th)):
+                continue
+            
+            # If all checks are positive, break random search loop
+            return observation, joint_state, self.target_obs      
+
+    def sample_saved_start_states(self):
+        # Sample a start state from saved start states
+        # start_state = [joint_state, target_obs]
+        start_state = self.start_states[np.random.randint(self.start_states_len)]
+
+        # Set Target Obs
+        self.target_obs = start_state[1]
+
+        # Go forward to start state
+        observation = self.forward(start_state[0])
+
+        # Return Observation
+        return observation
 
     # def reset(self, seed=None, options=None):
         # super().reset()
     def reset(self, **kwargs):
-        # Reset Model. TODO: Consider not reseting the model, let next goal run from the old state
-        # mj.mj_resetData(self.model, self.data)
-        # mj.mj_forward(self.model, self.data)
-        
-        # Create a new goal
-        self.target_obs = self.targets[np.random.randint(0, self.targets_len)]
-        if self.render_mode == "human":
-            # BUG: Set the finger index properly here [ii, i, iii, iv, v]
-            self.data.mocap_pos = self.target_obs.reshape(len(self.fingertip_indices),3)
-        mj.mj_forward(self.model, self.data)    # TODO: See possibility of turning thumb here
-        self.ep_start_time = self.data.time
-
-
-        observation = self._get_obs()
-        # info = self._get_info()
+        # ========================== Get a random valid pose and target ==========================
+        # observation, _, _ = self.generate_start_state()
+        observation = self.sample_saved_start_states()
 
+        # ========================== Render Frame ==========================
         if self.render_mode == "human":
+            # Update target visualization mocaps pos
+            self.data.mocap_pos = self.target_obs.reshape(len(self.fingertip_indices),3) + self.ref_pos
             self._render_frame()
 
-        return observation #, info
+        self.ep_start_time = self.data.time
+        return observation
 
     def step(self, action):
-        prev_obs = self._get_obs()
+        prev_obs = self._get_obs(prev_obs=None, new_obs=None, action_time=None)
+
+        # action from model is in the range [-1,1]
+        # action + 1 === [0, 2]
+        # action * x === [0, 2x]
+        action = (action + 1)*(self.max_tendon_tension/2)
+        action = np.clip(action, 0, self.max_tendon_tension)
         self.data.ctrl[0 : self.model.nu] = action
         time_prev = self.data.time   # simulation time in seconds
 
-        # Perform action    
-        while (self.data.time - time_prev < self.reaction_time):
+        # Perform action  
+        while (self.data.time - time_prev < self.action_time):
             mj.mj_step(self.model, self.data)
         time_after = self.data.time # time after performing action
 
-        # Get new observation
-        new_obs = self._get_obs()
 
-        # Get Reward
-        reward = self._get_reward(prev_obs, action, new_obs, time_after-time_prev)
+        # Get new observation (fingertips_pos)
+        new_obs = self._get_obs(prev_obs=None, new_obs=None, action_time=None)
+        # include velocity in new obs
+        new_obs = self._get_obs(prev_obs=prev_obs,
+                                new_obs=new_obs, 
+                                action_time=time_after-time_prev)
 
         if self.render_mode == "human":
             self._render_frame()
 
-        return new_obs, reward, self._get_done(new_obs), self._get_info()   # False, self._get_info()
+        # Get Reward
+        rwd_dict = self._get_reward(action, new_obs)
+        reward = rwd_dict["dense"].mean()
+        done = any(rwd_dict["done"])  # any(rwd_dict["done"])
         
+        return new_obs, reward, done, {**self._get_info(), "action": action, "reward": {**rwd_dict}}
+
+    def forward(self, joint_conf):
+        self.data.qpos = joint_conf
+        mj.mj_forward(self.model, self.data)
+        return self._get_obs(prev_obs=None, new_obs=None, action_time=None)
+
     def render(self, mode, **kwargs):
         if self.render_mode == "human":
             self._render_frame()
diff --git a/build/lib/darm_gym_env/darm_sf_gym.py b/build/lib/darm_gym_env/darm_sf_gym.py
index a4d8975..7c812a4 100644
--- a/build/lib/darm_gym_env/darm_sf_gym.py
+++ b/build/lib/darm_gym_env/darm_sf_gym.py
@@ -9,7 +9,7 @@ from mujoco.glfw import glfw
 from pathlib import Path
 
 
-TARGETS_FILE = f"{os.getenv('DARM_MUJOCO_PATH')}/darm_sf_joint_space_targets.npy"
+# TARGETS_FILE = f"{os.getenv('DARM_MUJOCO_PATH')}/darm_sf_joint_space_targets.npy"
 DARM_XML_FILE = f"{os.getenv('DARM_MUJOCO_PATH')}/mujoco_env/darm.xml"
 
 class DARMSFEnv(gym.Env):
@@ -18,8 +18,8 @@ class DARMSFEnv(gym.Env):
     def __init__(self, render_mode=None, reaction_time=0.08, hand_name="hand1",
                     target_joint_state_delta = [4, 8, 8, 8],
                     min_th = 0.004,
-                    min_target_th = 0.008,
-                    max_target_th = 0.04,
+                    min_target_th = 2*0.004,
+                    max_target_th = 10*0.004,
                     min_joint_vals = [-20, -45, -10, -10],
                     max_joint_vals = [20, 90, 90, 90]) -> None:
         super().__init__()
@@ -45,13 +45,13 @@ class DARMSFEnv(gym.Env):
             bonus = 4.0,
             penalty = 50,
             act_reg = 0.1,
-            sparse = 1,
-            solved = 1,
-            done = 1
+            # sparse = 1,
+            # solved = 1, # review - weight should not be assigned to this?
+            # done = 1 # review - weight should not be assigned to this?
         )
 
         # Load the Model
-        self._load_model("../mujoco_env/darm.xml")
+        self._load_model()
         if not (self.model and self.data):
             raise "Error loading model"
         self._get_fingertip_indices()
@@ -64,10 +64,10 @@ class DARMSFEnv(gym.Env):
         self.ref_pos = np.array(self.data.xpos[ref_body_idx])
 
         # Load targets
-        with open(TARGETS_FILE, 'rb') as f:
-            # np.array([np.random.random((15)) for _ in range(5)])
-            self.targets = np.load(f)
-        self.targets_len = len(self.targets)
+        # with open(TARGETS_FILE, 'rb') as f:
+        #     # np.array([np.random.random((15)) for _ in range(5)])
+        #     self.targets = np.load(f)
+        # self.targets_len = len(self.targets)
 
         # Define Observation Space
         self.observation_space = gym.spaces.Box(low=-np.inf, high=np.inf, 
@@ -86,7 +86,7 @@ class DARMSFEnv(gym.Env):
         self.window = None
         self.window_size = 1200, 900
 
-    def _load_model(self, xml_path):
+    def _load_model(self):
         xml_path = DARM_XML_FILE
         self.model = mj.MjModel.from_xml_path(xml_path)
 
@@ -111,9 +111,12 @@ class DARMSFEnv(gym.Env):
     def _get_obs(self, prev_obs, new_obs, action_time=None):
         if not action_time:
             # if no action time velocity is zero. i.e. after reset
-            vel_obs = np.zeros((3,))
+            vel_obs = np.zeros((3*len(self.fingertip_indices),))
         elif action_time: 
-            vel_obs = (prev_obs[:3] - new_obs[:3])/action_time
+            prev_fingertip_pos = prev_obs[:3*len(self.fingertip_indices)]
+            new_fingertip_pos = new_obs[:3*len(self.fingertip_indices)]
+            # BUG: FIXED: (prev - new) to (new - prev)
+            vel_obs = (new_fingertip_pos - prev_fingertip_pos)/action_time
 
         return np.concatenate((np.array([(np.array(self.data.site_xpos[i]) - self.ref_pos) for i in self.fingertip_indices]).flatten(),
                              self.target_obs,
@@ -125,7 +128,7 @@ class DARMSFEnv(gym.Env):
     def _norm_to_target(self, obs):
         """
         Returns the norm of each fingertip to the target position
-        obs: an observation from the observation space [...fingertip_pos, ...target_pos]
+        obs: an observation from the observation space [...fingertip_pos, ...target_pos, ...fingertip_vel]
         """
         obs = obs.reshape((-1, 3))
         n_fingertips = len(self.fingertip_indices)
diff --git a/darm_gym_env.egg-info/SOURCES.txt b/darm_gym_env.egg-info/SOURCES.txt
index 35183e3..a0b6525 100644
--- a/darm_gym_env.egg-info/SOURCES.txt
+++ b/darm_gym_env.egg-info/SOURCES.txt
@@ -1,9 +1,13 @@
 README.md
 setup.py
+darm_gym_env/DARMHand-v0_test.py
 darm_gym_env/__init__.py
 darm_gym_env/darm_gym.py
 darm_gym_env/darm_sf_gym.py
-darm_gym_env/multi_darm_gym.py
+darm_gym_env/generate_DARMHand_start_state.py
+darm_gym_env/obs_DARMSFHand-v0_test.py
+darm_gym_env/obs_DARMSFHand_targets_gen.py
+darm_gym_env/obs_multi_darm_gym.py
 darm_gym_env.egg-info/PKG-INFO
 darm_gym_env.egg-info/SOURCES.txt
 darm_gym_env.egg-info/dependency_links.txt
diff --git a/darm_gym_env/__pycache__/darm_gym.cpython-38.pyc b/darm_gym_env/__pycache__/darm_gym.cpython-38.pyc
index 8f03551..e38250d 100644
Binary files a/darm_gym_env/__pycache__/darm_gym.cpython-38.pyc and b/darm_gym_env/__pycache__/darm_gym.cpython-38.pyc differ
diff --git a/darm_gym_env/__pycache__/darm_sf_gym.cpython-38.pyc b/darm_gym_env/__pycache__/darm_sf_gym.cpython-38.pyc
index d78d38e..f36d85e 100644
Binary files a/darm_gym_env/__pycache__/darm_sf_gym.cpython-38.pyc and b/darm_gym_env/__pycache__/darm_sf_gym.cpython-38.pyc differ
diff --git a/darm_gym_env/darm_gym.py b/darm_gym_env/darm_gym.py
index 13994e6..e78d5ae 100644
--- a/darm_gym_env/darm_gym.py
+++ b/darm_gym_env/darm_gym.py
@@ -1,5 +1,6 @@
 import os
 import numpy as np
+import collections
 
 import gym
 import mujoco as mj
@@ -8,54 +9,109 @@ from mujoco.glfw import glfw
 from pathlib import Path
 
 
-TARGETS_FILE = Path(__file__).parent.parent/"darm_targets.npy REF_POSE TODO"
+DARM_XML_FILE = f"{os.getenv('DARM_MUJOCO_PATH')}/mujoco_env/darm.xml"
+SF_START_STATE_FILE = f"{os.getenv('DARM_MUJOCO_PATH')}/darm_gym_env/DARMHand_SF_start_state.npy"
+MF_START_STATE_FILE = f"{os.getenv('DARM_MUJOCO_PATH')}/darm_gym_env/DARMHand_MFNW_start_state.npy"
 
 class DARMEnv(gym.Env):
     metadata = {"render_modes": ["human"], "render_fps": 1}
 
-    def __init__(self, render_mode=None, reaction_time=0.08, hand_name="hand1") -> None:
+    def __init__(self, render_mode=None, action_time=0.08, hand_name="hand1",
+                min_th = 0.004,
+                min_target_th = 2*0.004,
+                max_target_th = 5*0.004, # 20 mm
+                target_joint_state_delta = [],
+                min_joint_vals = [],
+                max_joint_vals = [],
+                max_tendon_tension = [],
+                single_finger_env = False,
+                ignore_load_start_states = False
+                ) -> None:
         super().__init__()
-
-        # Env Parameters
         assert render_mode is None or render_mode in self.metadata["render_modes"]
+        
+
+        # ========================== Env Parameters ==========================
         self.render_mode = render_mode
         self.hand_name = hand_name
-        self.reaction_time = reaction_time
+        self.single_finger_env = single_finger_env
+        self.action_time = action_time
         self.ep_start_time = 0  # episode start time
 
-        # Load the Model
-        self._load_model("../mujoco_env/darm.xml")
+
+        # ========================== Load the Model ==========================
+        self._load_model()
         if not (self.model and self.data):
             raise "Error loading model"
         self._get_fingertip_indices()
 
-        # Load targets
-        with open(TARGETS_FILE, 'rb') as f:
-            # np.array([np.random.random((15)) for _ in range(5)])
-            self.targets = np.load(f)
-        self.targets_len = len(self.targets)
 
-        # Define Observation Space
+        # ========================== Load targets ==========================
+        if not ignore_load_start_states:
+            self._load_start_states()
+
+
+        # ========================== Mujoco Model Simulation Parameters ==========================
+        self.min_joint_vals = min_joint_vals or self._get_joint_limits("min")   # degrees
+        self.max_joint_vals = max_joint_vals or self._get_joint_limits("max")   # degress
+        # abs increament of joint state from starting state to target state
+        self.target_joint_state_delta = target_joint_state_delta or self._compute_target_joint_state_delta()   # degrees
+        self.max_tendon_tension = max_tendon_tension or self._get_actuator_ctrlrange("max")
+
+        self.min_joint_vals = self.min_joint_vals*(np.pi/180)
+        self.max_joint_vals = self.max_joint_vals*(np.pi/180)
+        self.target_joint_state_delta = self.target_joint_state_delta*(np.pi/180)
+
+        self.min_th = min_th    # norm threshold in metres at which env is solved
+        self.min_target_th = min_target_th  # min norm to target state
+        self.max_target_th = max_target_th  # max norm to target state
+
+        # Initialize target observation
+        self.target_obs = np.zeros(3*len(self.fingertip_indices))
+
+
+        # ========================== Reward Components Weights ==========================
+        self.rwd_keys_wt = dict(
+            reach = 1.0,
+            bonus = 4.0,
+            penalty = 50,
+            act_reg = 0.1,
+            # sparse = 1,
+            # solved = 1, # review - weight should not be assigned to this?
+            # done = 1 # review - weight should not be assigned to this?
+        )
+
+
+        # ========================== Get Ref Position ==========================
+        # Reference Position is at the centre of the wrist
+        # The ref pos will remain fixed since it was taken before simulation started
+        mj.mj_forward(self.model, self.data)
+        ref_body_idx = mj.mj_name2id(self.model, int(mj.mjtObj.mjOBJ_BODY), f"{self.hand_name}_rc_centre_block")
+        self.ref_pos = np.array(self.data.xpos[ref_body_idx])
+
+
+        # ========================== Define Observation and Action Space ==========================
         self.observation_space = gym.spaces.Box(low=-np.inf, high=np.inf, 
-                                                shape=(2*3*len(self.fingertip_indices),), 
-                                                dtype=float)
+                                                shape=(3*3*len(self.fingertip_indices),), 
+                                                dtype=np.float32)
         
-        # Define Action Space
+
         # NOTE: Watch out for Box upper limit if Carpal Actuators are involved
-        self.action_space = gym.spaces.Box(low=np.array([0.0]*self.model.nu), 
-                                            high=np.array([10.0]*4 + [2.0]*(self.model.nu-4)), 
-                                            shape=(self.model.nu,), dtype=float)
+        # FIXME: Fix action range in reward and step functions. Current ==> [0,2] after denorm
+        # Define a mujoco action range array used for scaling
+        self.action_space = gym.spaces.Box(low=np.array([-1.0]*self.model.nu), 
+                                            high=np.array([1.0]*self.model.nu), 
+                                            shape=(self.model.nu,), dtype=np.float32)
+
 
-        # For Human Rendering
+        # ========================== For Human Rendering ==========================
         self.window = None
         self.window_size = 1200, 900
 
-    def _load_model(self, xml_path):
-        dirname = os.path.dirname(__file__)
-        abspath = os.path.join(dirname + "/" + xml_path)
-        xml_path = abspath
-
+    def _load_model(self):
+        xml_path = DARM_XML_FILE
         self.model = mj.MjModel.from_xml_path(xml_path)
+
         if self.model: 
             print("Loaded XML file successfully") 
         else:
@@ -64,6 +120,44 @@ class DARMEnv(gym.Env):
         
         self.data = mj.MjData(self.model)
 
+    def _load_start_states(self):
+        if self.single_finger_env:
+            filename = SF_START_STATE_FILE
+        else:
+            filename = MF_START_STATE_FILE
+
+        with open(filename, 'rb') as f:
+            self.start_states = np.load(f, allow_pickle=True)
+            self.start_states_len = len(self.start_states)
+
+    def _get_joint_limits(self, type = None):
+        joint_limits = []
+        for i in range(self.model.njnt):
+            joint_limits.append(self.model.jnt_range[i]*(180/np.pi))
+        
+        joint_limits = np.asarray(joint_limits)
+        
+        if type == "min":
+            return joint_limits[:, 0]
+        if type == "max":
+            return joint_limits[:, 1]
+        
+        return joint_limits[:, 0], joint_limits[:, 1]
+
+    def _compute_target_joint_state_delta(self):
+        # return (self.max_joint_vals - self.min_joint_vals)//10  # for every range of 10 deg, have a delta of 1 deg
+        joint_state_delta =  ((self.max_joint_vals - self.min_joint_vals)//40) * 2  # for every range of 40 deg, have a delta of 2 deg
+        return np.clip(joint_state_delta, a_min=2, a_max=10)    # a minimum delta of 2 degrees, max of 10 degrees
+
+    def _get_actuator_ctrlrange(self, type = None):
+        if type == "min":
+            return np.array([self.model.actuator_ctrlrange[i][0] for i in range(self.model.nu)])
+        if type == "max":
+            return np.array([self.model.actuator_ctrlrange[i][1] for i in range(self.model.nu)])
+    
+        ctrl_range = np.array([self.model.actuator_ctrlrange[i] for i in range(self.model.nu)])
+        return ctrl_range[:, 0], ctrl_range[:, 1] # (min, max)
+
     def _init_controller(self):
         pass
 
@@ -72,12 +166,25 @@ class DARMEnv(gym.Env):
 
     def _get_fingertip_indices(self):
         # NOTE: Remember to set the mocap index properly in reset()
-        indices = ["i", "ii", "iii", "iv", "v"]
+        if self.single_finger_env:
+            indices = ["ii"]
+        else:
+            indices = ["i", "ii", "iii", "iv", "v"]
+
         self.fingertip_indices = [mj.mj_name2id(self.model, int(mj.mjtObj.mjOBJ_SITE), f"{self.hand_name}_fingertip_{i}") for i in indices]
     
-    def _get_obs(self):
-        return np.concatenate((np.array([self.data.site_xpos[i] for i in self.fingertip_indices]).flatten(),
-                             self.target_obs))
+    def _get_obs(self, prev_obs, new_obs, action_time=None):
+        if not action_time:
+            # if no action time velocity is zero. i.e. after reset
+            vel_obs = np.zeros((3*len(self.fingertip_indices),))
+        elif action_time: 
+            prev_fingertip_pos = prev_obs[:3*len(self.fingertip_indices)]
+            new_fingertip_pos = new_obs[:3*len(self.fingertip_indices)]
+            vel_obs = (new_fingertip_pos - prev_fingertip_pos)/action_time
+
+        return np.concatenate((np.array([(np.array(self.data.site_xpos[i]) - self.ref_pos) for i in self.fingertip_indices]).flatten(),
+                             self.target_obs,
+                             vel_obs))
 
     def _get_info(self):
         return {"sim_time": self.data.time - self.ep_start_time}
@@ -85,95 +192,168 @@ class DARMEnv(gym.Env):
     def _norm_to_target(self, obs):
         """
         Returns the norm of each fingertip to the target position
-        obs: an observation from the observation space [...fingertip_pos, ...target_pos]
+        obs: an observation from the observation space [...fingertip_pos, ...target_pos, ...fingertip_vel]
         """
         obs = obs.reshape((-1, 3))
-        n_fingertips = len(obs)//2
+        n_fingertips = len(self.fingertip_indices)
 
         fingertip_poses = obs[0:n_fingertips]
-        target_poses = obs[n_fingertips:]
+        target_poses = obs[n_fingertips:2*n_fingertips]
 
         return np.linalg.norm(fingertip_poses-target_poses, ord=2, axis=-1)
 
-    def _get_reward(self, state, action, new_state, time_delta):
+    def _get_reward(self, action, new_state):
         """
-        Reward function to compute reward given state, action, and new state.
-        R = R(S, a, S')
-
-        If norm to target reduces: -1 else (-1 + x) where x is a neg. number 
-                proportional to number of fingers with increased norms
-        Punish high velocity according to the eqution: -0.3 + 0.3*np.exp(-1*vel)
-        Punish high torque according to the equation: -0.2 + 0.2*np.exp(-1*action)
-        Reward reaching target with a tolerance of 4mm: 250
+        Reward function to compute reward given action, and new state.
+        R = R(a, S')
+
+        Agent is punished for being far from target
+        Agent is punished for going farther than a threshold from the target
+        Agent is punished for high action magnitude
+        Agent is rewarded for being close to the target
+        Agent is rewarded for coming close to the target beyond a threshold 
         """
 
-        # Change in Norm to Target
-        prev_norm = self._norm_to_target(state)
-        new_norm = self._norm_to_target(new_state)
-        norm_reward = -1 + sum(new_norm > prev_norm)*(-1/len(self.fingertip_indices))
+        reach_dist = self._norm_to_target(new_state)    # NOTE: Single finger
+        near_th = self.min_th
+        far_th = 2*self.max_target_th
 
-        # Velocity Correction
-        previous_pos = state[:len(state)//2].reshape((-1,3))
-        new_pos = new_state[:len(new_state)//2].reshape((-1,3))
-        vel = np.linalg.norm(new_pos-previous_pos, ord=2, axis=-1) / time_delta
-        vel_reward = (-0.3 + 0.3*np.exp(-1*vel)).mean() # scale vel term in exp beween [-1,-5]
+        # Scale action down to [0, 1] from [0, max_tendon_tension]
+        action = action / self.max_tendon_tension
         
-        # Effort Correction
-        action_reward = (-0.2 + 0.2*np.exp(-1*action)).mean()
-
-        # Reach Target Reward
-        target_reward = 250 if self._get_done(new_state) else 0
-
-        return (norm_reward + vel_reward + action_reward + target_reward)
+        # NOTE: Some of the fingers in five fingered hand have more than five actuators
+        # act_mag = np.linalg.norm(action.reshape(-1, 5)) # reshape action to (-1,5), ensure nu is ordered from mujoco
+        # TODO: Consider scaling down this act_mag to be equiv. to a single finger with nu=5
+        act_mag = np.linalg.norm(action)/np.sqrt(self.model.nu/1) # action magnitude is not measured per finger but as a whole
+        # by dividing by sqrt(nu/5), the norm is similar to when computing with nu==5. Check it out.
+        # by dividing by sqrt(nu) act_mag will have a max value in the order of the max_value of action now => 1
+        
+        rwd_dict = collections.OrderedDict((
+            # Optional Keys
+            ('reach',   -1.*reach_dist),
+            ('bonus',   1.*(reach_dist<2*near_th) + 1.*(reach_dist<near_th)),
+            ('act_reg', -1.*act_mag),
+            ('penalty', -1.*(reach_dist>far_th)),
+            # Must keys
+            ('sparse',  -1.*reach_dist),
+            ('solved',  reach_dist<near_th),
+            ('done',    reach_dist > far_th),
+        ))
+
+            # Weights:
+            # reach = 1.0,
+            # bonus = 4.0,
+            # penalty = 50,
+            # act_reg = 0.1,
+        rwd_dict['dense'] = np.sum([wt*rwd_dict[key] for key, wt in self.rwd_keys_wt.items()], axis=0)
+        return rwd_dict
 
     def _get_done(self, new_state):
-        return all(self._norm_to_target(new_state) < 0.004)
+        return all(self._norm_to_target(new_state) < self.min_th)
+
+    def _check_collision(self):
+        """Returns True if there is collision, otherwise False"""
+        return len(self.data.contact.geom1) > 0
+
+    def generate_start_state(self):
+        while True:
+            # ========================== Sample valid start_state from Joint Space ==========================
+            joint_state = np.random.uniform(low=self.min_joint_vals, high=self.max_joint_vals)
+            self.forward(joint_state)
+            if self._check_collision(): # returns True if there is collision
+                # ensure there is no collision at the start state
+                continue
+        
+            # ========================== Create a valid target ==========================
+            joint_state_delta = self.target_joint_state_delta*np.random.choice(a=[-1,1], size=(self.model.njnt,), replace=True)
+            target_joint_state = np.clip(a=joint_state + joint_state_delta, 
+                                        a_min=self.min_joint_vals, 
+                                        a_max=self.max_joint_vals)
+            self.target_obs = self.forward(target_joint_state)[:3*len(self.fingertip_indices)]
+            if self._check_collision(): # returns True if there is collision
+                # ensure there is no collision at the target state
+                continue
+            
+            # Return to start state
+            observation = self.forward(joint_state)
+
+            # Verify distance of start state to target state is within limits
+            norm = self._norm_to_target(observation)
+            if not (all(norm >= self.min_target_th) and all(norm <= self.max_target_th)):
+                continue
+            
+            # If all checks are positive, break random search loop
+            return observation, joint_state, self.target_obs      
+
+    def sample_saved_start_states(self):
+        # Sample a start state from saved start states
+        # start_state = [joint_state, target_obs]
+        start_state = self.start_states[np.random.randint(self.start_states_len)]
+
+        # Set Target Obs
+        self.target_obs = start_state[1]
+
+        # Go forward to start state
+        observation = self.forward(start_state[0])
+
+        # Return Observation
+        return observation
 
     # def reset(self, seed=None, options=None):
         # super().reset()
     def reset(self, **kwargs):
-        # Reset Model. TODO: Consider not reseting the model, let next goal run from the old state
-        # mj.mj_resetData(self.model, self.data)
-        # mj.mj_forward(self.model, self.data)
-        
-        # Create a new goal
-        self.target_obs = self.targets[np.random.randint(0, self.targets_len)]
-        if self.render_mode == "human":
-            # BUG: Set the finger index properly here [ii, i, iii, iv, v]
-            self.data.mocap_pos = self.target_obs.reshape(len(self.fingertip_indices),3)
-        mj.mj_forward(self.model, self.data)    # TODO: See possibility of turning thumb here
-        self.ep_start_time = self.data.time
-
-
-        observation = self._get_obs()
-        # info = self._get_info()
+        # ========================== Get a random valid pose and target ==========================
+        # observation, _, _ = self.generate_start_state()
+        observation = self.sample_saved_start_states()
 
+        # ========================== Render Frame ==========================
         if self.render_mode == "human":
+            # Update target visualization mocaps pos
+            self.data.mocap_pos = self.target_obs.reshape(len(self.fingertip_indices),3) + self.ref_pos
             self._render_frame()
 
-        return observation #, info
+        self.ep_start_time = self.data.time
+        return observation
 
     def step(self, action):
-        prev_obs = self._get_obs()
+        prev_obs = self._get_obs(prev_obs=None, new_obs=None, action_time=None)
+
+        # action from model is in the range [-1,1]
+        # action + 1 === [0, 2]
+        # action * x === [0, 2x]
+        action = (action + 1)*(self.max_tendon_tension/2)
+        action = np.clip(action, 0, self.max_tendon_tension)
         self.data.ctrl[0 : self.model.nu] = action
         time_prev = self.data.time   # simulation time in seconds
 
-        # Perform action    
-        while (self.data.time - time_prev < self.reaction_time):
+        # Perform action  
+        while (self.data.time - time_prev < self.action_time):
             mj.mj_step(self.model, self.data)
         time_after = self.data.time # time after performing action
 
-        # Get new observation
-        new_obs = self._get_obs()
 
-        # Get Reward
-        reward = self._get_reward(prev_obs, action, new_obs, time_after-time_prev)
+        # Get new observation (fingertips_pos)
+        new_obs = self._get_obs(prev_obs=None, new_obs=None, action_time=None)
+        # include velocity in new obs
+        new_obs = self._get_obs(prev_obs=prev_obs,
+                                new_obs=new_obs, 
+                                action_time=time_after-time_prev)
 
         if self.render_mode == "human":
             self._render_frame()
 
-        return new_obs, reward, self._get_done(new_obs), self._get_info()   # False, self._get_info()
+        # Get Reward
+        rwd_dict = self._get_reward(action, new_obs)
+        reward = rwd_dict["dense"].mean()
+        done = any(rwd_dict["done"])  # all(rwd_dict["done"])
         
+        return new_obs, reward, done, {**self._get_info(), "action": action, "reward": {**rwd_dict}}
+
+    def forward(self, joint_conf):
+        self.data.qpos = joint_conf
+        mj.mj_forward(self.model, self.data)
+        return self._get_obs(prev_obs=None, new_obs=None, action_time=None)
+
     def render(self, mode, **kwargs):
         if self.render_mode == "human":
             self._render_frame()
diff --git a/darm_gym_env/darm_sf_gym.py b/darm_gym_env/darm_sf_gym.py
index a4d8975..7c812a4 100644
--- a/darm_gym_env/darm_sf_gym.py
+++ b/darm_gym_env/darm_sf_gym.py
@@ -9,7 +9,7 @@ from mujoco.glfw import glfw
 from pathlib import Path
 
 
-TARGETS_FILE = f"{os.getenv('DARM_MUJOCO_PATH')}/darm_sf_joint_space_targets.npy"
+# TARGETS_FILE = f"{os.getenv('DARM_MUJOCO_PATH')}/darm_sf_joint_space_targets.npy"
 DARM_XML_FILE = f"{os.getenv('DARM_MUJOCO_PATH')}/mujoco_env/darm.xml"
 
 class DARMSFEnv(gym.Env):
@@ -18,8 +18,8 @@ class DARMSFEnv(gym.Env):
     def __init__(self, render_mode=None, reaction_time=0.08, hand_name="hand1",
                     target_joint_state_delta = [4, 8, 8, 8],
                     min_th = 0.004,
-                    min_target_th = 0.008,
-                    max_target_th = 0.04,
+                    min_target_th = 2*0.004,
+                    max_target_th = 10*0.004,
                     min_joint_vals = [-20, -45, -10, -10],
                     max_joint_vals = [20, 90, 90, 90]) -> None:
         super().__init__()
@@ -45,13 +45,13 @@ class DARMSFEnv(gym.Env):
             bonus = 4.0,
             penalty = 50,
             act_reg = 0.1,
-            sparse = 1,
-            solved = 1,
-            done = 1
+            # sparse = 1,
+            # solved = 1, # review - weight should not be assigned to this?
+            # done = 1 # review - weight should not be assigned to this?
         )
 
         # Load the Model
-        self._load_model("../mujoco_env/darm.xml")
+        self._load_model()
         if not (self.model and self.data):
             raise "Error loading model"
         self._get_fingertip_indices()
@@ -64,10 +64,10 @@ class DARMSFEnv(gym.Env):
         self.ref_pos = np.array(self.data.xpos[ref_body_idx])
 
         # Load targets
-        with open(TARGETS_FILE, 'rb') as f:
-            # np.array([np.random.random((15)) for _ in range(5)])
-            self.targets = np.load(f)
-        self.targets_len = len(self.targets)
+        # with open(TARGETS_FILE, 'rb') as f:
+        #     # np.array([np.random.random((15)) for _ in range(5)])
+        #     self.targets = np.load(f)
+        # self.targets_len = len(self.targets)
 
         # Define Observation Space
         self.observation_space = gym.spaces.Box(low=-np.inf, high=np.inf, 
@@ -86,7 +86,7 @@ class DARMSFEnv(gym.Env):
         self.window = None
         self.window_size = 1200, 900
 
-    def _load_model(self, xml_path):
+    def _load_model(self):
         xml_path = DARM_XML_FILE
         self.model = mj.MjModel.from_xml_path(xml_path)
 
@@ -111,9 +111,12 @@ class DARMSFEnv(gym.Env):
     def _get_obs(self, prev_obs, new_obs, action_time=None):
         if not action_time:
             # if no action time velocity is zero. i.e. after reset
-            vel_obs = np.zeros((3,))
+            vel_obs = np.zeros((3*len(self.fingertip_indices),))
         elif action_time: 
-            vel_obs = (prev_obs[:3] - new_obs[:3])/action_time
+            prev_fingertip_pos = prev_obs[:3*len(self.fingertip_indices)]
+            new_fingertip_pos = new_obs[:3*len(self.fingertip_indices)]
+            # BUG: FIXED: (prev - new) to (new - prev)
+            vel_obs = (new_fingertip_pos - prev_fingertip_pos)/action_time
 
         return np.concatenate((np.array([(np.array(self.data.site_xpos[i]) - self.ref_pos) for i in self.fingertip_indices]).flatten(),
                              self.target_obs,
@@ -125,7 +128,7 @@ class DARMSFEnv(gym.Env):
     def _norm_to_target(self, obs):
         """
         Returns the norm of each fingertip to the target position
-        obs: an observation from the observation space [...fingertip_pos, ...target_pos]
+        obs: an observation from the observation space [...fingertip_pos, ...target_pos, ...fingertip_vel]
         """
         obs = obs.reshape((-1, 3))
         n_fingertips = len(self.fingertip_indices)
diff --git a/darm_gym_env/multi_darm_gym.py b/darm_gym_env/multi_darm_gym.py
deleted file mode 100644
index bcd1878..0000000
--- a/darm_gym_env/multi_darm_gym.py
+++ /dev/null
@@ -1,272 +0,0 @@
-import os
-import numpy as np
-
-import gym
-import mujoco as mj
-from mujoco.glfw import glfw
-
-
-
-class DARMEnv(gym.Env):
-    metadata = {"render_modes": ["human"], "render_fps": 1}
-
-    # TODO: Single finger: Actuators, Fingertip Observations, ...
-
-    def __init__(self, n_hands=1, render_mode=None, reaction_time=0.08) -> None:
-        super().__init__()
-
-        # Load the Model
-        self.n_hands = n_hands
-        self._load_model("../mujoco_env/darm.xml")
-        if not (self.model and self.data):
-            raise "Error loading model"
-        self._get_fingertip_indices()
-
-        # Define Observation Space
-        self.observation_space = gym.spaces.Box(low=-np.inf, high=np.inf, 
-                                                shape=(self.n_hands*2*15,), dtype=float)
-        
-        # Define Action Space
-        self.action_space = gym.spaces.Box(low=np.array([0.0]*self.model.nu), 
-                                            high=np.array(([20.0]*4 + [2.0]*(self.model.nu//self.n_hands-4))*self.n_hands), 
-                                            shape=(self.model.nu,), dtype=float)
-
-        # Env Parameters
-        self.reaction_time = reaction_time
-        assert render_mode is None or render_mode in self.metadata["render_modes"]
-        self.render_mode = render_mode
-
-        # For Human Rendering
-        self.window = None
-        self.window_size = 1200, 900
-
-    def _load_model(self, xml_path):
-        dirname = os.path.dirname(__file__)
-        abspath = os.path.join(dirname + "/" + xml_path)
-        xml_path = abspath
-
-        self.model = mj.MjModel.from_xml_path(xml_path)
-        if self.model: 
-            print("Loaded XML file successfully") 
-        else:
-            print(f"Error Loading XML file: {xml_path}")
-            return
-        
-        self.data = mj.MjData(self.model)
-
-    def _init_controller(self):
-        pass
-
-    def _controller_cb(self, model, data):
-        pass
-
-    def _get_fingertip_indices(self):
-        # FIXME: For independent finger training
-        fingertip_names = [[f"hand{j}_fingertip_{i}" for i in ["i", "ii", "iii", "iv", "v"]] for j in range(1,self.n_hands+1)]
-        fingertip_names = np.array(fingertip_names).flatten()
-        self.fingertip_indices = np.array([mj.mj_name2id(self.model, int(mj.mjtObj.mjOBJ_SITE), name) for name in fingertip_names]).reshape((-1,5))
-
-    def _get_obs(self):
-        obs = []
-        for i in range(self.n_hands):
-            fingertip_idxs = self.fingertip_indices[i, :]
-            target_obs = self.target_obs[i, :]
-            obs.append(np.concatenate((np.array([self.data.site_xpos[i] for i in fingertip_idxs]).flatten(),
-                             target_obs)))
-        return np.array(obs).flatten()
-
-    def _get_info(self):
-        return {"sim_time": self.data.time}
-
-    def _norm_to_target(self, obs):
-        """
-        Returns the norm of each fingertip to the target position
-        obs: an observation from the observation space [...fingertip_pos, ...target_pos]
-        """
-        obs = obs.reshape((-1, 3))
-        n_fingertips = len(obs)//2
-
-        fingertip_poses = obs[0:n_fingertips]
-        target_poses = obs[n_fingertips:]
-
-        return np.linalg.norm(fingertip_poses-target_poses, ord=2, axis=-1)
-
-    def _get_reward(self, state, action, new_state, time_delta):
-        """
-        Reward function to compute reward given state, action, and new state.
-        R = R(S, a, S')
-
-        If norm to target reduces: -1 else (-1 + x) where x is a neg. number 
-                proportional to number of fingers with increased norms
-        Punish high velocity according to the eqution: -0.3 + 0.3*np.exp(-1*vel)
-        Punish high torque according to the equation: -0.2 + 0.2*np.exp(-1*action)
-        Reward reaching target with a tolerance of 4mm: 250
-        """
-
-        # Change in Norm to Target
-        prev_norm = self._norm_to_target(state)
-        new_norm = self._norm_to_target(new_state)
-        norm_reward = -1 + sum(new_norm > prev_norm)*(-1/5)
-
-        # Velocity Correction
-        previous_pos = state[:len(state)//2].reshape((-1,3))
-        new_pos = new_state[:len(new_state)//2].reshape((-1,3))
-        vel = np.linalg.norm(new_pos-previous_pos, ord=2, axis=-1) / time_delta
-        vel_reward = (-0.3 + 0.3*np.exp(-1*vel)).mean() # scale vel beween [-1,-5]
-        
-        # Effort Correction
-        action_reward = (-0.2 + 0.2*np.exp(-1*action)).mean()
-
-        # Reach Target Reward
-        target_reward = 250 if self._get_done(new_state) else 0
-
-        return (norm_reward + vel_reward + action_reward + target_reward)
-
-    def _get_done(self, new_state):
-        return all(self._norm_to_target(new_state) < 0.004)
-
-    def reset(self, seed=None, options=None):
-        super().reset(seed=seed)
-
-        mj.mj_resetData(self.model, self.data)
-        mj.mj_forward(self.model, self.data)
-
-        # Create a new goal
-        self.target_obs = np.ones((self.n_hands, 15,))   # TODO:
-
-        observation = self._get_obs()
-        info = self._get_info()
-
-        if self.render_mode == "human":
-            self._render_frame()
-
-        return observation, info
-
-    def step(self, action):
-        prev_obs = self._get_obs()
-        self.data.ctrl[0 : self.model.nu] = action
-        time_prev = self.data.time   # simulation time in seconds
-
-        # Perform action    
-        while (self.data.time - time_prev < self.reaction_time):
-            mj.mj_step(self.model, self.data)
-        time_after = self.data.time # time after performing action
-
-        # Get new observation
-        new_obs = self._get_obs()
-
-        # Get Reward
-        reward = self._get_reward(prev_obs, action, new_obs, time_after-time_prev)
-
-        if self.render_mode == "human":
-            self._render_frame()
-
-        return new_obs, reward, self._get_done(new_obs), False, self._get_info()
-        
-    def render(self):
-        if self.render_mode == "human":
-            self._render_frame()
-
-    def _render_frame(self):
-        if self.render_mode == "human" and not self.window:
-            # Init GLFW, create window, make OpenGL context current, request v-sync
-            glfw.init()
-            self.window = glfw.create_window(self.window_size[0], self.window_size[1], "DARM", None, None)
-            glfw.make_context_current(self.window)
-            glfw.swap_interval(1)
-
-            # Visualization
-            self.cam = mj.MjvCamera()    # abstract camera
-            self.opt = mj.MjvOption()    # visualization options
-            mj.mjv_defaultCamera(self.cam)
-            mj.mjv_defaultOption(self.opt)
-            self.scene = mj.MjvScene(self.model, maxgeom=10000)
-            self.context = mj.MjrContext(self.model, mj.mjtFontScale.mjFONTSCALE_150.value)
-
-            self.cam.azimuth = 90
-            self.cam.elevation = -45
-            self.cam.distance = 2
-            self.cam.lookat = np.array([0.0, 0.0, 0])
-
-            # For callback functions TODO:
-            self.window_button_left = False
-            self.window_button_middle = False
-            self.window_button_right = False
-            self.window_lastx = 0
-            self.window_lasty = 0
-
-            def mouse_button(window, button, act, mods):
-                # update button state
-                self.window_button_left = (glfw.get_mouse_button(
-                    window, glfw.MOUSE_BUTTON_LEFT) == glfw.PRESS)
-                self.window_button_middle = (glfw.get_mouse_button(
-                    window, glfw.MOUSE_BUTTON_MIDDLE) == glfw.PRESS)
-                self.window_button_right = (glfw.get_mouse_button(
-                    window, glfw.MOUSE_BUTTON_RIGHT) == glfw.PRESS)
-
-                # update mouse position
-                glfw.get_cursor_pos(window) # TODO: Why is this needed again
-
-            def mouse_move(window, xpos, ypos):
-                # compute mouse displacement, save
-                dx = xpos - self.window_lastx
-                dy = ypos - self.window_lasty
-                self.window_lastx = xpos
-                self.window_lasty = ypos
-
-                # no buttons down: nothing to do
-                if (not self.window_button_left) and (not self.window_button_middle) and (not self.window_button_right):
-                    return
-
-                # get current window size
-                width, height = glfw.get_window_size(window)
-
-                # get shift key state
-                PRESS_LEFT_SHIFT = glfw.get_key(
-                    window, glfw.KEY_LEFT_SHIFT) == glfw.PRESS
-                PRESS_RIGHT_SHIFT = glfw.get_key(
-                    window, glfw.KEY_RIGHT_SHIFT) == glfw.PRESS
-                mod_shift = (PRESS_LEFT_SHIFT or PRESS_RIGHT_SHIFT)
-
-                # determine action based on mouse button
-                if self.window_button_right:
-                    if mod_shift:
-                        action = mj.mjtMouse.mjMOUSE_MOVE_H
-                    else:
-                        action = mj.mjtMouse.mjMOUSE_MOVE_V
-                elif self.window_button_left:
-                    if mod_shift:
-                        action = mj.mjtMouse.mjMOUSE_ROTATE_H
-                    else:
-                        action = mj.mjtMouse.mjMOUSE_ROTATE_V
-                else:
-                    action = mj.mjtMouse.mjMOUSE_ZOOM
-
-                mj.mjv_moveCamera(self.model, action, dx/width,
-                                dy/height, self.scene, self.cam)    # TODO: Look into this, height/width issue
-
-            def scroll(window, xoffset, yoffset):
-                action = mj.mjtMouse.mjMOUSE_ZOOM
-                mj.mjv_moveCamera(self.model, action, 0.0, -0.05 *
-                                yoffset, self.scene, self.cam)
-
-            glfw.set_cursor_pos_callback(self.window, mouse_move)
-            glfw.set_mouse_button_callback(self.window, mouse_button)
-            glfw.set_scroll_callback(self.window, scroll)
-
-        # Get Framebuffer Viewport
-        vp_width, vp_height = glfw.get_framebuffer_size(self.window)
-        viewport = mj.MjrRect(0, 0, vp_width, vp_height)
-
-        # Update scene and render
-        mj.mjv_updateScene(self.model, self.data, self.opt, None, self.cam, mj.mjtCatBit.mjCAT_ALL.value, self.scene)
-        mj.mjr_render(viewport, self.scene, self.context)
-
-        # swap OpenGL buffers (blocking call due to v-sync)
-        glfw.swap_buffers(self.window)
-
-        # process pending GUI events, call GLFW callbacks
-        glfw.poll_events()
-
-    def close(self):
-        glfw.terminate()
\ No newline at end of file
diff --git a/darm_gym_targets_gen.py b/darm_gym_targets_gen.py
deleted file mode 100644
index f7a0e00..0000000
--- a/darm_gym_targets_gen.py
+++ /dev/null
@@ -1,54 +0,0 @@
-import gym
-from darm_gym_env import DARMEnv
-import time
-import random
-import numpy as np
-import os
-
-# [AdAb: rand; FlEx: rand]
-# [AdAb: 0; FlEx: 0]
-
-TARGETS_FILE = "darm_targets.npy"
-
-targets = []
-if os.path.exists(TARGETS_FILE):
-    print("Loading saved target")
-    with open(TARGETS_FILE, 'rb') as f:
-        targets = [i for i in np.load(f)]
-        print(f"Targets: {targets[-2:]}, ...")
-else:
-    f = open(TARGETS_FILE, 'x')
-    f.close()
-
-env = gym.make("darm/DarmHand-v0", render_mode=None, hand_name="hand1")
-N_EPISODES = 100
-
-for _ in range(N_EPISODES):
-    done = False
-    obs, info = env.reset()
-
-    start = time.time() 
-    while not done:
-        ac = env.action_space.sample()
-        if random.random() > 0.5: 
-            ac[0:4] = [0.0, 0.0, 20.0, 20.0]
-        obs, rew, term, trunc, info = env.step(ac)
-        targets.append(obs[:15])
-
-        done = term or trunc
-
-    # Print Statistics
-    print(f"Duration: {time.time() - start}")
-    print(info)
-
-# Close the environment
-env.close()
-
-# save the targets
-print("Saving Targets")
-print(f"Num of Targets: {len(targets)}")
-print(f"Targets: {targets[-2:]}, ...")
-with open(TARGETS_FILE, 'wb') as f:
-    np.save(f, np.array(targets))
-
-# TODO: Get Targets from other wrist pose
\ No newline at end of file
diff --git a/darm_sf_gym_targets_gen.py b/darm_sf_gym_targets_gen.py
deleted file mode 100644
index 53d9ae9..0000000
--- a/darm_sf_gym_targets_gen.py
+++ /dev/null
@@ -1,77 +0,0 @@
-import gym
-from darm_gym_env import DARMSFEnv
-import time
-import random
-import numpy as np
-import os
-
-
-TARGETS_FILE = "darm_sf_joint_space_targets.npy"
-
-targets = []
-if os.path.exists(TARGETS_FILE):
-    print("Loading saved target")
-    with open(TARGETS_FILE, 'rb') as f:
-        targets = [i for i in np.load(f)]
-        print(f"Targets: {targets[-2:]}, ...")
-else:
-    f = open(TARGETS_FILE, 'x')
-    f.close()
-
-env = gym.make("darm/DarmSFHand-v0", render_mode=None, hand_name="hand1")
-env.reset()
-N_SAMPLES = 10_000
-
-for i in range(N_SAMPLES):
-    max_vals = np.array([20, 90, 90, 90])*(np.pi/180)
-    min_vals = np.array([-20, -45, -10, -10])*(np.pi/180)
-
-    joint_space = gym.spaces.Box(low=min_vals, high=max_vals, 
-                                    shape=(4,), dtype=float)
-    
-    target = joint_space.sample()
-    while(target[3] > target[2]):
-        target = joint_space.sample()
-
-    obs = env.forward(target)
-    targets.append(obs[:3])
-
-    # env.render()
-    # time.sleep(1)
-
-    if i % 500 == 0:
-        print(f"Sampled {i} targets")
-
-# Close the environment
-env.close()
-
-# save the targets
-print("Saving Targets")
-print(f"Num of Targets: {len(targets)}")
-print(f"Targets: {targets[-2:]}, ...")
-with open(TARGETS_FILE, 'wb') as f:
-    np.save(f, np.array(targets))
-
-
-
-# all_obs = []
-# valid_obs_idx = []
-
-# def filter_obs(obs) -> bool:
-#     all_obs.append(obs)
-#     # if len(all_obs) % 70 == 0: print(f"Random Observation {len(all_obs)}: {obs}")
-#     max_vals = np.array([20, 90, 90, 90])*(np.pi/180)
-#     min_vals = np.array([-20, -45, -10, -10])*(np.pi/180)
-#     # max_vals = np.array([0.3491, 1.571, 1.571, 1.571])
-#     # min_vals = np.array([-0.3491, -0.7854, -0.1745, -0.1745])
-
-#     lm = obs <= max_vals
-#     gm = obs >= min_vals
-#     lm = lm.all(); gm = gm.all()
-#     valid = lm and gm
-
-#     if valid:
-#         print(f"Valid Observation: Total Valid: {len(valid_obs_idx)} All: {len(all_obs)} || {obs}")
-#         valid_obs_idx.append(len(all_obs) - 1)
-#         return True
-#     return False
\ No newline at end of file
diff --git a/darm_sf_joint_space_targets.npy b/darm_sf_joint_space_targets.npy
deleted file mode 100644
index 43b37e0..0000000
Binary files a/darm_sf_joint_space_targets.npy and /dev/null differ
diff --git a/darm_targets.npy b/darm_targets.npy
deleted file mode 100644
index c52656a..0000000
Binary files a/darm_targets.npy and /dev/null differ
diff --git a/darm_training/.ipynb_checkpoints/sb3_sac_darm_sf_hand-nb2-checkpoint.ipynb b/darm_training/.ipynb_checkpoints/sb3_sac_darm_sf_hand-nb2-checkpoint.ipynb
index efc2d4f..4d00475 100644
--- a/darm_training/.ipynb_checkpoints/sb3_sac_darm_sf_hand-nb2-checkpoint.ipynb
+++ b/darm_training/.ipynb_checkpoints/sb3_sac_darm_sf_hand-nb2-checkpoint.ipynb
@@ -44,6 +44,32 @@
   {
    "cell_type": "code",
    "execution_count": 3,
+   "id": "fea4d0f1-e0e8-41d0-9c7c-006cb9a9926a",
+   "metadata": {},
+   "outputs": [
+    {
+     "name": "stdout",
+     "output_type": "stream",
+     "text": [
+      "Running generate_darm_xml.sh\n",
+      "Single Finger: true\n",
+      "No Wrist: true\n",
+      "\n",
+      "\n",
+      "\n",
+      "\n"
+     ]
+    }
+   ],
+   "source": [
+    "%%bash\n",
+    "cd ../mujoco_env\n",
+    "bash generate_darm_xml.sh true true"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 5,
    "id": "8a6733be-66a7-4aa7-81b5-b1b28b0a0efd",
    "metadata": {},
    "outputs": [
@@ -78,17 +104,130 @@
   },
   {
    "cell_type": "code",
-   "execution_count": null,
+   "execution_count": 6,
    "id": "3c027ec5-151e-4986-a444-2107e98fb741",
-   "metadata": {},
-   "outputs": [],
+   "metadata": {
+    "collapsed": true,
+    "jupyter": {
+     "outputs_hidden": true
+    },
+    "tags": []
+   },
+   "outputs": [
+    {
+     "name": "stdout",
+     "output_type": "stream",
+     "text": [
+      "running install\n",
+      "running bdist_egg\n",
+      "running egg_info\n",
+      "writing darm_gym_env.egg-info/PKG-INFO\n",
+      "writing dependency_links to darm_gym_env.egg-info/dependency_links.txt\n",
+      "writing requirements to darm_gym_env.egg-info/requires.txt\n",
+      "writing top-level names to darm_gym_env.egg-info/top_level.txt\n",
+      "reading manifest file 'darm_gym_env.egg-info/SOURCES.txt'\n",
+      "writing manifest file 'darm_gym_env.egg-info/SOURCES.txt'\n",
+      "installing library code to build/bdist.linux-x86_64/egg\n",
+      "running install_lib\n",
+      "running build_py\n",
+      "creating build/bdist.linux-x86_64/egg\n",
+      "creating build/bdist.linux-x86_64/egg/darm_gym_env\n",
+      "copying build/lib/darm_gym_env/darm_sf_gym.py -> build/bdist.linux-x86_64/egg/darm_gym_env\n",
+      "copying build/lib/darm_gym_env/__init__.py -> build/bdist.linux-x86_64/egg/darm_gym_env\n",
+      "copying build/lib/darm_gym_env/multi_darm_gym.py -> build/bdist.linux-x86_64/egg/darm_gym_env\n",
+      "copying build/lib/darm_gym_env/darm_gym.py -> build/bdist.linux-x86_64/egg/darm_gym_env\n",
+      "copying build/lib/darm_gym_env/env_test.py -> build/bdist.linux-x86_64/egg/darm_gym_env\n",
+      "byte-compiling build/bdist.linux-x86_64/egg/darm_gym_env/darm_sf_gym.py to darm_sf_gym.cpython-38.pyc\n",
+      "byte-compiling build/bdist.linux-x86_64/egg/darm_gym_env/__init__.py to __init__.cpython-38.pyc\n",
+      "byte-compiling build/bdist.linux-x86_64/egg/darm_gym_env/multi_darm_gym.py to multi_darm_gym.cpython-38.pyc\n",
+      "byte-compiling build/bdist.linux-x86_64/egg/darm_gym_env/darm_gym.py to darm_gym.cpython-38.pyc\n",
+      "byte-compiling build/bdist.linux-x86_64/egg/darm_gym_env/env_test.py to env_test.cpython-38.pyc\n",
+      "creating build/bdist.linux-x86_64/egg/EGG-INFO\n",
+      "copying darm_gym_env.egg-info/PKG-INFO -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
+      "copying darm_gym_env.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
+      "copying darm_gym_env.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
+      "copying darm_gym_env.egg-info/requires.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
+      "copying darm_gym_env.egg-info/top_level.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
+      "creating 'dist/darm_gym_env-0.0.1-py3.8.egg' and adding 'build/bdist.linux-x86_64/egg' to it\n"
+     ]
+    },
+    {
+     "name": "stderr",
+     "output_type": "stream",
+     "text": [
+      "/home/daniel/miniconda3/lib/python3.8/site-packages/setuptools/command/install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools.\n",
+      "  warnings.warn(\n",
+      "/home/daniel/miniconda3/lib/python3.8/site-packages/setuptools/command/easy_install.py:144: EasyInstallDeprecationWarning: easy_install command is deprecated. Use build and pip and other standards-based tools.\n",
+      "  warnings.warn(\n",
+      "zip_safe flag not set; analyzing archive contents...\n",
+      "darm_gym_env.__pycache__.multi_darm_gym.cpython-38: module references __file__\n"
+     ]
+    },
+    {
+     "name": "stdout",
+     "output_type": "stream",
+     "text": [
+      "removing 'build/bdist.linux-x86_64/egg' (and everything under it)\n",
+      "Processing darm_gym_env-0.0.1-py3.8.egg\n",
+      "removing '/home/daniel/miniconda3/lib/python3.8/site-packages/darm_gym_env-0.0.1-py3.8.egg' (and everything under it)\n",
+      "creating /home/daniel/miniconda3/lib/python3.8/site-packages/darm_gym_env-0.0.1-py3.8.egg\n",
+      "Extracting darm_gym_env-0.0.1-py3.8.egg to /home/daniel/miniconda3/lib/python3.8/site-packages\n",
+      "darm-gym-env 0.0.1 is already the active version in easy-install.pth\n",
+      "\n",
+      "Installed /home/daniel/miniconda3/lib/python3.8/site-packages/darm_gym_env-0.0.1-py3.8.egg\n",
+      "Processing dependencies for darm-gym-env==0.0.1\n",
+      "Searching for gym==0.21.0\n",
+      "Best match: gym 0.21.0\n",
+      "Adding gym 0.21.0 to easy-install.pth file\n",
+      "\n",
+      "Using /home/daniel/miniconda3/lib/python3.8/site-packages\n",
+      "Searching for mujoco==2.2.2\n",
+      "Best match: mujoco 2.2.2\n",
+      "Processing mujoco-2.2.2-py3.8-linux-x86_64.egg\n",
+      "mujoco 2.2.2 is already the active version in easy-install.pth\n",
+      "\n",
+      "Using /home/daniel/miniconda3/lib/python3.8/site-packages/mujoco-2.2.2-py3.8-linux-x86_64.egg\n",
+      "Searching for cloudpickle==2.2.0\n",
+      "Best match: cloudpickle 2.2.0\n",
+      "Adding cloudpickle 2.2.0 to easy-install.pth file\n",
+      "\n",
+      "Using /home/daniel/miniconda3/lib/python3.8/site-packages\n",
+      "Searching for numpy==1.23.4\n",
+      "Best match: numpy 1.23.4\n",
+      "Adding numpy 1.23.4 to easy-install.pth file\n",
+      "Installing f2py script to /home/daniel/miniconda3/bin\n",
+      "Installing f2py3 script to /home/daniel/miniconda3/bin\n",
+      "Installing f2py3.8 script to /home/daniel/miniconda3/bin\n",
+      "\n",
+      "Using /home/daniel/miniconda3/lib/python3.8/site-packages\n",
+      "Searching for PyOpenGL==3.1.6\n",
+      "Best match: PyOpenGL 3.1.6\n",
+      "Adding PyOpenGL 3.1.6 to easy-install.pth file\n",
+      "\n",
+      "Using /home/daniel/miniconda3/lib/python3.8/site-packages\n",
+      "Searching for glfw==2.5.5\n",
+      "Best match: glfw 2.5.5\n",
+      "Adding glfw 2.5.5 to easy-install.pth file\n",
+      "\n",
+      "Using /home/daniel/miniconda3/lib/python3.8/site-packages\n",
+      "Searching for absl-py==1.2.0\n",
+      "Best match: absl-py 1.2.0\n",
+      "Adding absl-py 1.2.0 to easy-install.pth file\n",
+      "\n",
+      "Using /home/daniel/miniconda3/lib/python3.8/site-packages\n",
+      "Finished processing dependencies for darm-gym-env==0.0.1\n"
+     ]
+    }
+   ],
    "source": [
-    "!python setup.py install"
+    "%%bash\n",
+    "cd ..\n",
+    "python setup.py install"
    ]
   },
   {
    "cell_type": "code",
-   "execution_count": null,
+   "execution_count": 7,
    "id": "9e8b1cb4-49d2-416d-b23b-004c54cefbf1",
    "metadata": {},
    "outputs": [],
@@ -135,13 +274,13 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 3,
+   "execution_count": 4,
    "id": "ea66991b-3d7c-4e2e-8133-c5f358fa797e",
    "metadata": {},
    "outputs": [],
    "source": [
     "import gym\n",
-    "from darm_gym_env import DARMSFEnv\n",
+    "from darm_gym_env import DARMEnv\n",
     "from stable_baselines3 import SAC\n",
     "from stable_baselines3.common.vec_env.subproc_vec_env import SubprocVecEnv\n",
     "from stable_baselines3.common.vec_env.vec_monitor import VecMonitor\n",
@@ -159,13 +298,16 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 4,
+   "execution_count": 5,
    "id": "2d7b4bbe-9994-4dc5-b212-c0f5d6ca25b2",
    "metadata": {},
    "outputs": [],
    "source": [
+    "run_name = \"test1_SF_SB3_SAC_4\"\n",
+    "\n",
     "config = {\n",
-    "    \"env_id\": \"darm/DarmSFHand-v0\",\n",
+    "    \"env_id\": \"darm/DarmHand-v0\", # changed from SF\n",
+    "    \"single_finger_env\": True,\n",
     "    \"algo\": \"SAC\",\n",
     "    \"rl_lib\": \"SB3\",\n",
     "    \n",
@@ -182,13 +324,15 @@
     "    \"no_improvement_min_evals\": 20,\n",
     "    \n",
     "    \"log_interval\": 20, # episodes\n",
-    "    \"wandb_model_save_freq\": 2_000 #5_000 timesteps?\n",
+    "    \"wandb_model_save_freq\": 2_000, #5_000 timesteps?\n",
+    "    \n",
+    "    \"run_local_dir\": f\"{os.getenv('DARM_MUJOCO_PATH')}/darm_training/results/darm_sf_hand/{run_name}\"\n",
     "}"
    ]
   },
   {
    "cell_type": "code",
-   "execution_count": 5,
+   "execution_count": 6,
    "id": "c6bdd726-0edc-4e6b-8c4b-b679c56ce71f",
    "metadata": {},
    "outputs": [
@@ -214,7 +358,7 @@
     {
      "data": {
       "text/html": [
-       "Run data is saved locally in <code>/home/daniel/DARM/darm_mujoco/darm_training/wandb/run-20230221_162741-fwqpzibt</code>"
+       "Run data is saved locally in <code>/home/daniel/DARM/darm_mujoco/darm_training/wandb/run-20230302_145847-jfvnsuje</code>"
       ],
       "text/plain": [
        "<IPython.core.display.HTML object>"
@@ -226,7 +370,7 @@
     {
      "data": {
       "text/html": [
-       "Syncing run <strong><a href='https://wandb.ai/danieladejumo/DARM/runs/fwqpzibt' target=\"_blank\">test4_SF_SB3_SAC</a></strong> to <a href='https://wandb.ai/danieladejumo/DARM' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
+       "Syncing run <strong><a href='https://wandb.ai/danieladejumo/DARM/runs/jfvnsuje' target=\"_blank\">test1_SF_SB3_SAC_4</a></strong> to <a href='https://wandb.ai/danieladejumo/DARM' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
       ],
       "text/plain": [
        "<IPython.core.display.HTML object>"
@@ -250,7 +394,7 @@
     {
      "data": {
       "text/html": [
-       " View run at <a href='https://wandb.ai/danieladejumo/DARM/runs/fwqpzibt' target=\"_blank\">https://wandb.ai/danieladejumo/DARM/runs/fwqpzibt</a>"
+       " View run at <a href='https://wandb.ai/danieladejumo/DARM/runs/jfvnsuje' target=\"_blank\">https://wandb.ai/danieladejumo/DARM/runs/jfvnsuje</a>"
       ],
       "text/plain": [
        "<IPython.core.display.HTML object>"
@@ -261,8 +405,6 @@
     }
    ],
    "source": [
-    "run_name = \"test1_SF_SB3_SAC_3\"\n",
-    "\n",
     "notes = \"\"\"\n",
     "- The environment was updated such that the target is within a range from the start point\n",
     "- Velocity penalty was removed and only effort penalty was used\n",
@@ -273,8 +415,14 @@
     "- max_episode_steps was updated to 200.\n",
     "- Velocity vector (size [3,]) was added to observation. Observation size is now (9,)\n",
     "- Action range was increased to [0, 5]\n",
+    "<Changes: ID 3>\n",
     "- Observation warpper to scale observation from m and m/s to cm and cm/s was applied\n",
+    "<Changes: ID 4>\n",
+    "- Max Tension for Digitorum Extensor Communis was increased to 10\n",
+    "- FIXED: Velocity Observation from (prev_pos - new_pos)/time to (new_pos - prev_pos)/time\n",
+    "- FIXED: Removed weight of 1 from 'sparse', 'solved', and 'done' in reward weighting\n",
     "\n",
+    "- Single-Finger; No Wrist Environment\n",
     "- This run was trained on vast_ai using SB3's SAC algo.\n",
     "\"\"\"\n",
     "\n",
@@ -294,7 +442,7 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 5,
+   "execution_count": 7,
    "id": "376efc5a-273e-4074-ad31-329575675bdc",
    "metadata": {},
    "outputs": [],
@@ -302,12 +450,12 @@
     "from gym.wrappers import TransformObservation\n",
     "# from gym.wrappers import RescaleAction\n",
     "\n",
-    "create_env = lambda: TransformObservation(gym.make(config[\"env_id\"]), lambda obs: obs*100)"
+    "create_env = lambda: TransformObservation(gym.make(config[\"env_id\"], single_finger_env=config[\"single_finger_env\"]), lambda obs: obs*100)"
    ]
   },
   {
    "cell_type": "code",
-   "execution_count": 6,
+   "execution_count": 8,
    "id": "87846de9-eca5-498d-a18d-c41b9363b5ab",
    "metadata": {
     "tags": []
@@ -332,6 +480,8 @@
      "name": "stdout",
      "output_type": "stream",
      "text": [
+      "Loaded XML file successfully\n",
+      "Loaded XML file successfully\n",
       "Loaded XML file successfully\n",
       "Loaded XML file successfully\n",
       "Loaded XML file successfully\n",
@@ -352,12 +502,12 @@
     "            learning_starts=config[\"learning_starts\"],\n",
     "            gradient_steps=NUM_CPU, # num of envs\n",
     "            policy_kwargs=policy_kwargs,\n",
-    "            tensorboard_log=\"./results/darm_sf_hand\")"
+    "            tensorboard_log=config['run_local_dir'])"
    ]
   },
   {
    "cell_type": "code",
-   "execution_count": 7,
+   "execution_count": 9,
    "id": "665027f7-6606-4178-9ddb-614d3c1c4706",
    "metadata": {},
    "outputs": [
@@ -371,10 +521,10 @@
     {
      "data": {
       "text/plain": [
-       "<stable_baselines3.common.callbacks.CallbackList at 0x7f4920e50fa0>"
+       "<stable_baselines3.common.callbacks.CallbackList at 0x7fede6ed14f0>"
       ]
      },
-     "execution_count": 7,
+     "execution_count": 9,
      "metadata": {},
      "output_type": "execute_result"
     }
@@ -383,23 +533,23 @@
     "eval_env = make_vec_env(create_env, n_envs=1, seed=config[\"seed\"])\n",
     "\n",
     "# Stop training when the model reaches the reward threshold\n",
-    "reward_thresh_callback = StopTrainingOnRewardThreshold(reward_threshold=config[\"mean_reward_thresh\"], verbose=1)\n",
+    "# reward_thresh_callback = StopTrainingOnRewardThreshold(reward_threshold=config[\"mean_reward_thresh\"], verbose=1)\n",
     "\n",
     "# Stop training if there is no improvement after more than N evaluations\n",
-    "stop_train_callback = StopTrainingOnNoModelImprovement(\n",
-    "    max_no_improvement_evals=config[\"max_no_improvement_evals\"], \n",
-    "    min_evals=config[\"no_improvement_min_evals\"], \n",
-    "    verbose=1)\n",
+    "# stop_train_callback = StopTrainingOnNoModelImprovement(\n",
+    "#     max_no_improvement_evals=config[\"max_no_improvement_evals\"], \n",
+    "#     min_evals=config[\"no_improvement_min_evals\"], \n",
+    "#     verbose=1)\n",
     "\n",
     "eval_callback = EvalCallback(eval_env, \n",
-    "                             best_model_save_path=f\"./results/darm_sf_hand/{run_name}/models/best\",\n",
-    "                             log_path=f\"./results/darm_sf_hand/{run_name}/models/best/logs\", \n",
+    "                             best_model_save_path=f\"{config['run_local_dir']}/models/best\",\n",
+    "                             log_path=f\"{config['run_local_dir']}/models/best/logs\", \n",
     "                             eval_freq=config[\"eval_freq\"],\n",
-    "                             callback_on_new_best=reward_thresh_callback,\n",
+    "                             # callback_on_new_best=reward_thresh_callback,\n",
     "                             # callback_after_eval=stop_train_callback,\n",
     "                             deterministic=True, render=False, verbose=1)\n",
     "\n",
-    "wandb_callback=WandbCallback(model_save_path=f\"./results/darm_sf_hand/{run_name}/models\",\n",
+    "wandb_callback=WandbCallback(model_save_path=f\"{config['run_local_dir']}/models\",\n",
     "                             model_save_freq=config[\"wandb_model_save_freq\"],\n",
     "                             verbose=2)\n",
     "\n",
@@ -410,13 +560,9 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 8,
+   "execution_count": 10,
    "id": "2866ed92-c5e1-4179-a261-0378da2b421f",
    "metadata": {
-    "collapsed": true,
-    "jupyter": {
-     "outputs_hidden": true
-    },
     "tags": []
    },
    "outputs": [
@@ -424,7 +570,7 @@
      "name": "stdout",
      "output_type": "stream",
      "text": [
-      "Logging to ./results/darm_sf_hand/test4_SF_SB3_SAC_1\n"
+      "Logging to /home/daniel/DARM/darm_mujoco/darm_training/results/darm_sf_hand/test1_SF_SB3_SAC_4/test1_SF_SB3_SAC_4_1\n"
      ]
     },
     {
@@ -441,484 +587,909 @@
      "text": [
       "---------------------------------\n",
       "| rollout/           |          |\n",
-      "|    ep_len_mean     | 91.5     |\n",
-      "|    ep_rew_mean     | -32.1    |\n",
+      "|    ep_len_mean     | 5        |\n",
+      "|    ep_rew_mean     | -48      |\n",
       "| time/              |          |\n",
       "|    episodes        | 20       |\n",
-      "|    fps             | 500      |\n",
-      "|    time_elapsed    | 3        |\n",
-      "|    total_timesteps | 1968     |\n",
+      "|    fps             | 495      |\n",
+      "|    time_elapsed    | 0        |\n",
+      "|    total_timesteps | 210      |\n",
       "---------------------------------\n",
       "---------------------------------\n",
       "| rollout/           |          |\n",
-      "|    ep_len_mean     | 92.2     |\n",
-      "|    ep_rew_mean     | -27.9    |\n",
+      "|    ep_len_mean     | 22.6     |\n",
+      "|    ep_rew_mean     | -40      |\n",
       "| time/              |          |\n",
       "|    episodes        | 40       |\n",
-      "|    fps             | 502      |\n",
-      "|    time_elapsed    | 8        |\n",
-      "|    total_timesteps | 4020     |\n",
+      "|    fps             | 589      |\n",
+      "|    time_elapsed    | 2        |\n",
+      "|    total_timesteps | 1398     |\n",
       "---------------------------------\n",
       "---------------------------------\n",
       "| rollout/           |          |\n",
-      "|    ep_len_mean     | 85.2     |\n",
-      "|    ep_rew_mean     | -30.4    |\n",
+      "|    ep_len_mean     | 23.2     |\n",
+      "|    ep_rew_mean     | -36.4    |\n",
       "| time/              |          |\n",
       "|    episodes        | 60       |\n",
-      "|    fps             | 509      |\n",
-      "|    time_elapsed    | 10       |\n",
-      "|    total_timesteps | 5384     |\n",
-      "---------------------------------\n",
-      "Eval num_timesteps=8000, episode_reward=-50.95 +/- 3.88\n",
-      "Episode length: 17.00 +/- 5.33\n",
-      "---------------------------------\n",
-      "| eval/              |          |\n",
-      "|    mean_ep_length  | 17       |\n",
-      "|    mean_reward     | -50.9    |\n",
-      "| time/              |          |\n",
-      "|    total_timesteps | 8000     |\n",
+      "|    fps             | 577      |\n",
+      "|    time_elapsed    | 2        |\n",
+      "|    total_timesteps | 1620     |\n",
       "---------------------------------\n",
-      "New best mean reward!\n",
       "---------------------------------\n",
       "| rollout/           |          |\n",
-      "|    ep_len_mean     | 99.8     |\n",
-      "|    ep_rew_mean     | -31.9    |\n",
+      "|    ep_len_mean     | 19.1     |\n",
+      "|    ep_rew_mean     | -39.5    |\n",
       "| time/              |          |\n",
       "|    episodes        | 80       |\n",
-      "|    fps             | 488      |\n",
-      "|    time_elapsed    | 16       |\n",
-      "|    total_timesteps | 8228     |\n",
+      "|    fps             | 573      |\n",
+      "|    time_elapsed    | 3        |\n",
+      "|    total_timesteps | 1938     |\n",
       "---------------------------------\n",
       "---------------------------------\n",
       "| rollout/           |          |\n",
-      "|    ep_len_mean     | 99.7     |\n",
-      "|    ep_rew_mean     | -35      |\n",
+      "|    ep_len_mean     | 18.6     |\n",
+      "|    ep_rew_mean     | -40.3    |\n",
       "| time/              |          |\n",
       "|    episodes        | 100      |\n",
-      "|    fps             | 495      |\n",
-      "|    time_elapsed    | 20       |\n",
-      "|    total_timesteps | 10092    |\n",
+      "|    fps             | 576      |\n",
+      "|    time_elapsed    | 3        |\n",
+      "|    total_timesteps | 2208     |\n",
       "---------------------------------\n",
       "---------------------------------\n",
       "| rollout/           |          |\n",
-      "|    ep_len_mean     | 95.4     |\n",
-      "|    ep_rew_mean     | -38.3    |\n",
+      "|    ep_len_mean     | 18.5     |\n",
+      "|    ep_rew_mean     | -40.5    |\n",
       "| time/              |          |\n",
       "|    episodes        | 120      |\n",
-      "|    fps             | 499      |\n",
-      "|    time_elapsed    | 23       |\n",
-      "|    total_timesteps | 11580    |\n",
+      "|    fps             | 572      |\n",
+      "|    time_elapsed    | 4        |\n",
+      "|    total_timesteps | 2400     |\n",
       "---------------------------------\n",
       "---------------------------------\n",
       "| rollout/           |          |\n",
-      "|    ep_len_mean     | 92.5     |\n",
-      "|    ep_rew_mean     | -42.4    |\n",
+      "|    ep_len_mean     | 13.5     |\n",
+      "|    ep_rew_mean     | -40.4    |\n",
       "| time/              |          |\n",
       "|    episodes        | 140      |\n",
-      "|    fps             | 503      |\n",
-      "|    time_elapsed    | 26       |\n",
-      "|    total_timesteps | 13224    |\n",
+      "|    fps             | 576      |\n",
+      "|    time_elapsed    | 4        |\n",
+      "|    total_timesteps | 2604     |\n",
       "---------------------------------\n",
       "---------------------------------\n",
       "| rollout/           |          |\n",
-      "|    ep_len_mean     | 98.3     |\n",
-      "|    ep_rew_mean     | -44      |\n",
+      "|    ep_len_mean     | 11.6     |\n",
+      "|    ep_rew_mean     | -43.6    |\n",
       "| time/              |          |\n",
       "|    episodes        | 160      |\n",
-      "|    fps             | 494      |\n",
-      "|    time_elapsed    | 30       |\n",
-      "|    total_timesteps | 15104    |\n",
-      "---------------------------------\n",
-      "Eval num_timesteps=16000, episode_reward=-41.16 +/- 10.08\n",
-      "Episode length: 127.00 +/- 89.41\n",
-      "---------------------------------\n",
-      "| eval/              |          |\n",
-      "|    mean_ep_length  | 127      |\n",
-      "|    mean_reward     | -41.2    |\n",
-      "| time/              |          |\n",
-      "|    total_timesteps | 16000    |\n",
+      "|    fps             | 578      |\n",
+      "|    time_elapsed    | 4        |\n",
+      "|    total_timesteps | 2760     |\n",
       "---------------------------------\n",
-      "New best mean reward!\n",
       "---------------------------------\n",
       "| rollout/           |          |\n",
-      "|    ep_len_mean     | 90       |\n",
-      "|    ep_rew_mean     | -44.4    |\n",
+      "|    ep_len_mean     | 17.7     |\n",
+      "|    ep_rew_mean     | -41.7    |\n",
       "| time/              |          |\n",
       "|    episodes        | 180      |\n",
-      "|    fps             | 455      |\n",
-      "|    time_elapsed    | 37       |\n",
-      "|    total_timesteps | 17172    |\n",
+      "|    fps             | 594      |\n",
+      "|    time_elapsed    | 6        |\n",
+      "|    total_timesteps | 3948     |\n",
       "---------------------------------\n",
       "---------------------------------\n",
       "| rollout/           |          |\n",
-      "|    ep_len_mean     | 88.2     |\n",
-      "|    ep_rew_mean     | -44.1    |\n",
+      "|    ep_len_mean     | 19.7     |\n",
+      "|    ep_rew_mean     | -35.3    |\n",
       "| time/              |          |\n",
       "|    episodes        | 200      |\n",
-      "|    fps             | 454      |\n",
-      "|    time_elapsed    | 41       |\n",
-      "|    total_timesteps | 18992    |\n",
+      "|    fps             | 596      |\n",
+      "|    time_elapsed    | 7        |\n",
+      "|    total_timesteps | 4194     |\n",
       "---------------------------------\n",
       "---------------------------------\n",
       "| rollout/           |          |\n",
-      "|    ep_len_mean     | 90       |\n",
-      "|    ep_rew_mean     | -41.5    |\n",
+      "|    ep_len_mean     | 21.9     |\n",
+      "|    ep_rew_mean     | -26.5    |\n",
       "| time/              |          |\n",
       "|    episodes        | 220      |\n",
-      "|    fps             | 458      |\n",
-      "|    time_elapsed    | 44       |\n",
-      "|    total_timesteps | 20512    |\n",
+      "|    fps             | 598      |\n",
+      "|    time_elapsed    | 7        |\n",
+      "|    total_timesteps | 4470     |\n",
       "---------------------------------\n",
       "---------------------------------\n",
       "| rollout/           |          |\n",
-      "|    ep_len_mean     | 90.9     |\n",
-      "|    ep_rew_mean     | -38.7    |\n",
+      "|    ep_len_mean     | 22       |\n",
+      "|    ep_rew_mean     | -29.2    |\n",
       "| time/              |          |\n",
       "|    episodes        | 240      |\n",
-      "|    fps             | 461      |\n",
-      "|    time_elapsed    | 48       |\n",
-      "|    total_timesteps | 22248    |\n",
-      "---------------------------------\n",
-      "Eval num_timesteps=24000, episode_reward=-47.28 +/- 8.29\n",
-      "Episode length: 56.60 +/- 71.96\n",
-      "---------------------------------\n",
-      "| eval/              |          |\n",
-      "|    mean_ep_length  | 56.6     |\n",
-      "|    mean_reward     | -47.3    |\n",
-      "| time/              |          |\n",
-      "|    total_timesteps | 24000    |\n",
+      "|    fps             | 587      |\n",
+      "|    time_elapsed    | 8        |\n",
+      "|    total_timesteps | 4752     |\n",
       "---------------------------------\n",
       "---------------------------------\n",
       "| rollout/           |          |\n",
-      "|    ep_len_mean     | 96.2     |\n",
-      "|    ep_rew_mean     | -38.9    |\n",
+      "|    ep_len_mean     | 24       |\n",
+      "|    ep_rew_mean     | -28.4    |\n",
       "| time/              |          |\n",
       "|    episodes        | 260      |\n",
-      "|    fps             | 455      |\n",
-      "|    time_elapsed    | 54       |\n",
-      "|    total_timesteps | 24784    |\n",
+      "|    fps             | 579      |\n",
+      "|    time_elapsed    | 9        |\n",
+      "|    total_timesteps | 5484     |\n",
       "---------------------------------\n",
       "---------------------------------\n",
       "| rollout/           |          |\n",
-      "|    ep_len_mean     | 88.7     |\n",
-      "|    ep_rew_mean     | -40.5    |\n",
+      "|    ep_len_mean     | 19.7     |\n",
+      "|    ep_rew_mean     | -29.4    |\n",
       "| time/              |          |\n",
       "|    episodes        | 280      |\n",
-      "|    fps             | 456      |\n",
-      "|    time_elapsed    | 56       |\n",
-      "|    total_timesteps | 25936    |\n",
+      "|    fps             | 580      |\n",
+      "|    time_elapsed    | 9        |\n",
+      "|    total_timesteps | 5760     |\n",
       "---------------------------------\n",
       "---------------------------------\n",
       "| rollout/           |          |\n",
-      "|    ep_len_mean     | 87.9     |\n",
-      "|    ep_rew_mean     | -40.1    |\n",
+      "|    ep_len_mean     | 21.8     |\n",
+      "|    ep_rew_mean     | -29.2    |\n",
       "| time/              |          |\n",
       "|    episodes        | 300      |\n",
-      "|    fps             | 457      |\n",
-      "|    time_elapsed    | 61       |\n",
-      "|    total_timesteps | 27976    |\n",
+      "|    fps             | 580      |\n",
+      "|    time_elapsed    | 11       |\n",
+      "|    total_timesteps | 6468     |\n",
       "---------------------------------\n",
       "---------------------------------\n",
       "| rollout/           |          |\n",
-      "|    ep_len_mean     | 101      |\n",
-      "|    ep_rew_mean     | -40.6    |\n",
+      "|    ep_len_mean     | 21.3     |\n",
+      "|    ep_rew_mean     | -37.5    |\n",
       "| time/              |          |\n",
       "|    episodes        | 320      |\n",
-      "|    fps             | 459      |\n",
-      "|    time_elapsed    | 66       |\n",
-      "|    total_timesteps | 30712    |\n",
-      "---------------------------------\n",
-      "Eval num_timesteps=32000, episode_reward=-43.29 +/- 9.81\n",
-      "Episode length: 58.40 +/- 71.01\n",
-      "---------------------------------\n",
-      "| eval/              |          |\n",
-      "|    mean_ep_length  | 58.4     |\n",
-      "|    mean_reward     | -43.3    |\n",
-      "| time/              |          |\n",
-      "|    total_timesteps | 32000    |\n",
+      "|    fps             | 579      |\n",
+      "|    time_elapsed    | 11       |\n",
+      "|    total_timesteps | 6894     |\n",
       "---------------------------------\n",
       "---------------------------------\n",
       "| rollout/           |          |\n",
-      "|    ep_len_mean     | 105      |\n",
-      "|    ep_rew_mean     | -43.4    |\n",
+      "|    ep_len_mean     | 24.8     |\n",
+      "|    ep_rew_mean     | -36.5    |\n",
       "| time/              |          |\n",
       "|    episodes        | 340      |\n",
-      "|    fps             | 455      |\n",
-      "|    time_elapsed    | 71       |\n",
-      "|    total_timesteps | 32560    |\n",
+      "|    fps             | 579      |\n",
+      "|    time_elapsed    | 12       |\n",
+      "|    total_timesteps | 7194     |\n",
       "---------------------------------\n",
       "---------------------------------\n",
       "| rollout/           |          |\n",
-      "|    ep_len_mean     | 94.1     |\n",
-      "|    ep_rew_mean     | -44.1    |\n",
+      "|    ep_len_mean     | 23.1     |\n",
+      "|    ep_rew_mean     | -37.4    |\n",
       "| time/              |          |\n",
       "|    episodes        | 360      |\n",
-      "|    fps             | 456      |\n",
-      "|    time_elapsed    | 75       |\n",
-      "|    total_timesteps | 34276    |\n",
+      "|    fps             | 578      |\n",
+      "|    time_elapsed    | 12       |\n",
+      "|    total_timesteps | 7380     |\n",
       "---------------------------------\n",
       "---------------------------------\n",
       "| rollout/           |          |\n",
-      "|    ep_len_mean     | 96.7     |\n",
-      "|    ep_rew_mean     | -44      |\n",
+      "|    ep_len_mean     | 21.6     |\n",
+      "|    ep_rew_mean     | -38.3    |\n",
       "| time/              |          |\n",
       "|    episodes        | 380      |\n",
-      "|    fps             | 459      |\n",
-      "|    time_elapsed    | 77       |\n",
-      "|    total_timesteps | 35696    |\n",
+      "|    fps             | 577      |\n",
+      "|    time_elapsed    | 13       |\n",
+      "|    total_timesteps | 7812     |\n",
       "---------------------------------\n",
       "---------------------------------\n",
       "| rollout/           |          |\n",
-      "|    ep_len_mean     | 99.6     |\n",
-      "|    ep_rew_mean     | -44.3    |\n",
+      "|    ep_len_mean     | 21.6     |\n",
+      "|    ep_rew_mean     | -29.8    |\n",
       "| time/              |          |\n",
       "|    episodes        | 400      |\n",
-      "|    fps             | 460      |\n",
-      "|    time_elapsed    | 82       |\n",
-      "|    total_timesteps | 37876    |\n",
-      "---------------------------------\n",
-      "Eval num_timesteps=40000, episode_reward=-24.95 +/- 41.75\n",
-      "Episode length: 92.40 +/- 87.95\n",
-      "---------------------------------\n",
-      "| eval/              |          |\n",
-      "|    mean_ep_length  | 92.4     |\n",
-      "|    mean_reward     | -24.9    |\n",
-      "| time/              |          |\n",
-      "|    total_timesteps | 40000    |\n",
+      "|    fps             | 580      |\n",
+      "|    time_elapsed    | 14       |\n",
+      "|    total_timesteps | 8664     |\n",
       "---------------------------------\n",
-      "New best mean reward!\n",
       "---------------------------------\n",
       "| rollout/           |          |\n",
-      "|    ep_len_mean     | 94.9     |\n",
-      "|    ep_rew_mean     | -36.7    |\n",
+      "|    ep_len_mean     | 24.4     |\n",
+      "|    ep_rew_mean     | -27.9    |\n",
       "| time/              |          |\n",
       "|    episodes        | 420      |\n",
-      "|    fps             | 416      |\n",
-      "|    time_elapsed    | 96       |\n",
-      "|    total_timesteps | 40360    |\n",
-      "| train/             |          |\n",
-      "|    actor_loss      | -7.7     |\n",
-      "|    critic_loss     | 15.7     |\n",
-      "|    ent_coef        | 0.899    |\n",
-      "|    ent_coef_loss   | -0.89    |\n",
-      "|    learning_rate   | 0.0003   |\n",
-      "|    n_updates       | 356      |\n",
+      "|    fps             | 580      |\n",
+      "|    time_elapsed    | 15       |\n",
+      "|    total_timesteps | 9000     |\n",
       "---------------------------------\n",
       "---------------------------------\n",
       "| rollout/           |          |\n",
-      "|    ep_len_mean     | 91.6     |\n",
-      "|    ep_rew_mean     | -30.8    |\n",
+      "|    ep_len_mean     | 24.6     |\n",
+      "|    ep_rew_mean     | -27.3    |\n",
       "| time/              |          |\n",
       "|    episodes        | 440      |\n",
-      "|    fps             | 304      |\n",
-      "|    time_elapsed    | 137      |\n",
-      "|    total_timesteps | 41960    |\n",
-      "| train/             |          |\n",
-      "|    actor_loss      | -17.6    |\n",
-      "|    critic_loss     | 25.1     |\n",
-      "|    ent_coef        | 0.557    |\n",
-      "|    ent_coef_loss   | -4.92    |\n",
-      "|    learning_rate   | 0.0003   |\n",
-      "|    n_updates       | 1956     |\n",
+      "|    fps             | 582      |\n",
+      "|    time_elapsed    | 17       |\n",
+      "|    total_timesteps | 9906     |\n",
       "---------------------------------\n",
       "---------------------------------\n",
       "| rollout/           |          |\n",
-      "|    ep_len_mean     | 95.7     |\n",
-      "|    ep_rew_mean     | -30.9    |\n",
+      "|    ep_len_mean     | 27.4     |\n",
+      "|    ep_rew_mean     | -18.9    |\n",
       "| time/              |          |\n",
       "|    episodes        | 460      |\n",
-      "|    fps             | 233      |\n",
-      "|    time_elapsed    | 188      |\n",
-      "|    total_timesteps | 43876    |\n",
-      "| train/             |          |\n",
-      "|    actor_loss      | -22.4    |\n",
-      "|    critic_loss     | 25       |\n",
-      "|    ent_coef        | 0.317    |\n",
-      "|    ent_coef_loss   | -8.85    |\n",
-      "|    learning_rate   | 0.0003   |\n",
-      "|    n_updates       | 3872     |\n",
+      "|    fps             | 582      |\n",
+      "|    time_elapsed    | 17       |\n",
+      "|    total_timesteps | 10206    |\n",
       "---------------------------------\n",
       "---------------------------------\n",
       "| rollout/           |          |\n",
-      "|    ep_len_mean     | 100      |\n",
-      "|    ep_rew_mean     | -12.7    |\n",
+      "|    ep_len_mean     | 27.5     |\n",
+      "|    ep_rew_mean     | -18.4    |\n",
       "| time/              |          |\n",
       "|    episodes        | 480      |\n",
-      "|    fps             | 191      |\n",
-      "|    time_elapsed    | 238      |\n",
-      "|    total_timesteps | 45808    |\n",
-      "| train/             |          |\n",
-      "|    actor_loss      | -23.8    |\n",
-      "|    critic_loss     | 4.18     |\n",
-      "|    ent_coef        | 0.182    |\n",
-      "|    ent_coef_loss   | -10.4    |\n",
-      "|    learning_rate   | 0.0003   |\n",
-      "|    n_updates       | 5804     |\n",
+      "|    fps             | 580      |\n",
+      "|    time_elapsed    | 18       |\n",
+      "|    total_timesteps | 10458    |\n",
+      "---------------------------------\n",
       "---------------------------------\n",
-      "Eval num_timesteps=48000, episode_reward=-37.06 +/- 13.40\n",
-      "Episode length: 200.00 +/- 0.00\n",
+      "| rollout/           |          |\n",
+      "|    ep_len_mean     | 23.3     |\n",
+      "|    ep_rew_mean     | -33.9    |\n",
+      "| time/              |          |\n",
+      "|    episodes        | 500      |\n",
+      "|    fps             | 580      |\n",
+      "|    time_elapsed    | 18       |\n",
+      "|    total_timesteps | 10740    |\n",
+      "---------------------------------\n",
+      "---------------------------------\n",
+      "| rollout/           |          |\n",
+      "|    ep_len_mean     | 22.2     |\n",
+      "|    ep_rew_mean     | -35.5    |\n",
+      "| time/              |          |\n",
+      "|    episodes        | 520      |\n",
+      "|    fps             | 581      |\n",
+      "|    time_elapsed    | 19       |\n",
+      "|    total_timesteps | 11388    |\n",
+      "---------------------------------\n",
+      "Eval num_timesteps=12000, episode_reward=-42.82 +/- 14.96\n",
+      "Episode length: 43.00 +/- 78.51\n",
       "---------------------------------\n",
       "| eval/              |          |\n",
-      "|    mean_ep_length  | 200      |\n",
-      "|    mean_reward     | -37.1    |\n",
+      "|    mean_ep_length  | 43       |\n",
+      "|    mean_reward     | -42.8    |\n",
       "| time/              |          |\n",
-      "|    total_timesteps | 48000    |\n",
-      "| train/             |          |\n",
-      "|    actor_loss      | -20.1    |\n",
-      "|    critic_loss     | 8.23     |\n",
-      "|    ent_coef        | 0.0977   |\n",
-      "|    ent_coef_loss   | -13.9    |\n",
-      "|    learning_rate   | 0.0003   |\n",
-      "|    n_updates       | 7996     |\n",
+      "|    total_timesteps | 12000    |\n",
       "---------------------------------\n",
+      "New best mean reward!\n",
       "---------------------------------\n",
       "| rollout/           |          |\n",
-      "|    ep_len_mean     | 114      |\n",
-      "|    ep_rew_mean     | -2.58    |\n",
+      "|    ep_len_mean     | 22.6     |\n",
+      "|    ep_rew_mean     | -27.3    |\n",
       "| time/              |          |\n",
-      "|    episodes        | 500      |\n",
-      "|    fps             | 148      |\n",
-      "|    time_elapsed    | 330      |\n",
-      "|    total_timesteps | 49132    |\n",
-      "| train/             |          |\n",
-      "|    actor_loss      | -19.4    |\n",
-      "|    critic_loss     | 6.48     |\n",
-      "|    ent_coef        | 0.0746   |\n",
-      "|    ent_coef_loss   | -10.9    |\n",
-      "|    learning_rate   | 0.0003   |\n",
-      "|    n_updates       | 9128     |\n",
+      "|    episodes        | 540      |\n",
+      "|    fps             | 553      |\n",
+      "|    time_elapsed    | 21       |\n",
+      "|    total_timesteps | 12042    |\n",
       "---------------------------------\n",
       "---------------------------------\n",
       "| rollout/           |          |\n",
-      "|    ep_len_mean     | 117      |\n",
-      "|    ep_rew_mean     | -9.55    |\n",
+      "|    ep_len_mean     | 21.9     |\n",
+      "|    ep_rew_mean     | -31.4    |\n",
       "| time/              |          |\n",
-      "|    episodes        | 520      |\n",
-      "|    fps             | 128      |\n",
-      "|    time_elapsed    | 402      |\n",
-      "|    total_timesteps | 51840    |\n",
-      "| train/             |          |\n",
-      "|    actor_loss      | -17.6    |\n",
-      "|    critic_loss     | 2.88     |\n",
-      "|    ent_coef        | 0.048    |\n",
-      "|    ent_coef_loss   | -3.41    |\n",
-      "|    learning_rate   | 0.0003   |\n",
-      "|    n_updates       | 11836    |\n",
+      "|    episodes        | 560      |\n",
+      "|    fps             | 553      |\n",
+      "|    time_elapsed    | 22       |\n",
+      "|    total_timesteps | 12528    |\n",
       "---------------------------------\n",
-      "Saving last checkpoint\n",
-      "Last checkpoint saved in: ./results/darm_sf_hand/test4_SF_SB3_SAC/models/last_model\n"
-     ]
-    },
-    {
-     "ename": "KeyboardInterrupt",
-     "evalue": "",
-     "output_type": "error",
-     "traceback": [
-      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
-      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
-      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m----> 2\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtotal_timesteps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlog_interval\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m                \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mException caught:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
-      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/stable_baselines3/sac/sac.py:309\u001b[0m, in \u001b[0;36mSAC.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    295\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[1;32m    296\u001b[0m     \u001b[38;5;28mself\u001b[39m: SACSelf,\n\u001b[1;32m    297\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    306\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    307\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SACSelf:\n\u001b[0;32m--> 309\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_env\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_eval_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_eval_episodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_log_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_log_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
-      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/stable_baselines3/common/off_policy_algorithm.py:375\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    373\u001b[0m         \u001b[38;5;66;03m# Special case when the user passes `gradient_steps=0`\u001b[39;00m\n\u001b[1;32m    374\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m gradient_steps \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 375\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgradient_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    377\u001b[0m callback\u001b[38;5;241m.\u001b[39mon_training_end()\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
-      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/stable_baselines3/sac/sac.py:265\u001b[0m, in \u001b[0;36mSAC.train\u001b[0;34m(self, gradient_steps, batch_size)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcritic\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m    264\u001b[0m critic_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m--> 265\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcritic\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[38;5;66;03m# Compute actor loss\u001b[39;00m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;66;03m# Alternative: actor_loss = th.mean(log_prob - qf1_pi)\u001b[39;00m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;66;03m# Min over all critic networks\u001b[39;00m\n\u001b[1;32m    270\u001b[0m q_values_pi \u001b[38;5;241m=\u001b[39m th\u001b[38;5;241m.\u001b[39mcat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcritic(replay_data\u001b[38;5;241m.\u001b[39mobservations, actions_pi), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
-      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/optim/optimizer.py:140\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m profile_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimizer.step#\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.step\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m--> 140\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m     obj\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
-      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/optim/optimizer.py:23\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     22\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m---> 23\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     25\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(prev_grad)\n",
-      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/optim/adam.py:234\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure, grad_scaler)\u001b[0m\n\u001b[1;32m    231\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`requires_grad` is not supported for `step` in differentiable mode\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    232\u001b[0m             state_steps\u001b[38;5;241m.\u001b[39mappend(state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstep\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m--> 234\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[43m         \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[43m         \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m         \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[43m         \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[43m         \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    240\u001b[0m \u001b[43m         \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    241\u001b[0m \u001b[43m         \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    242\u001b[0m \u001b[43m         \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[43m         \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    244\u001b[0m \u001b[43m         \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[43m         \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[43m         \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[43m         \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[43m         \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[43m         \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[43m         \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m         \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m         \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
-      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/optim/adam.py:300\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    298\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 300\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    301\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    302\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    303\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    304\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[43m     \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    306\u001b[0m \u001b[43m     \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    307\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    308\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m     \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[43m     \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m     \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m     \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m     \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m)\u001b[49m\n",
-      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/optim/adam.py:412\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    409\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    410\u001b[0m     denom \u001b[38;5;241m=\u001b[39m (exp_avg_sq\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[0;32m--> 412\u001b[0m \u001b[43mparam\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maddcdiv_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexp_avg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdenom\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mstep_size\u001b[49m\u001b[43m)\u001b[49m\n",
-      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
-     ]
-    }
-   ],
-   "source": [
-    "try:\n",
-    "    model.learn(total_timesteps=config[\"total_timesteps\"], \n",
-    "                log_interval=config[\"log_interval\"], \n",
-    "                tb_log_name=run_name,\n",
-    "                callback=callback)\n",
-    "except Exception as e:\n",
-    "    print(\"Exception caught:\")\n",
-    "    print(e)\n",
-    "finally:\n",
-    "    # timestamp = f\"{datetime.now().date()}__{datetime.now().time()}\"\n",
-    "    print(\"Saving last checkpoint\")\n",
-    "    model_name = f\"./results/darm_sf_hand/{run_name}/models/last_model\"\n",
-    "    env_norm_name = f\"./results/darm_sf_hand/{run_name}/env_norm\"\n",
-    "    model.save(model_name)\n",
-    "    print(f\"Last checkpoint saved in: {model_name}\")\n",
-    "    # env.save(env_norm_name) # FIXME: Remember to save norm params if using VecNorm env\n",
-    "    "
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": 9,
-   "id": "139daf70-71ae-4b08-8798-a9e86b0a87a0",
-   "metadata": {
-    "collapsed": true,
-    "jupyter": {
-     "outputs_hidden": true
-    },
-    "tags": []
-   },
-   "outputs": [
-    {
-     "data": {
-      "text/html": [
-       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
-      ],
-      "text/plain": [
-       "<IPython.core.display.HTML object>"
-      ]
-     },
-     "metadata": {},
-     "output_type": "display_data"
-    },
-    {
-     "data": {
-      "text/html": [
-       "<style>\n",
-       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
-       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
-       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
-       "    </style>\n",
-       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/mean_ep_length</td><td>▁▅▃▃▄█</td></tr><tr><td>eval/mean_reward</td><td>▁▄▂▃█▅</td></tr><tr><td>global_step</td><td>▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇██</td></tr><tr><td>rollout/ep_len_mean</td><td>▂▃▁▄▄▃▃▄▂▂▂▂▃▂▂▅▅▃▄▄▃▂▃▄▇█</td></tr><tr><td>rollout/ep_rew_mean</td><td>▃▄▃▃▃▂▁▁▁▁▁▂▂▂▂▂▁▁▁▁▂▃▃▆█▇</td></tr><tr><td>time/fps</td><td>████████▇▇▇▇▇▇▇▇▇▇▇▇▆▄▃▂▁▁</td></tr><tr><td>train/actor_loss</td><td>█▄▂▁▃▃▄</td></tr><tr><td>train/critic_loss</td><td>▅██▁▃▂▁</td></tr><tr><td>train/ent_coef</td><td>█▅▃▂▁▁▁</td></tr><tr><td>train/ent_coef_loss</td><td>█▆▄▃▁▃▇</td></tr><tr><td>train/learning_rate</td><td>▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/mean_ep_length</td><td>200.0</td></tr><tr><td>eval/mean_reward</td><td>-37.05579</td></tr><tr><td>global_step</td><td>51840</td></tr><tr><td>rollout/ep_len_mean</td><td>116.95</td></tr><tr><td>rollout/ep_rew_mean</td><td>-9.54759</td></tr><tr><td>time/fps</td><td>128.0</td></tr><tr><td>train/actor_loss</td><td>-17.60356</td></tr><tr><td>train/critic_loss</td><td>2.88043</td></tr><tr><td>train/ent_coef</td><td>0.04804</td></tr><tr><td>train/ent_coef_loss</td><td>-3.41416</td></tr><tr><td>train/learning_rate</td><td>0.0003</td></tr></table><br/></div></div>"
-      ],
-      "text/plain": [
-       "<IPython.core.display.HTML object>"
-      ]
-     },
-     "metadata": {},
-     "output_type": "display_data"
-    },
-    {
-     "data": {
-      "text/html": [
-       " View run <strong style=\"color:#cdcd00\">test4_SF_SB3_SAC</strong> at: <a href='https://wandb.ai/danieladejumo/DARM/runs/fwqpzibt' target=\"_blank\">https://wandb.ai/danieladejumo/DARM/runs/fwqpzibt</a><br/>Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 3 other file(s)"
-      ],
-      "text/plain": [
-       "<IPython.core.display.HTML object>"
-      ]
-     },
-     "metadata": {},
-     "output_type": "display_data"
-    },
-    {
-     "data": {
-      "text/html": [
-       "Find logs at: <code>./wandb/run-20230221_162741-fwqpzibt/logs</code>"
-      ],
-      "text/plain": [
-       "<IPython.core.display.HTML object>"
-      ]
-     },
-     "metadata": {},
-     "output_type": "display_data"
-    },
+      "---------------------------------\n",
+      "| rollout/           |          |\n",
+      "|    ep_len_mean     | 31.2     |\n",
+      "|    ep_rew_mean     | -0.488   |\n",
+      "| time/              |          |\n",
+      "|    episodes        | 580      |\n",
+      "|    fps             | 545      |\n",
+      "|    time_elapsed    | 24       |\n",
+      "|    total_timesteps | 13620    |\n",
+      "---------------------------------\n",
+      "---------------------------------\n",
+      "| rollout/           |          |\n",
+      "|    ep_len_mean     | 35       |\n",
+      "|    ep_rew_mean     | 0.27     |\n",
+      "| time/              |          |\n",
+      "|    episodes        | 600      |\n",
+      "|    fps             | 548      |\n",
+      "|    time_elapsed    | 26       |\n",
+      "|    total_timesteps | 14520    |\n",
+      "---------------------------------\n",
+      "---------------------------------\n",
+      "| rollout/           |          |\n",
+      "|    ep_len_mean     | 35.5     |\n",
+      "|    ep_rew_mean     | 9.08     |\n",
+      "| time/              |          |\n",
+      "|    episodes        | 620      |\n",
+      "|    fps             | 548      |\n",
+      "|    time_elapsed    | 27       |\n",
+      "|    total_timesteps | 14898    |\n",
+      "---------------------------------\n",
+      "---------------------------------\n",
+      "| rollout/           |          |\n",
+      "|    ep_len_mean     | 30.3     |\n",
+      "|    ep_rew_mean     | 2.21     |\n",
+      "| time/              |          |\n",
+      "|    episodes        | 640      |\n",
+      "|    fps             | 549      |\n",
+      "|    time_elapsed    | 27       |\n",
+      "|    total_timesteps | 15150    |\n",
+      "---------------------------------\n",
+      "---------------------------------\n",
+      "| rollout/           |          |\n",
+      "|    ep_len_mean     | 30.6     |\n",
+      "|    ep_rew_mean     | 1.14     |\n",
+      "| time/              |          |\n",
+      "|    episodes        | 660      |\n",
+      "|    fps             | 549      |\n",
+      "|    time_elapsed    | 28       |\n",
+      "|    total_timesteps | 15690    |\n",
+      "---------------------------------\n",
+      "---------------------------------\n",
+      "| rollout/           |          |\n",
+      "|    ep_len_mean     | 26.6     |\n",
+      "|    ep_rew_mean     | -18.6    |\n",
+      "| time/              |          |\n",
+      "|    episodes        | 680      |\n",
+      "|    fps             | 549      |\n",
+      "|    time_elapsed    | 29       |\n",
+      "|    total_timesteps | 16386    |\n",
+      "---------------------------------\n",
+      "---------------------------------\n",
+      "| rollout/           |          |\n",
+      "|    ep_len_mean     | 23       |\n",
+      "|    ep_rew_mean     | -17.2    |\n",
+      "| time/              |          |\n",
+      "|    episodes        | 700      |\n",
+      "|    fps             | 549      |\n",
+      "|    time_elapsed    | 30       |\n",
+      "|    total_timesteps | 16854    |\n",
+      "---------------------------------\n",
+      "---------------------------------\n",
+      "| rollout/           |          |\n",
+      "|    ep_len_mean     | 24.1     |\n",
+      "|    ep_rew_mean     | -26.2    |\n",
+      "| time/              |          |\n",
+      "|    episodes        | 720      |\n",
+      "|    fps             | 548      |\n",
+      "|    time_elapsed    | 31       |\n",
+      "|    total_timesteps | 17130    |\n",
+      "---------------------------------\n",
+      "---------------------------------\n",
+      "| rollout/           |          |\n",
+      "|    ep_len_mean     | 25.2     |\n",
+      "|    ep_rew_mean     | -29.5    |\n",
+      "| time/              |          |\n",
+      "|    episodes        | 740      |\n",
+      "|    fps             | 549      |\n",
+      "|    time_elapsed    | 31       |\n",
+      "|    total_timesteps | 17436    |\n",
+      "---------------------------------\n",
+      "---------------------------------\n",
+      "| rollout/           |          |\n",
+      "|    ep_len_mean     | 23.2     |\n",
+      "|    ep_rew_mean     | -32.6    |\n",
+      "| time/              |          |\n",
+      "|    episodes        | 760      |\n",
+      "|    fps             | 549      |\n",
+      "|    time_elapsed    | 32       |\n",
+      "|    total_timesteps | 18114    |\n",
+      "---------------------------------\n",
+      "---------------------------------\n",
+      "| rollout/           |          |\n",
+      "|    ep_len_mean     | 22.6     |\n",
+      "|    ep_rew_mean     | -43      |\n",
+      "| time/              |          |\n",
+      "|    episodes        | 780      |\n",
+      "|    fps             | 550      |\n",
+      "|    time_elapsed    | 33       |\n",
+      "|    total_timesteps | 18666    |\n",
+      "---------------------------------\n",
+      "---------------------------------\n",
+      "| rollout/           |          |\n",
+      "|    ep_len_mean     | 24.4     |\n",
+      "|    ep_rew_mean     | -34.8    |\n",
+      "| time/              |          |\n",
+      "|    episodes        | 800      |\n",
+      "|    fps             | 551      |\n",
+      "|    time_elapsed    | 34       |\n",
+      "|    total_timesteps | 19014    |\n",
+      "---------------------------------\n",
+      "---------------------------------\n",
+      "| rollout/           |          |\n",
+      "|    ep_len_mean     | 23.9     |\n",
+      "|    ep_rew_mean     | -34.8    |\n",
+      "| time/              |          |\n",
+      "|    episodes        | 820      |\n",
+      "|    fps             | 552      |\n",
+      "|    time_elapsed    | 36       |\n",
+      "|    total_timesteps | 19926    |\n",
+      "---------------------------------\n",
+      "---------------------------------\n",
+      "| rollout/           |          |\n",
+      "|    ep_len_mean     | 26.7     |\n",
+      "|    ep_rew_mean     | -22.9    |\n",
+      "| time/              |          |\n",
+      "|    episodes        | 840      |\n",
+      "|    fps             | 552      |\n",
+      "|    time_elapsed    | 37       |\n",
+      "|    total_timesteps | 20496    |\n",
+      "---------------------------------\n",
+      "---------------------------------\n",
+      "| rollout/           |          |\n",
+      "|    ep_len_mean     | 32.7     |\n",
+      "|    ep_rew_mean     | -19.5    |\n",
+      "| time/              |          |\n",
+      "|    episodes        | 860      |\n",
+      "|    fps             | 553      |\n",
+      "|    time_elapsed    | 38       |\n",
+      "|    total_timesteps | 21138    |\n",
+      "---------------------------------\n",
+      "---------------------------------\n",
+      "| rollout/           |          |\n",
+      "|    ep_len_mean     | 29.6     |\n",
+      "|    ep_rew_mean     | -13      |\n",
+      "| time/              |          |\n",
+      "|    episodes        | 880      |\n",
+      "|    fps             | 554      |\n",
+      "|    time_elapsed    | 38       |\n",
+      "|    total_timesteps | 21486    |\n",
+      "---------------------------------\n",
+      "---------------------------------\n",
+      "| rollout/           |          |\n",
+      "|    ep_len_mean     | 31.4     |\n",
+      "|    ep_rew_mean     | -20.5    |\n",
+      "| time/              |          |\n",
+      "|    episodes        | 900      |\n",
+      "|    fps             | 552      |\n",
+      "|    time_elapsed    | 40       |\n",
+      "|    total_timesteps | 22434    |\n",
+      "---------------------------------\n",
+      "---------------------------------\n",
+      "| rollout/           |          |\n",
+      "|    ep_len_mean     | 31       |\n",
+      "|    ep_rew_mean     | -20.2    |\n",
+      "| time/              |          |\n",
+      "|    episodes        | 920      |\n",
+      "|    fps             | 552      |\n",
+      "|    time_elapsed    | 41       |\n",
+      "|    total_timesteps | 22650    |\n",
+      "---------------------------------\n",
+      "---------------------------------\n",
+      "| rollout/           |          |\n",
+      "|    ep_len_mean     | 28.7     |\n",
+      "|    ep_rew_mean     | -32.2    |\n",
+      "| time/              |          |\n",
+      "|    episodes        | 940      |\n",
+      "|    fps             | 550      |\n",
+      "|    time_elapsed    | 41       |\n",
+      "|    total_timesteps | 22908    |\n",
+      "---------------------------------\n",
+      "---------------------------------\n",
+      "| rollout/           |          |\n",
+      "|    ep_len_mean     | 21.6     |\n",
+      "|    ep_rew_mean     | -36      |\n",
+      "| time/              |          |\n",
+      "|    episodes        | 960      |\n",
+      "|    fps             | 548      |\n",
+      "|    time_elapsed    | 42       |\n",
+      "|    total_timesteps | 23100    |\n",
+      "---------------------------------\n",
+      "---------------------------------\n",
+      "| rollout/           |          |\n",
+      "|    ep_len_mean     | 19.4     |\n",
+      "|    ep_rew_mean     | -43.5    |\n",
+      "| time/              |          |\n",
+      "|    episodes        | 980      |\n",
+      "|    fps             | 548      |\n",
+      "|    time_elapsed    | 43       |\n",
+      "|    total_timesteps | 23658    |\n",
+      "---------------------------------\n",
+      "Eval num_timesteps=24000, episode_reward=-47.14 +/- 3.87\n",
+      "Episode length: 4.40 +/- 1.36\n",
+      "---------------------------------\n",
+      "| eval/              |          |\n",
+      "|    mean_ep_length  | 4.4      |\n",
+      "|    mean_reward     | -47.1    |\n",
+      "| time/              |          |\n",
+      "|    total_timesteps | 24000    |\n",
+      "---------------------------------\n",
+      "---------------------------------\n",
+      "| rollout/           |          |\n",
+      "|    ep_len_mean     | 23.4     |\n",
+      "|    ep_rew_mean     | -25      |\n",
+      "| time/              |          |\n",
+      "|    episodes        | 1000     |\n",
+      "|    fps             | 546      |\n",
+      "|    time_elapsed    | 44       |\n",
+      "|    total_timesteps | 24510    |\n",
+      "---------------------------------\n",
+      "---------------------------------\n",
+      "| rollout/           |          |\n",
+      "|    ep_len_mean     | 19.5     |\n",
+      "|    ep_rew_mean     | -25.6    |\n",
+      "| time/              |          |\n",
+      "|    episodes        | 1020     |\n",
+      "|    fps             | 545      |\n",
+      "|    time_elapsed    | 45       |\n",
+      "|    total_timesteps | 24852    |\n",
+      "---------------------------------\n",
+      "---------------------------------\n",
+      "| rollout/           |          |\n",
+      "|    ep_len_mean     | 20.1     |\n",
+      "|    ep_rew_mean     | -24.6    |\n",
+      "| time/              |          |\n",
+      "|    episodes        | 1040     |\n",
+      "|    fps             | 545      |\n",
+      "|    time_elapsed    | 46       |\n",
+      "|    total_timesteps | 25194    |\n",
+      "---------------------------------\n",
+      "---------------------------------\n",
+      "| rollout/           |          |\n",
+      "|    ep_len_mean     | 23.1     |\n",
+      "|    ep_rew_mean     | -24      |\n",
+      "| time/              |          |\n",
+      "|    episodes        | 1060     |\n",
+      "|    fps             | 546      |\n",
+      "|    time_elapsed    | 46       |\n",
+      "|    total_timesteps | 25614    |\n",
+      "---------------------------------\n",
+      "---------------------------------\n",
+      "| rollout/           |          |\n",
+      "|    ep_len_mean     | 24       |\n",
+      "|    ep_rew_mean     | -23.6    |\n",
+      "| time/              |          |\n",
+      "|    episodes        | 1080     |\n",
+      "|    fps             | 546      |\n",
+      "|    time_elapsed    | 47       |\n",
+      "|    total_timesteps | 25974    |\n",
+      "---------------------------------\n",
+      "---------------------------------\n",
+      "| rollout/           |          |\n",
+      "|    ep_len_mean     | 20.6     |\n",
+      "|    ep_rew_mean     | -44.1    |\n",
+      "| time/              |          |\n",
+      "|    episodes        | 1100     |\n",
+      "|    fps             | 547      |\n",
+      "|    time_elapsed    | 48       |\n",
+      "|    total_timesteps | 26424    |\n",
+      "---------------------------------\n",
+      "---------------------------------\n",
+      "| rollout/           |          |\n",
+      "|    ep_len_mean     | 28.4     |\n",
+      "|    ep_rew_mean     | -36      |\n",
+      "| time/              |          |\n",
+      "|    episodes        | 1120     |\n",
+      "|    fps             | 550      |\n",
+      "|    time_elapsed    | 49       |\n",
+      "|    total_timesteps | 27504    |\n",
+      "---------------------------------\n",
+      "---------------------------------\n",
+      "| rollout/           |          |\n",
+      "|    ep_len_mean     | 26.4     |\n",
+      "|    ep_rew_mean     | -31.4    |\n",
+      "| time/              |          |\n",
+      "|    episodes        | 1140     |\n",
+      "|    fps             | 550      |\n",
+      "|    time_elapsed    | 50       |\n",
+      "|    total_timesteps | 27768    |\n",
+      "---------------------------------\n",
+      "---------------------------------\n",
+      "| rollout/           |          |\n",
+      "|    ep_len_mean     | 34.4     |\n",
+      "|    ep_rew_mean     | -6.9     |\n",
+      "| time/              |          |\n",
+      "|    episodes        | 1160     |\n",
+      "|    fps             | 554      |\n",
+      "|    time_elapsed    | 52       |\n",
+      "|    total_timesteps | 29034    |\n",
+      "---------------------------------\n",
+      "---------------------------------\n",
+      "| rollout/           |          |\n",
+      "|    ep_len_mean     | 43.3     |\n",
+      "|    ep_rew_mean     | -4.62    |\n",
+      "| time/              |          |\n",
+      "|    episodes        | 1180     |\n",
+      "|    fps             | 550      |\n",
+      "|    time_elapsed    | 54       |\n",
+      "|    total_timesteps | 30264    |\n",
+      "---------------------------------\n",
+      "---------------------------------\n",
+      "| rollout/           |          |\n",
+      "|    ep_len_mean     | 38.8     |\n",
+      "|    ep_rew_mean     | -5.32    |\n",
+      "| time/              |          |\n",
+      "|    episodes        | 1200     |\n",
+      "|    fps             | 550      |\n",
+      "|    time_elapsed    | 55       |\n",
+      "|    total_timesteps | 30516    |\n",
+      "---------------------------------\n",
+      "---------------------------------\n",
+      "| rollout/           |          |\n",
+      "|    ep_len_mean     | 33.2     |\n",
+      "|    ep_rew_mean     | -13.2    |\n",
+      "| time/              |          |\n",
+      "|    episodes        | 1220     |\n",
+      "|    fps             | 550      |\n",
+      "|    time_elapsed    | 55       |\n",
+      "|    total_timesteps | 30732    |\n",
+      "---------------------------------\n",
+      "---------------------------------\n",
+      "| rollout/           |          |\n",
+      "|    ep_len_mean     | 33.1     |\n",
+      "|    ep_rew_mean     | -14.3    |\n",
+      "| time/              |          |\n",
+      "|    episodes        | 1240     |\n",
+      "|    fps             | 551      |\n",
+      "|    time_elapsed    | 56       |\n",
+      "|    total_timesteps | 31026    |\n",
+      "---------------------------------\n",
+      "---------------------------------\n",
+      "| rollout/           |          |\n",
+      "|    ep_len_mean     | 23.3     |\n",
+      "|    ep_rew_mean     | -39.4    |\n",
+      "| time/              |          |\n",
+      "|    episodes        | 1260     |\n",
+      "|    fps             | 551      |\n",
+      "|    time_elapsed    | 57       |\n",
+      "|    total_timesteps | 31662    |\n",
+      "---------------------------------\n",
+      "---------------------------------\n",
+      "| rollout/           |          |\n",
+      "|    ep_len_mean     | 19.8     |\n",
+      "|    ep_rew_mean     | -34.1    |\n",
+      "| time/              |          |\n",
+      "|    episodes        | 1280     |\n",
+      "|    fps             | 552      |\n",
+      "|    time_elapsed    | 58       |\n",
+      "|    total_timesteps | 32226    |\n",
+      "---------------------------------\n",
+      "---------------------------------\n",
+      "| rollout/           |          |\n",
+      "|    ep_len_mean     | 21.9     |\n",
+      "|    ep_rew_mean     | -30.2    |\n",
+      "| time/              |          |\n",
+      "|    episodes        | 1300     |\n",
+      "|    fps             | 552      |\n",
+      "|    time_elapsed    | 58       |\n",
+      "|    total_timesteps | 32538    |\n",
+      "---------------------------------\n",
+      "---------------------------------\n",
+      "| rollout/           |          |\n",
+      "|    ep_len_mean     | 22.1     |\n",
+      "|    ep_rew_mean     | -28.3    |\n",
+      "| time/              |          |\n",
+      "|    episodes        | 1320     |\n",
+      "|    fps             | 550      |\n",
+      "|    time_elapsed    | 60       |\n",
+      "|    total_timesteps | 33336    |\n",
+      "---------------------------------\n",
+      "---------------------------------\n",
+      "| rollout/           |          |\n",
+      "|    ep_len_mean     | 30.5     |\n",
+      "|    ep_rew_mean     | -22.9    |\n",
+      "| time/              |          |\n",
+      "|    episodes        | 1340     |\n",
+      "|    fps             | 549      |\n",
+      "|    time_elapsed    | 62       |\n",
+      "|    total_timesteps | 34146    |\n",
+      "---------------------------------\n",
+      "---------------------------------\n",
+      "| rollout/           |          |\n",
+      "|    ep_len_mean     | 38.4     |\n",
+      "|    ep_rew_mean     | -4.81    |\n",
+      "| time/              |          |\n",
+      "|    episodes        | 1360     |\n",
+      "|    fps             | 549      |\n",
+      "|    time_elapsed    | 63       |\n",
+      "|    total_timesteps | 35088    |\n",
+      "---------------------------------\n",
+      "---------------------------------\n",
+      "| rollout/           |          |\n",
+      "|    ep_len_mean     | 31.9     |\n",
+      "|    ep_rew_mean     | -12.4    |\n",
+      "| time/              |          |\n",
+      "|    episodes        | 1380     |\n",
+      "|    fps             | 549      |\n",
+      "|    time_elapsed    | 64       |\n",
+      "|    total_timesteps | 35262    |\n",
+      "---------------------------------\n",
+      "---------------------------------\n",
+      "| rollout/           |          |\n",
+      "|    ep_len_mean     | 30.1     |\n",
+      "|    ep_rew_mean     | -15      |\n",
+      "| time/              |          |\n",
+      "|    episodes        | 1400     |\n",
+      "|    fps             | 549      |\n",
+      "|    time_elapsed    | 64       |\n",
+      "|    total_timesteps | 35694    |\n",
+      "---------------------------------\n",
+      "Eval num_timesteps=36000, episode_reward=235.63 +/- 501.15\n",
+      "Episode length: 83.60 +/- 95.04\n",
+      "---------------------------------\n",
+      "| eval/              |          |\n",
+      "|    mean_ep_length  | 83.6     |\n",
+      "|    mean_reward     | 236      |\n",
+      "| time/              |          |\n",
+      "|    total_timesteps | 36000    |\n",
+      "---------------------------------\n",
+      "New best mean reward!\n",
+      "---------------------------------\n",
+      "| rollout/           |          |\n",
+      "|    ep_len_mean     | 34.3     |\n",
+      "|    ep_rew_mean     | -4.42    |\n",
+      "| time/              |          |\n",
+      "|    episodes        | 1420     |\n",
+      "|    fps             | 538      |\n",
+      "|    time_elapsed    | 67       |\n",
+      "|    total_timesteps | 36396    |\n",
+      "---------------------------------\n",
+      "---------------------------------\n",
+      "| rollout/           |          |\n",
+      "|    ep_len_mean     | 28.8     |\n",
+      "|    ep_rew_mean     | -12.8    |\n",
+      "| time/              |          |\n",
+      "|    episodes        | 1440     |\n",
+      "|    fps             | 539      |\n",
+      "|    time_elapsed    | 69       |\n",
+      "|    total_timesteps | 37242    |\n",
+      "---------------------------------\n",
+      "---------------------------------\n",
+      "| rollout/           |          |\n",
+      "|    ep_len_mean     | 25.6     |\n",
+      "|    ep_rew_mean     | -28.5    |\n",
+      "| time/              |          |\n",
+      "|    episodes        | 1460     |\n",
+      "|    fps             | 539      |\n",
+      "|    time_elapsed    | 70       |\n",
+      "|    total_timesteps | 37836    |\n",
+      "---------------------------------\n",
+      "---------------------------------\n",
+      "| rollout/           |          |\n",
+      "|    ep_len_mean     | 31.5     |\n",
+      "|    ep_rew_mean     | -27.7    |\n",
+      "| time/              |          |\n",
+      "|    episodes        | 1480     |\n",
+      "|    fps             | 538      |\n",
+      "|    time_elapsed    | 72       |\n",
+      "|    total_timesteps | 38922    |\n",
+      "---------------------------------\n",
+      "---------------------------------\n",
+      "| rollout/           |          |\n",
+      "|    ep_len_mean     | 39.4     |\n",
+      "|    ep_rew_mean     | -15.2    |\n",
+      "| time/              |          |\n",
+      "|    episodes        | 1500     |\n",
+      "|    fps             | 517      |\n",
+      "|    time_elapsed    | 77       |\n",
+      "|    total_timesteps | 40104    |\n",
+      "| train/             |          |\n",
+      "|    actor_loss      | -5.3     |\n",
+      "|    critic_loss     | 77.1     |\n",
+      "|    ent_coef        | 0.971    |\n",
+      "|    ent_coef_loss   | -0.248   |\n",
+      "|    learning_rate   | 0.0003   |\n",
+      "|    n_updates       | 102      |\n",
+      "---------------------------------\n",
+      "---------------------------------\n",
+      "| rollout/           |          |\n",
+      "|    ep_len_mean     | 40.8     |\n",
+      "|    ep_rew_mean     | -15.5    |\n",
+      "| time/              |          |\n",
+      "|    episodes        | 1520     |\n",
+      "|    fps             | 456      |\n",
+      "|    time_elapsed    | 88       |\n",
+      "|    total_timesteps | 40446    |\n",
+      "| train/             |          |\n",
+      "|    actor_loss      | -8.63    |\n",
+      "|    critic_loss     | 52.7     |\n",
+      "|    ent_coef        | 0.88     |\n",
+      "|    ent_coef_loss   | -0.869   |\n",
+      "|    learning_rate   | 0.0003   |\n",
+      "|    n_updates       | 444      |\n",
+      "---------------------------------\n",
+      "Saving last checkpoint\n",
+      "Last checkpoint saved in: /home/daniel/DARM/darm_mujoco/darm_training/results/darm_sf_hand/test1_SF_SB3_SAC_4/models/last_model\n"
+     ]
+    },
+    {
+     "ename": "KeyboardInterrupt",
+     "evalue": "",
+     "output_type": "error",
+     "traceback": [
+      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
+      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
+      "Input \u001b[0;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m----> 2\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtotal_timesteps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlog_interval\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m                \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mException caught:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
+      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/stable_baselines3/sac/sac.py:309\u001b[0m, in \u001b[0;36mSAC.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    295\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[1;32m    296\u001b[0m     \u001b[38;5;28mself\u001b[39m: SACSelf,\n\u001b[1;32m    297\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    306\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    307\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SACSelf:\n\u001b[0;32m--> 309\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_env\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_eval_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_eval_episodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_log_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_log_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
+      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/stable_baselines3/common/off_policy_algorithm.py:375\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    373\u001b[0m         \u001b[38;5;66;03m# Special case when the user passes `gradient_steps=0`\u001b[39;00m\n\u001b[1;32m    374\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m gradient_steps \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 375\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgradient_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    377\u001b[0m callback\u001b[38;5;241m.\u001b[39mon_training_end()\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
+      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/stable_baselines3/sac/sac.py:271\u001b[0m, in \u001b[0;36mSAC.train\u001b[0;34m(self, gradient_steps, batch_size)\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[38;5;66;03m# Compute actor loss\u001b[39;00m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;66;03m# Alternative: actor_loss = th.mean(log_prob - qf1_pi)\u001b[39;00m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;66;03m# Min over all critic networks\u001b[39;00m\n\u001b[1;32m    270\u001b[0m q_values_pi \u001b[38;5;241m=\u001b[39m th\u001b[38;5;241m.\u001b[39mcat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcritic(replay_data\u001b[38;5;241m.\u001b[39mobservations, actions_pi), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 271\u001b[0m min_qf_pi, _ \u001b[38;5;241m=\u001b[39m \u001b[43mth\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq_values_pi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    272\u001b[0m actor_loss \u001b[38;5;241m=\u001b[39m (ent_coef \u001b[38;5;241m*\u001b[39m log_prob \u001b[38;5;241m-\u001b[39m min_qf_pi)\u001b[38;5;241m.\u001b[39mmean()\n\u001b[1;32m    273\u001b[0m actor_losses\u001b[38;5;241m.\u001b[39mappend(actor_loss\u001b[38;5;241m.\u001b[39mitem())\n",
+      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
+     ]
+    }
+   ],
+   "source": [
+    "try:\n",
+    "    model.learn(total_timesteps=config[\"total_timesteps\"], \n",
+    "                log_interval=config[\"log_interval\"], \n",
+    "                tb_log_name=run_name,\n",
+    "                callback=callback)\n",
+    "except Exception as e:\n",
+    "    print(\"Exception caught:\")\n",
+    "    print(e)\n",
+    "finally:\n",
+    "    # timestamp = f\"{datetime.now().date()}__{datetime.now().time()}\"\n",
+    "    print(\"Saving last checkpoint\")\n",
+    "    model_name = f\"{config['run_local_dir']}/models/last_model\"\n",
+    "    model.save(model_name)\n",
+    "    print(f\"Last checkpoint saved in: {model_name}\")"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 11,
+   "id": "139daf70-71ae-4b08-8798-a9e86b0a87a0",
+   "metadata": {
+    "tags": []
+   },
+   "outputs": [
+    {
+     "data": {
+      "text/html": [
+       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
+      ],
+      "text/plain": [
+       "<IPython.core.display.HTML object>"
+      ]
+     },
+     "metadata": {},
+     "output_type": "display_data"
+    },
+    {
+     "data": {
+      "text/html": [
+       "<style>\n",
+       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
+       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
+       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
+       "    </style>\n",
+       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/mean_ep_length</td><td>▄▁█</td></tr><tr><td>eval/mean_reward</td><td>▁▁█</td></tr><tr><td>global_step</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇██</td></tr><tr><td>rollout/ep_len_mean</td><td>▁▄▄▄▂▄▄▄▄▅▄▅▅▄▄▆▇▆▅▅▄▅▆▆▆▄▄▄▅▆▇█▆▄▄█▆▆▆█</td></tr><tr><td>rollout/ep_rew_mean</td><td>▁▂▂▂▂▃▃▃▂▂▃▄▅▃▄▇█▇▅▃▂▃▄▄▃▂▄▄▄▂▆▆▅▃▃▆▅▅▃▅</td></tr><tr><td>time/fps</td><td>▃█▇▇▇██▇▇▇▇▇▇▇▆▅▆▆▆▆▆▆▆▆▆▆▅▅▅▆▆▆▆▆▆▆▆▅▅▁</td></tr><tr><td>train/actor_loss</td><td>█▁</td></tr><tr><td>train/critic_loss</td><td>█▁</td></tr><tr><td>train/ent_coef</td><td>█▁</td></tr><tr><td>train/ent_coef_loss</td><td>█▁</td></tr><tr><td>train/learning_rate</td><td>▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/mean_ep_length</td><td>83.6</td></tr><tr><td>eval/mean_reward</td><td>235.63208</td></tr><tr><td>global_step</td><td>40446</td></tr><tr><td>rollout/ep_len_mean</td><td>40.81</td></tr><tr><td>rollout/ep_rew_mean</td><td>-15.53837</td></tr><tr><td>time/fps</td><td>456.0</td></tr><tr><td>train/actor_loss</td><td>-8.62987</td></tr><tr><td>train/critic_loss</td><td>52.72518</td></tr><tr><td>train/ent_coef</td><td>0.88031</td></tr><tr><td>train/ent_coef_loss</td><td>-0.86934</td></tr><tr><td>train/learning_rate</td><td>0.0003</td></tr></table><br/></div></div>"
+      ],
+      "text/plain": [
+       "<IPython.core.display.HTML object>"
+      ]
+     },
+     "metadata": {},
+     "output_type": "display_data"
+    },
+    {
+     "data": {
+      "text/html": [
+       " View run <strong style=\"color:#cdcd00\">test1_SF_SB3_SAC_4</strong> at: <a href='https://wandb.ai/danieladejumo/DARM/runs/jfvnsuje' target=\"_blank\">https://wandb.ai/danieladejumo/DARM/runs/jfvnsuje</a><br/>Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 3 other file(s)"
+      ],
+      "text/plain": [
+       "<IPython.core.display.HTML object>"
+      ]
+     },
+     "metadata": {},
+     "output_type": "display_data"
+    },
+    {
+     "data": {
+      "text/html": [
+       "Find logs at: <code>./wandb/run-20230302_145847-jfvnsuje/logs</code>"
+      ],
+      "text/plain": [
+       "<IPython.core.display.HTML object>"
+      ]
+     },
+     "metadata": {},
+     "output_type": "display_data"
+    },
     {
      "name": "stdout",
      "output_type": "stream",
      "text": [
-      "Finished run test4_SF_SB3_SAC\n"
+      "Finished run test1_SF_SB3_SAC_4\n"
      ]
     }
    ],
@@ -928,6 +1499,19 @@
     "print(f\"Finished run {run_name}\")"
    ]
   },
+  {
+   "cell_type": "code",
+   "execution_count": 12,
+   "id": "6801db30-7721-45e3-af29-a2bc557d4fe8",
+   "metadata": {
+    "tags": []
+   },
+   "outputs": [],
+   "source": [
+    "env.close()\n",
+    "eval_env.close()"
+   ]
+  },
   {
    "cell_type": "code",
    "execution_count": null,
@@ -941,22 +1525,23 @@
     "\n",
     "# LOAD TRAINED MODEL\n",
     "\n",
-    "try:\n",
-    "    model.learn(total_timesteps=10_000_000, log_interval=8, tb_log_name=\"PlainDarmEnv\",\n",
-    "                    callback=WandbCallback(model_save_path=f\"checkpoints/wandb/{run.id}\",\n",
-    "                                           model_save_freq=10, verbose=2)\n",
-    "               )\n",
-    "    # Add calbacks\n",
-    "except Exception as e:\n",
-    "    print(\"Exception caught:\")\n",
-    "    print(e)\n",
-    "finally:\n",
-    "    timestamp = f\"{datetime.now().date()}__{datetime.now().time()}\"\n",
-    "    print(f\"Saving checkpoint {timestamp}\")\n",
-    "    model_name = f\"./checkpoints/darm_sf_hand_{timestamp}\"\n",
-    "    env_norm_name = f\"./checkpoints/darm_sf_hand_env_norm_{timestamp}\"\n",
-    "    model.save(model_name)\n",
-    "    # env.save(env_norm_name) # FIXME: Remember to save norm params if using VecNorm env\n",
+    "# ########### PATHS NEED TO BE UPDATED\n",
+    "# try:\n",
+    "#     model.learn(total_timesteps=10_000_000, log_interval=8, tb_log_name=\"PlainDarmEnv\",\n",
+    "#                     callback=WandbCallback(model_save_path=f\"checkpoints/wandb/{run.id}\",\n",
+    "#                                            model_save_freq=10, verbose=2)\n",
+    "#                )\n",
+    "#     # Add calbacks\n",
+    "# except Exception as e:\n",
+    "#     print(\"Exception caught:\")\n",
+    "#     print(e)\n",
+    "# finally:\n",
+    "#     timestamp = f\"{datetime.now().date()}__{datetime.now().time()}\"\n",
+    "#     print(f\"Saving checkpoint {timestamp}\")\n",
+    "#     model_name = f\"./checkpoints/darm_sf_hand_{timestamp}\"\n",
+    "#     env_norm_name = f\"./checkpoints/darm_sf_hand_env_norm_{timestamp}\"\n",
+    "#     model.save(model_name)\n",
+    "#     # env.save(env_norm_name) # FIXME: Remember to save norm params if using VecNorm env\n",
     "    "
    ]
   },
@@ -969,7 +1554,8 @@
    },
    "outputs": [],
    "source": [
-    "env.close()"
+    "env.close()\n",
+    "eval_env.close()"
    ]
   },
   {
@@ -982,7 +1568,7 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 6,
+   "execution_count": 7,
    "id": "c54a6abe-1b86-4682-9d7d-456dc1364ba8",
    "metadata": {},
    "outputs": [
@@ -1000,7 +1586,7 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 7,
+   "execution_count": 13,
    "id": "0e4596f3-355b-4200-9eb5-128405d6b914",
    "metadata": {
     "tags": []
@@ -1013,32 +1599,22 @@
       "Loaded XML file successfully\n"
      ]
     },
-    {
-     "name": "stderr",
-     "output_type": "stream",
-     "text": [
-      "/home/daniel/miniconda3/lib/python3.8/site-packages/gym/spaces/box.py:73: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
-      "  logger.warn(\n"
-     ]
-    },
     {
      "data": {
       "text/plain": [
-       "<stable_baselines3.sac.sac.SAC at 0x7f9da5ae2f10>"
+       "<stable_baselines3.sac.sac.SAC at 0x7fede49427f0>"
       ]
      },
-     "execution_count": 7,
+     "execution_count": 13,
      "metadata": {},
      "output_type": "execute_result"
     }
    ],
    "source": [
-    "model_name = \"./results/darm_sf_hand/SF_SB3_SAC_3/best_model\"\n",
+    "model_name = f\"{config['run_local_dir']}/models/best/best_model\"\n",
     "# env_norm_name = \"./checkpoints/darm_sf_hand_env_norm_2022-12-28__10:10:05.637581\"\n",
     "\n",
     "eval_env = make_vec_env(create_env, n_envs=1, seed=config[\"seed\"])\n",
-    "# eval_env = DummyVecEnv([lambda: gym.make(\"darm/DarmSFHand-v0\", render_mode=\"human\", hand_name=\"hand1\")])\n",
-    "# eval_env = gym.make(\"darm/DarmSFHand-v0\", render_mode=\"human\", hand_name=\"hand1\")\n",
     "\n",
     "eval_model = SAC.load(model_name, env=eval_env)\n",
     "eval_model"
@@ -1046,7 +1622,7 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 10,
+   "execution_count": 14,
    "id": "759192eb-a5d0-43e5-825a-8f550c09ad5e",
    "metadata": {},
    "outputs": [
@@ -1054,7 +1630,7 @@
      "name": "stdout",
      "output_type": "stream",
      "text": [
-      "1189.5793403000002 741.351242315577\n"
+      "-6.5527834 119.55361281297256\n"
      ]
     }
    ],
@@ -1071,27 +1647,9 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 141,
-   "id": "c1818f92-c4f6-4af5-a1d3-1752af7e1806",
-   "metadata": {},
-   "outputs": [],
-   "source": [
-    "# env = DummyVecEnv([lambda: gym.make(\"darm/DarmSFHand-v0\", render_mode=\"human\", hand_name=\"hand1\")])\n",
-    "\n",
-    "# env = VecNormalize.load(env_norm_name, env)\n",
-    "# env.training = False\n",
-    "# print(\"Zero Norm: \", env.unnormalize_reward(-0.47959065))"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": 18,
-   "id": "cbec922f-40a9-4f6f-b7f8-99ebd5113835",
+   "execution_count": 16,
+   "id": "89fa3c73-6a78-4b4f-93d3-f4cd855fdb2f",
    "metadata": {
-    "collapsed": true,
-    "jupyter": {
-     "outputs_hidden": true
-    },
     "tags": []
    },
    "outputs": [
@@ -1099,221 +1657,62 @@
      "name": "stdout",
      "output_type": "stream",
      "text": [
-      "Loaded XML file successfully\n",
-      "Episode Return: 770.678056485951 Episode Length: 200\n",
-      "{'TimeLimit.truncated': True,\n",
-      " 'action': array([0.1272218 , 0.38648322, 0.41795075, 0.96957695, 0.17801568],\n",
-      "      dtype=float32),\n",
-      " 'episode': {'l': 200, 'r': 770.678058, 't': 4.201306},\n",
-      " 'model_action': array([[-0.9491113, -0.8454067, -0.8328197, -0.6121692, -0.9287937]],\n",
-      "      dtype=float32),\n",
-      " 'reward': {'act_reg': -0.22908571362495422,\n",
-      "            'bonus': array([1.]),\n",
-      "            'dense': array([3.96524759]),\n",
-      "            'done': array([False]),\n",
-      "            'penalty': array([-0.]),\n",
-      "            'reach': array([-0.00592192]),\n",
-      "            'solved': array([False]),\n",
-      "            'sparse': array([-0.00592192])},\n",
-      " 'sim_time': 16.154000000001925,\n",
-      " 'terminal_observation': array([-1.43847659, -7.91392647,  0.31345961, -1.34039728, -7.80854287,\n",
-      "       -0.26096704,  0.01737392,  0.05786671, -0.02924779])}\n",
-      "Episode Return: 635.6054340600967 Episode Length: 200\n",
-      "{'TimeLimit.truncated': True,\n",
-      " 'action': array([0.01671374, 0.03772706, 4.880125  , 2.3247674 , 0.07127702],\n",
-      "      dtype=float32),\n",
-      " 'episode': {'l': 200, 'r': 635.605434, 't': 8.174224},\n",
-      " 'model_action': array([[-0.9933145, -0.9849092,  0.95205  , -0.0700931, -0.9714892]],\n",
-      "      dtype=float32),\n",
-      " 'reward': {'act_reg': -1.081239104270935,\n",
-      "            'bonus': array([0.]),\n",
-      "            'dense': array([-0.13392945]),\n",
-      "            'done': array([False]),\n",
-      "            'penalty': array([-0.]),\n",
-      "            'reach': array([-0.01290277]),\n",
-      "            'solved': array([False]),\n",
-      "            'sparse': array([-0.01290277])},\n",
-      " 'sim_time': 16.385999999991878,\n",
-      " 'terminal_observation': array([-0.97768326, -7.2132605 , -1.03575665, -1.136482  , -6.26464382,\n",
-      "       -0.17568471, -0.01834931,  0.08324079,  0.1292913 ])}\n",
-      "Episode Return: 698.4176086336374 Episode Length: 200\n",
-      "{'TimeLimit.truncated': True,\n",
-      " 'action': array([1.4136398 , 1.9245778 , 0.25722325, 2.4980981 , 0.18945813],\n",
-      "      dtype=float32),\n",
-      " 'episode': {'l': 200, 'r': 698.417607, 't': 12.202491},\n",
-      " 'model_action': array([[-4.3454409e-01, -2.3016888e-01, -8.9711070e-01, -7.6073408e-04,\n",
-      "        -9.2421675e-01]], dtype=float32),\n",
-      " 'reward': {'act_reg': -0.6941161155700684,\n",
-      "            'bonus': array([1.]),\n",
-      "            'dense': array([3.91971437]),\n",
-      "            'done': array([False]),\n",
-      "            'penalty': array([-0.]),\n",
-      "            'reach': array([-0.00543701]),\n",
-      "            'solved': array([False]),\n",
-      "            'sparse': array([-0.00543701])},\n",
-      " 'sim_time': 16.000000000019554,\n",
-      " 'terminal_observation': array([ 0.74869396, -3.39518286,  8.58578734,  0.5289286 , -3.84832334,\n",
-      "        8.38090363,  0.01507195,  0.10874858,  0.04706846])}\n",
-      "Episode Return: 762.3988908529282 Episode Length: 200\n",
-      "{'TimeLimit.truncated': True,\n",
-      " 'action': array([4.8835716, 0.6218907, 4.532805 , 1.0692017, 0.3363648],\n",
-      "      dtype=float32),\n",
-      " 'episode': {'l': 200, 'r': 762.398892, 't': 16.093096},\n",
-      " 'model_action': array([[ 0.9534286 , -0.7512437 ,  0.81312203, -0.5723193 , -0.8654541 ]],\n",
-      "      dtype=float32),\n",
-      " 'reward': {'act_reg': -1.3570361137390137,\n",
-      "            'bonus': array([1.]),\n",
-      "            'dense': array([3.85199221]),\n",
-      "            'done': array([False]),\n",
-      "            'penalty': array([-0.]),\n",
-      "            'reach': array([-0.00615209]),\n",
-      "            'solved': array([False]),\n",
-      "            'sparse': array([-0.00615209])},\n",
-      " 'sim_time': 16.014000000017596,\n",
-      " 'terminal_observation': array([-2.17373708e+00, -7.50087186e+00, -3.53731302e+00, -2.20807879e+00,\n",
-      "       -7.81725214e+00, -3.01080938e+00,  2.33116142e-03, -7.95696272e-03,\n",
-      "        1.00434933e-02])}\n",
-      "Episode Return: 274.17127342522144 Episode Length: 200\n",
-      "{'TimeLimit.truncated': True,\n",
-      " 'action': array([0.41142836, 0.07208899, 3.8438308 , 2.905532  , 0.53034574],\n",
-      "      dtype=float32),\n",
-      " 'episode': {'l': 200, 'r': 274.171275, 't': 20.10166},\n",
-      " 'model_action': array([[-0.83542866, -0.9711644 ,  0.5375323 ,  0.16221273, -0.7878617 ]],\n",
-      "      dtype=float32),\n",
-      " 'reward': {'act_reg': -0.9730958342552185,\n",
-      "            'bonus': array([0.]),\n",
-      "            'dense': array([-0.13675444]),\n",
-      "            'done': array([False]),\n",
-      "            'penalty': array([-0.]),\n",
-      "            'reach': array([-0.01972243]),\n",
-      "            'solved': array([False]),\n",
-      "            'sparse': array([-0.01972243])},\n",
-      " 'sim_time': 16.39999999996178,\n",
-      " 'terminal_observation': array([-1.41096584, -7.04316762,  0.76866325, -0.97992386, -7.20407405,\n",
-      "        2.68648856, -0.05202748,  0.02185201, -0.00837738])}\n",
-      "Episode Return: 790.0150463581085 Episode Length: 200\n",
-      "{'TimeLimit.truncated': True,\n",
-      " 'action': array([0.626176 , 0.230418 , 3.0357594, 2.8480678, 1.5895853],\n",
-      "      dtype=float32),\n",
-      " 'episode': {'l': 200, 'r': 790.015047, 't': 24.183044},\n",
-      " 'model_action': array([[-0.7495296 , -0.9078328 ,  0.21430373,  0.13922715, -0.3641659 ]],\n",
-      "      dtype=float32),\n",
-      " 'reward': {'act_reg': -0.9010948538780212,\n",
-      "            'bonus': array([1.]),\n",
-      "            'dense': array([3.8982463]),\n",
-      "            'done': array([False]),\n",
-      "            'penalty': array([-0.]),\n",
-      "            'reach': array([-0.00582211]),\n",
-      "            'solved': array([False]),\n",
-      "            'sparse': array([-0.00582211])},\n",
-      " 'sim_time': 16.39999999996178,\n",
-      " 'terminal_observation': array([-1.69379596e+00, -6.41159166e+00,  2.34630250e+00, -1.40258968e+00,\n",
-      "       -6.58174560e+00,  1.87173357e+00,  1.13313858e-02,  7.53286189e-04,\n",
-      "       -4.42717139e-02])}\n",
-      "Episode Return: -17.83931428194046 Episode Length: 200\n",
-      "{'TimeLimit.truncated': True,\n",
-      " 'action': array([0.01443997, 4.0789003 , 4.928274  , 1.3416407 , 0.08007079],\n",
-      "      dtype=float32),\n",
-      " 'episode': {'l': 200, 'r': -17.839314, 't': 28.122158},\n",
-      " 'model_action': array([[-0.994224  ,  0.6315601 ,  0.97130966, -0.46334374, -0.9679717 ]],\n",
-      "      dtype=float32),\n",
-      " 'reward': {'act_reg': -1.3073933124542236,\n",
-      "            'bonus': array([0.]),\n",
-      "            'dense': array([-0.15065615]),\n",
-      "            'done': array([False]),\n",
-      "            'penalty': array([-0.]),\n",
-      "            'reach': array([-0.00995841]),\n",
-      "            'solved': array([False]),\n",
-      "            'sparse': array([-0.00995841])},\n",
-      " 'sim_time': 16.39999999996178,\n",
-      " 'terminal_observation': array([ 2.31332325e+00,  3.58036822e+00,  8.62615753e+00,  1.32623710e+00,\n",
-      "        3.71192344e+00,  8.63344786e+00, -2.32576980e-01,  1.39302252e-01,\n",
-      "        8.62924880e-03])}\n",
-      "Episode Return: 774.3884239196777 Episode Length: 200\n",
-      "{'TimeLimit.truncated': True,\n",
-      " 'action': array([0.78777623, 0.1897496 , 3.7601388 , 1.480814  , 4.1308985 ],\n",
-      "      dtype=float32),\n",
-      " 'episode': {'l': 200, 'r': 774.388425, 't': 32.212381},\n",
-      " 'model_action': array([[-0.6848895 , -0.92410016,  0.5040555 , -0.40767443,  0.6523595 ]],\n",
-      "      dtype=float32),\n",
-      " 'reward': {'act_reg': -1.1670881509780884,\n",
-      "            'bonus': array([1.]),\n",
-      "            'dense': array([3.87202517]),\n",
-      "            'done': array([False]),\n",
-      "            'penalty': array([-0.]),\n",
-      "            'reach': array([-0.00563301]),\n",
-      "            'solved': array([False]),\n",
-      "            'sparse': array([-0.00563301])},\n",
-      " 'sim_time': 16.34799999997682,\n",
-      " 'terminal_observation': array([-2.33427851e+00, -4.14350529e+00,  6.42536776e+00, -2.37802387e+00,\n",
-      "       -4.58753966e+00,  6.08152511e+00, -1.63520234e-05, -8.45316575e-06,\n",
-      "        5.89392780e-05])}\n",
-      "Episode Return: 769.3141285777092 Episode Length: 200\n",
-      "{'TimeLimit.truncated': True,\n",
-      " 'action': array([0.5708501, 1.8720514, 1.7804887, 1.2069921, 0.5779172],\n",
-      "      dtype=float32),\n",
-      " 'episode': {'l': 200, 'r': 769.31413, 't': 36.213876},\n",
-      " 'model_action': array([[-0.77166   , -0.25117946, -0.28780448, -0.51720315, -0.7688331 ]],\n",
-      "      dtype=float32),\n",
-      " 'reward': {'act_reg': -0.5930060744285583,\n",
-      "            'bonus': array([1.]),\n",
-      "            'dense': array([3.92649667]),\n",
-      "            'done': array([False]),\n",
-      "            'penalty': array([-0.]),\n",
-      "            'reach': array([-0.00710136]),\n",
-      "            'solved': array([False]),\n",
-      "            'sparse': array([-0.00710136])},\n",
-      " 'sim_time': 16.000000000076398,\n",
-      " 'terminal_observation': array([-1.39438631e+00, -8.90305365e+00, -7.95455178e-01, -1.45342848e+00,\n",
-      "       -9.00528122e+00, -1.49571028e+00,  1.13871858e-05, -2.33744274e-08,\n",
-      "       -4.36979276e-06])}\n",
-      "Episode Return: 703.4400441199541 Episode Length: 200\n",
-      "{'TimeLimit.truncated': True,\n",
-      " 'action': array([4.6980705 , 0.33982232, 4.484064  , 3.5164433 , 1.7755698 ],\n",
-      "      dtype=float32),\n",
-      " 'episode': {'l': 200, 'r': 703.440043, 't': 40.214405},\n",
-      " 'model_action': array([[ 0.8792281 , -0.8640711 ,  0.7936256 ,  0.40657735, -0.2897721 ]],\n",
-      "      dtype=float32),\n",
-      " 'reward': {'act_reg': -1.5206867456436157,\n",
-      "            'bonus': array([1.]),\n",
-      "            'dense': array([3.83444965]),\n",
-      "            'done': array([False]),\n",
-      "            'penalty': array([-0.]),\n",
-      "            'reach': array([-0.00674084]),\n",
-      "            'solved': array([False]),\n",
-      "            'sparse': array([-0.00674084])},\n",
-      " 'sim_time': 16.000000000076398,\n",
-      " 'terminal_observation': array([ 3.03725062,  1.24027421,  6.26230394,  3.00486633,  0.6216548 ,\n",
-      "        6.52810497,  0.14403881, -0.09292099,  0.35840393])}\n"
+      "Loaded XML file successfully\n"
+     ]
+    },
+    {
+     "name": "stderr",
+     "output_type": "stream",
+     "text": [
+      "100%|██████████| 10/10 [00:20<00:00,  2.02s/it]"
+     ]
+    },
+    {
+     "name": "stdout",
+     "output_type": "stream",
+     "text": [
+      "0/10 Solved\n"
+     ]
+    },
+    {
+     "name": "stderr",
+     "output_type": "stream",
+     "text": [
+      "\n"
      ]
     }
    ],
    "source": [
     "import pprint\n",
+    "import numpy as np\n",
+    "from tqdm import tqdm\n",
     "\n",
-    "create_env_render = lambda: TransformObservation(gym.make(config[\"env_id\"], render_mode=\"human\"), lambda obs: obs*100)\n",
+    "create_env_render = lambda: TransformObservation(gym.make(config[\"env_id\"], render_mode=\"human\", single_finger_env=config[\"single_finger_env\"]), lambda obs: obs*100)\n",
     "env = make_vec_env(create_env_render, n_envs=1, seed=config[\"seed\"])\n",
-    "# env = DummyVecEnv([lambda: gym.make(\"darm/DarmSFHand-v0\", render_mode=\"human\", hand_name=\"hand1\")])\n",
-    "# env = gym.make(\"darm/DarmSFHand-v0\", render_mode=\"human\", hand_name=\"hand1\")\n",
     "\n",
     "obs = env.reset()\n",
     "episode_return = 0\n",
     "episode_length = 0\n",
     "N_EPISODES = 10\n",
+    "solved = 0\n",
+    "actions = []\n",
     "\n",
-    "for i in range(N_EPISODES):\n",
+    "for i in tqdm(range(N_EPISODES)):\n",
     "    done = False\n",
     "    while not done:\n",
     "        # env.render()\n",
     "        action, _states = eval_model.predict(obs, deterministic=True)\n",
     "        obs, reward, done, info = env.step(action)\n",
+    "        actions.append(info[0][\"action\"])\n",
     "        episode_return += reward[0]\n",
     "        episode_length += 1\n",
     "        done = done[0]\n",
     "        \n",
-    "    print(f\"Episode Return: {episode_return} Episode Length: {episode_length}\")\n",
+    "  #  print(f\"Episode Return: {episode_return} Episode Length: {episode_length}\")\n",
     "    info[0][\"model_action\"] = action\n",
-    "    pprint.pprint(info[0])\n",
+    "    # pprint.pprint(info[0])\n",
+    "  # print(f\"Solved: {info[0]['reward']['solved']}\")\n",
+    "    solved += info[0]['reward']['solved'][0]\n",
     "    # info[\"model_action\"] = action\n",
     "    # pprint.pprint(info)\n",
     "    \n",
@@ -1321,7 +1720,317 @@
     "    episode_return = 0\n",
     "    episode_length = 0\n",
     "\n",
-    "env.close()"
+    "env.close()\n",
+    "print(f\"{solved}/{N_EPISODES} Solved\")"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 101,
+   "id": "b2f714a1-e583-4c5a-a7bd-f9936df07ba8",
+   "metadata": {
+    "tags": []
+   },
+   "outputs": [
+    {
+     "data": {
+      "text/plain": [
+       "(5, 195072)"
+      ]
+     },
+     "execution_count": 101,
+     "metadata": {},
+     "output_type": "execute_result"
+    }
+   ],
+   "source": [
+    "actions = np.asarray(actions)\n",
+    "\n",
+    "tendon_tensions = []\n",
+    "tendon_names = [\"Dorsal Interossei\",\n",
+    "                \"Palmar Interossei\",\n",
+    "                \"Extensor Digitorum Communis\",\n",
+    "                \"Flexor Digitorum Profundus\",\n",
+    "                \"Flexor Digitorum Superficialis\"]\n",
+    "\n",
+    "\n",
+    "for i in range(5):\n",
+    "    tendon_tensions.append(actions[:, i])\n",
+    "\n",
+    "tendon_tensions = np.asarray(tendon_tensions)\n",
+    "tendon_tensions.shape"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 102,
+   "id": "4af34a7e-25f2-4d1a-bcaa-19f2f5fa0d59",
+   "metadata": {
+    "tags": []
+   },
+   "outputs": [
+    {
+     "data": {
+      "text/html": [
+       "<div>\n",
+       "<style scoped>\n",
+       "    .dataframe tbody tr th:only-of-type {\n",
+       "        vertical-align: middle;\n",
+       "    }\n",
+       "\n",
+       "    .dataframe tbody tr th {\n",
+       "        vertical-align: top;\n",
+       "    }\n",
+       "\n",
+       "    .dataframe thead th {\n",
+       "        text-align: right;\n",
+       "    }\n",
+       "</style>\n",
+       "<table border=\"1\" class=\"dataframe\">\n",
+       "  <thead>\n",
+       "    <tr style=\"text-align: right;\">\n",
+       "      <th></th>\n",
+       "      <th>Dorsal Interossei</th>\n",
+       "      <th>Palmar Interossei</th>\n",
+       "      <th>Extensor Digitorum Communis</th>\n",
+       "      <th>Flexor Digitorum Profundus</th>\n",
+       "      <th>Flexor Digitorum Superficialis</th>\n",
+       "    </tr>\n",
+       "  </thead>\n",
+       "  <tbody>\n",
+       "    <tr>\n",
+       "      <th>count</th>\n",
+       "      <td>195072.000000</td>\n",
+       "      <td>195072.000000</td>\n",
+       "      <td>195072.000000</td>\n",
+       "      <td>195072.000000</td>\n",
+       "      <td>195072.000000</td>\n",
+       "    </tr>\n",
+       "    <tr>\n",
+       "      <th>mean</th>\n",
+       "      <td>0.950422</td>\n",
+       "      <td>0.767148</td>\n",
+       "      <td>3.509327</td>\n",
+       "      <td>1.329884</td>\n",
+       "      <td>0.429009</td>\n",
+       "    </tr>\n",
+       "    <tr>\n",
+       "      <th>std</th>\n",
+       "      <td>1.014455</td>\n",
+       "      <td>0.936925</td>\n",
+       "      <td>1.026986</td>\n",
+       "      <td>0.747989</td>\n",
+       "      <td>0.561555</td>\n",
+       "    </tr>\n",
+       "    <tr>\n",
+       "      <th>min</th>\n",
+       "      <td>0.001007</td>\n",
+       "      <td>0.000811</td>\n",
+       "      <td>0.000827</td>\n",
+       "      <td>0.000035</td>\n",
+       "      <td>0.000511</td>\n",
+       "    </tr>\n",
+       "    <tr>\n",
+       "      <th>25%</th>\n",
+       "      <td>0.244022</td>\n",
+       "      <td>0.115738</td>\n",
+       "      <td>2.881964</td>\n",
+       "      <td>0.832723</td>\n",
+       "      <td>0.155681</td>\n",
+       "    </tr>\n",
+       "    <tr>\n",
+       "      <th>50%</th>\n",
+       "      <td>0.590215</td>\n",
+       "      <td>0.373623</td>\n",
+       "      <td>3.788050</td>\n",
+       "      <td>1.291511</td>\n",
+       "      <td>0.288769</td>\n",
+       "    </tr>\n",
+       "    <tr>\n",
+       "      <th>75%</th>\n",
+       "      <td>1.207549</td>\n",
+       "      <td>1.124003</td>\n",
+       "      <td>4.289469</td>\n",
+       "      <td>1.689200</td>\n",
+       "      <td>0.432691</td>\n",
+       "    </tr>\n",
+       "    <tr>\n",
+       "      <th>max</th>\n",
+       "      <td>4.999928</td>\n",
+       "      <td>5.000000</td>\n",
+       "      <td>4.999597</td>\n",
+       "      <td>4.999947</td>\n",
+       "      <td>4.991437</td>\n",
+       "    </tr>\n",
+       "  </tbody>\n",
+       "</table>\n",
+       "</div>"
+      ],
+      "text/plain": [
+       "       Dorsal Interossei  Palmar Interossei  Extensor Digitorum Communis  \\\n",
+       "count      195072.000000      195072.000000                195072.000000   \n",
+       "mean            0.950422           0.767148                     3.509327   \n",
+       "std             1.014455           0.936925                     1.026986   \n",
+       "min             0.001007           0.000811                     0.000827   \n",
+       "25%             0.244022           0.115738                     2.881964   \n",
+       "50%             0.590215           0.373623                     3.788050   \n",
+       "75%             1.207549           1.124003                     4.289469   \n",
+       "max             4.999928           5.000000                     4.999597   \n",
+       "\n",
+       "       Flexor Digitorum Profundus  Flexor Digitorum Superficialis  \n",
+       "count               195072.000000                   195072.000000  \n",
+       "mean                     1.329884                        0.429009  \n",
+       "std                      0.747989                        0.561555  \n",
+       "min                      0.000035                        0.000511  \n",
+       "25%                      0.832723                        0.155681  \n",
+       "50%                      1.291511                        0.288769  \n",
+       "75%                      1.689200                        0.432691  \n",
+       "max                      4.999947                        4.991437  "
+      ]
+     },
+     "execution_count": 102,
+     "metadata": {},
+     "output_type": "execute_result"
+    }
+   ],
+   "source": [
+    "import pandas as pd\n",
+    "\n",
+    "tendon_tensions_dict = {}\n",
+    "\n",
+    "for tendon_name, tensions in zip(tendon_names, tendon_tensions):\n",
+    "    tendon_tensions_dict[tendon_name] = tensions\n",
+    "\n",
+    "tensions_df = pd.DataFrame(tendon_tensions_dict)\n",
+    "tensions_df.describe()"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 126,
+   "id": "b82ad6cc-b70e-45c0-8f5a-503381b6372a",
+   "metadata": {
+    "tags": []
+   },
+   "outputs": [
+    {
+     "data": {
+      "text/plain": [
+       "count    195072.000000\n",
+       "mean          0.950422\n",
+       "std           1.014455\n",
+       "min           0.001007\n",
+       "25%           0.244022\n",
+       "50%           0.590215\n",
+       "75%           1.207549\n",
+       "max           4.999928\n",
+       "Name: Dorsal Interossei, dtype: float64"
+      ]
+     },
+     "execution_count": 126,
+     "metadata": {},
+     "output_type": "execute_result"
+    },
+    {
+     "data": {
+      "image/png": "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\n",
+      "text/plain": [
+       "<Figure size 432x288 with 1 Axes>"
+      ]
+     },
+     "metadata": {
+      "needs_background": "light"
+     },
+     "output_type": "display_data"
+    }
+   ],
+   "source": [
+    "tendon_idx = 0\n",
+    "# tensions_df[tensions_df.columns[tendon_idx]].plot(kind='density', xlim=(-0.5,5.5))\n",
+    "tensions_df[tensions_df.columns[tendon_idx]].plot(kind='hist', bins=50)\n",
+    "plt.title(f\"{tensions_df.columns[tendon_idx]} Tension Histogram plot\")\n",
+    "_ = plt.xlabel(\"Tendon Tension [N]\")\n",
+    "# _ = plt.ylabel(\"Count\")\n",
+    "\n",
+    "tensions_df[tensions_df.columns[tendon_idx]].describe()"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "id": "4d02519f-98c7-48d8-a9af-e266565e5758",
+   "metadata": {},
+   "outputs": [],
+   "source": []
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "id": "fac02162-fb76-4e93-a1f9-373253dc4ccd",
+   "metadata": {},
+   "outputs": [],
+   "source": []
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "id": "dd03e71f-e842-45a8-9638-5477d0efd2c7",
+   "metadata": {},
+   "outputs": [],
+   "source": []
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "id": "e4d81643-ff6f-441b-a92f-d6b48df9de74",
+   "metadata": {},
+   "outputs": [],
+   "source": []
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "id": "ac0254e0-8fa4-4b67-a708-0f9e9e56e5a3",
+   "metadata": {},
+   "outputs": [],
+   "source": []
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 48,
+   "id": "b181d28b-35c8-4620-acde-49eaa01ea458",
+   "metadata": {
+    "tags": []
+   },
+   "outputs": [
+    {
+     "data": {
+      "text/plain": [
+       "<AxesSubplot: >"
+      ]
+     },
+     "execution_count": 48,
+     "metadata": {},
+     "output_type": "execute_result"
+    },
+    {
+     "data": {
+      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD4CAYAAAAXUaZHAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/av/WaAAAACXBIWXMAAAsTAAALEwEAmpwYAAAQ8klEQVR4nO3df4xdZZ3H8fd3y88w2lHAUtrG6SpBCs2iTJANm80txA2CsWz8EQmrxXTT0ECDcXGt+08h2Y0YV2F10aQRs3VXGInIj4DdXQKdGBLRbbUytGPXaspuS2uBUnTUVqHf/WMOWGdnOvfeuT96n3m/ksncc57nnOf7tPQzh+eeeyYyE0lSWf6o2wVIklrPcJekAhnuklQgw12SCmS4S1KBTuh2AQBnnHFGDgwMNHXsr371K0477bTWFtQDnPfs4rxnl3rnvWXLlucz88zJ2o6LcB8YGGDz5s1NHTs8PEytVmttQT3Aec8uznt2qXfeEfHMVG0uy0hSgQx3SSqQ4S5JBTLcJalAhrskFchwl6QCGe6SVCDDXZIKZLhLUoGOi0+ozsjerXDL8vr73/JS20qRpOOFV+6SVCDDXZIKZLhLUoEMd0kqkOEuSQUy3CWpQIa7JBXIcJekAhnuklQgw12SCmS4S1KBDHdJKpDhLkkFMtwlqUCGuyQVyHCXpAIZ7pJUIMNdkgpkuEtSgQx3SSqQ4S5JBTLcJalAhrskFchwl6QCGe6SVCDDXZIKZLhLUoHqDveImBMRP4yIh6vtxRHxvYjYGRHfiIiTqv0nV9s7q/aBNtUuSZpCI1fuNwGjR21/Brg9M98KvAisrPavBF6s9t9e9ZMkdVBd4R4RC4GrgK9U2wFcBnyz6rIBuLp6vbzapmq/vOovSeqQyMzpO0V8E/g08DrgZuA64Mnq6pyIWARszMwLIuJp4IrM3F21/RR4Z2Y+P+Gcq4BVAPPmzbtoaGioqQmMHdhP3+Fn6+4/cmRxU+NMZemCuS09X73Gxsbo6+vrytjd5LxnF+d9bMuWLduSmYOTtZ0w3cER8R5gf2ZuiYhao0VOJTPXA+sBBgcHs1Zr7tTD99xBbce6uvtfd+jupsaZyq5ray09X72Gh4dp9s+slznv2cV5N2/acAcuBd4bEVcCpwCvB/4J6I+IEzLzZWAhsKfqvwdYBOyOiBOAucALM6pSktSQadfcM/NTmbkwMweADwGPZ+a1wCbg/VW3FcCD1euHqm2q9seznrUfSVLLzOQ+908CH4+IncDpwF3V/ruA06v9HwfWzqxESVKj6lmWeU1mDgPD1eufARdP0ucQ8IEW1CZJapKfUJWkAhnuklQgw12SCmS4S1KBDHdJKpDhLkkFMtwlqUCGuyQVyHCXpAIZ7pJUIMNdkgpkuEtSgQx3SSqQ4S5JBTLcJalAhrskFaihX9YhSaVaumFpx8YaWTHS9jG8cpekAhnuklQgw12SCmS4S1KBDHdJKpDhLkkFMtwlqUCGuyQVyHCXpAIZ7pJUoFn7+IFPHDx10v2f7f9NhyuRpNbzyl2SCmS4S1KBDHdJKtCsXXNXfUbfdl5bz3/ej0fben5ptvLKXZIKZLhLUoEMd0kq0LThHhGnRMT3I+JHEbEtIm6t9i+OiO9FxM6I+EZEnFTtP7na3lm1D7R5DpKkCeq5cj8MXJaZfwJcCFwREZcAnwFuz8y3Ai8CK6v+K4EXq/23V/0kSR00bbjnuLFq88TqK4HLgG9W+zcAV1evl1fbVO2XR0S0qmBJ0vQiM6fvFDEH2AK8FbgT+CzwZHV1TkQsAjZm5gUR8TRwRWburtp+CrwzM5+fcM5VwCqAefPmXTQ0NNTUBMYO7Kfv8LN19x85shiAs16Z/OfavjlHGhp/6YK5DfVvlbGxMfr6+to+zqFt29p6/lPOP7+h/p2a9/HGebff9he2d2QcgCWnLzlme73zXrZs2ZbMHJysra773DPzFeDCiOgH7gfeVs9x05xzPbAeYHBwMGu1WlPnGb7nDmo71tXd/7pDdwNTP1vmcw0+W2bXtbWG+rfK8PAwzf6ZNWL0+tVtPX+j97l3at7HG+fdfms2rOnIOAAj7xs5Znsr5t3Q3TKZeRDYBPwp0B8Rr/5wWAjsqV7vARYBVO1zgRdmVKUkqSHTXrlHxJnA7zLzYEScCryL8TdJNwHvB4aAFcCD1SEPVdvfrdofz3rWfmbozn33T7r/hrP+st1DS9Jxp55lmfnAhmrd/Y+AezPz4YjYDgxFxN8DPwTuqvrfBfxrROwEDgAfakPdkqRjmDbcM/Mp4O2T7P8ZcPEk+w8BH2hJdZKkpvgJVUkqkOEuSQUy3CWpQIa7JBXIcJekAvmbmHrRLXPh3FvhluV19n+pvfVIOu545S5JBTLcJalAhrskFchwl6QCGe6SVCDDXZIKZLhLUoEMd0kqkOEuSQUy3CWpQIa7JBXIcJekAhnuklQgw12SCmS4S1KBDHdJKpDhLkkFMtwlqUCGuyQVyHCXpAL5C7LVVQNrH6m7767brmpjJVJZvHKXpAIZ7pJUIMNdkgpkuEtSgQx3SSqQ4S5JBTLcJalAhrskFWjacI+IRRGxKSK2R8S2iLip2v/GiHg0In5SfX9DtT8i4gsRsTMinoqId7R7EpKkP1TPlfvLwN9k5hLgEuCGiFgCrAUey8xzgMeqbYB3A+dUX6uAL7e8aknSMU0b7pm5NzN/UL3+JTAKLACWAxuqbhuAq6vXy4Gv5bgngf6ImN/qwiVJU4vMrL9zxADwHeAC4H8ys7/aH8CLmdkfEQ8Dt2XmE1XbY8AnM3PzhHOtYvzKnnnz5l00NDTU1ATGDuyn7/CzPPe7t0zafuaJP/2D7ZEjiwE465XJf67tm3OkofGXLpjbUP+W2LuVsZPPpu/ws/X1n39h00Md2rat6WPr8ZP+hXX3XbpgLmNjY/T19bWxouOT826/7S9s78g4AEtOX3LM9nrnvWzZsi2ZOThZW90PDouIPuA+4GOZ+YvxPB+XmRkR9f+UGD9mPbAeYHBwMGu1WiOHv2b4njuo7VjHnfvun7T9A2et+4Pt6w7dDcAnDp46af/P9f+mofF3XVtrqH9L3LKc4XNvpbZj3fR9Aa55qemhRq9f3fSx9bj+6n+su++ua2sMDw/T7H8rvcx5t9+aDWs6Mg7AyPtGjtneinnXdbdMRJzIeLB/PTO/Ve3++avLLdX3/dX+PcCiow5fWO2TJHVIPXfLBHAXMJqZnz+q6SFgRfV6BfDgUfs/Ut01cwnwUmbubWHNkqRp1LMscynwYWAkIrZW+/4OuA24NyJWAs8AH6zavg1cCewEfg18tJUFS5KmN224V2+MxhTNl0/SP4EbZliXJGkG/ISqJBXIcJekAhnuklQgw12SCmS4S1KBDHdJKlDdjx/oVRMfS/CJLtUhSZ3klbskFchwl6QCGe6SVCDDXZIKZLhLUoEMd0kqkOEuSQUy3CWpQIa7JBXIcJekAhnuklQgw12SClT8g8N0fNv4wM119x194GYOrbmR0etXNzTGeT8ebbQsHSe2v7CdNRvWdLuMnuSVuyQVyHCXpAIZ7pJUIMNdkgpkuEtSgbxbZhYYWPtI08dubGEdkjrHK3dJKpDhLkkFMtwlqUCGuyQVyHCXpAIZ7pJUIMNdkgpkuEtSgQx3SSrQtOEeEV+NiP0R8fRR+94YEY9GxE+q72+o9kdEfCEidkbEUxHxjnYWL0maXD1X7v8CXDFh31rgscw8B3is2gZ4N3BO9bUK+HJrypQkNWLacM/M7wAHJuxeDmyoXm8Arj5q/9dy3JNAf0TMb1GtkqQ6RWZO3yliAHg4My+otg9mZn/1OoAXM7M/Ih4GbsvMJ6q2x4BPZubmSc65ivGre+bNm3fR0NBQUxMYO7CfvsPP8tzv3tLU8RPtm3Okof5LF8xtybgN2buVsZPPpu/ws3V1HzmyuOmhzjm4u+lj2+G3b3oTJ+3f39Axp5x/fpuq6ZyxsTH6+vq6XUbHPXfwOZ575blul9FyS05fcsz2ev++ly1btiUzBydrm/FTITMzI2L6nxD//7j1wHqAwcHBrNVqTY0/fM8d1Has48599zd1/ESf6/9NQ/13XVtrybgNuWU5w+feSm3Hurq6X3fo7qaH2vjAPzd9bDs8s+ZG3vzFxmoq4XeoDg8P0+y/kV72pfu+xJfHylvdHXnfyDHbW/H33ezdMj9/dbml+v7qpdQeYNFR/RZW+yRJHdRsuD8ErKherwAePGr/R6q7Zi4BXsrMvTOsUZLUoGmXZSLiHqAGnBERu4F1wG3AvRGxEngG+GDV/dvAlcBO4NfAR9tQs9rsdeet/f3GA10rQ9IMTBvumXnNFE2XT9I3gRtmWpQkaWb8NXsTbHzg5ob6j07Rv4Q38ST1LsNdUkOWbljasbFW963u2Fil8dkyklQgr9xVvNG3ndf2MVyG0/HGcO9ho0Nn19VvI429j+AdMlLvc1lGkgpkuEtSgQx3SSqQa+5t0t438c7m0JoT23h+Sb3OK3dJKpDhLkkFMtwlqUCGuyQVyHCXpAIZ7pJUIMNdkgrkfe5SC0z2GNyRFcf+JchSO3nlLkkF8spdapNW/lKL1X2rWbNhzZTt/l+CJvLKXZIKZLhLUoEMd0kqkOEuSQUy3CWpQIa7JBXIcJekAhnuklQgw12SCmS4S1KBfPyA1AL3fvrltp5/0z+09fQqkFfuklQgw12SCmS4S1KBXHOf4PHanZPuv2z4hg5XIv3eH+9L7v3i1Ov6o58+b8ZjfPBTxkFJ/NuUpA4bWPvIa6933XZVW8Zoy7JMRFwRETsiYmdErG3HGJKkqbU83CNiDnAn8G5gCXBNRCxp9TiSpKm1Y1nmYmBnZv4MICKGgOXA9jaMVTTX/1WSZj4L8MyaY7/XMJHvG/xeZGZrTxjxfuCKzPzravvDwDsz88YJ/VYBq6rNc4EdTQ55BvB8k8f2Muc9uzjv2aXeeb85M8+crKFrP+Yycz2wfqbniYjNmTnYgpJ6ivOeXZz37NKKebfjDdU9wKKjthdW+yRJHdKOcP8v4JyIWBwRJwEfAh5qwziSpCm0fFkmM1+OiBuB/wDmAF/NzG2tHucoM17a6VHOe3Zx3rPLzJesW/2GqiSp+3y2jCQVyHCXpAL1bLjP1kccRMRXI2J/RDzd7Vo6JSIWRcSmiNgeEdsi4qZu19QpEXFKRHw/In5Uzf3WbtfUKRExJyJ+GBEPd7uWToqIXRExEhFbI2Jz0+fpxTX36hEH/w28C9jN+B0612Rm8Z+CjYg/B8aAr2XmBd2upxMiYj4wPzN/EBGvA7YAV8+Sv+8ATsvMsYg4EXgCuCkzn+xyaW0XER8HBoHXZ+Z7ul1Pp0TELmAwM2f04a1evXJ/7REHmflb4NVHHBQvM78DHOh2HZ2UmXsz8wfV618Co8CC7lbVGTlurNo8sfrqvSuyBkXEQuAq4CvdrqVX9Wq4LwD+96jt3cySf+yzXUQMAG8HvtflUjqmWp7YCuwHHs3M2TD3O4C/BY50uY5uSOA/I2JL9ZiWpvRquGsWiog+4D7gY5n5i27X0ymZ+UpmXsj4p70vjoiil+Mi4j3A/szc0u1auuTPMvMdjD9Z94ZqKbZhvRruPuJglqnWm+8Dvp6Z3+p2Pd2QmQeBTcAVXS6l3S4F3lutPQ8Bl0XEv3W3pM7JzD3V9/3A/YwvQzesV8PdRxzMItWbincBo5n5+W7X00kRcWZE9FevT2X8JoIfd7WoNsvMT2XmwswcYPzf9uOZ+VddLqsjIuK06qYBIuI04C+Apu6M68lwz8yXgVcfcTAK3NvmRxwcNyLiHuC7wLkRsTsiVna7pg64FPgw41dwW6uvK7tdVIfMBzZFxFOMX9Q8mpmz6tbAWWYe8ERE/Aj4PvBIZv57MyfqyVshJUnH1pNX7pKkYzPcJalAhrskFchwl6QCGe6SVCDDXZIKZLhLUoH+D3gojbKxVQvnAAAAAElFTkSuQmCC\n",
+      "text/plain": [
+       "<Figure size 432x288 with 1 Axes>"
+      ]
+     },
+     "metadata": {
+      "needs_background": "light"
+     },
+     "output_type": "display_data"
+    }
+   ],
+   "source": [
+    "# tensions_df[\"Flexor Digitorum Superficialis\"].hist()\n",
+    "# tensions_df[\"Extensor Digitorum Communis\"].hist()\n",
+    "# tensions_df[\"Dorsal Interossei\"].hist()\n",
+    "# tensions_df[\"Palmar Interossei\"].hist()"
    ]
   },
   {
diff --git a/darm_training/MUJOCO_LOG.TXT b/darm_training/MUJOCO_LOG.TXT
index 0535486..14896ce 100644
--- a/darm_training/MUJOCO_LOG.TXT
+++ b/darm_training/MUJOCO_LOG.TXT
@@ -16,3 +16,21 @@ WARNING: Nan, Inf or huge value in QACC at DOF 0. The simulation is unstable. Ti
 Wed Feb 22 00:15:39 2023
 WARNING: Nan, Inf or huge value in QACC at DOF 0. The simulation is unstable. Time = 418.8080.
 
+Sat Feb 25 17:58:50 2023
+WARNING: Nan, Inf or huge value in QACC at DOF 0. The simulation is unstable. Time = 1385.6520.
+
+Sat Feb 25 18:05:18 2023
+WARNING: Nan, Inf or huge value in QACC at DOF 0. The simulation is unstable. Time = 51.5960.
+
+Sun Feb 26 21:12:36 2023
+WARNING: Nan, Inf or huge value in QACC at DOF 0. The simulation is unstable. Time = 640.4980.
+
+Sun Feb 26 21:31:03 2023
+WARNING: Nan, Inf or huge value in QACC at DOF 0. The simulation is unstable. Time = 893.3940.
+
+Sun Feb 26 21:52:18 2023
+WARNING: Nan, Inf or huge value in QACC at DOF 0. The simulation is unstable. Time = 14.5680.
+
+Sun Feb 26 21:54:49 2023
+WARNING: Nan, Inf or huge value in QACC at DOF 0. The simulation is unstable. Time = 1626.2540.
+
diff --git a/darm_training/results/darm_sf_hand/SF_SB3_SAC_3/best_model.zip b/darm_training/results/darm_sf_hand/SF_SB3_SAC_3/best_model.zip
index 5f04d6d..ddd895d 100644
Binary files a/darm_training/results/darm_sf_hand/SF_SB3_SAC_3/best_model.zip and b/darm_training/results/darm_sf_hand/SF_SB3_SAC_3/best_model.zip differ
diff --git a/darm_training/results/darm_sf_hand/SF_SB3_SAC_3/last_model.zip b/darm_training/results/darm_sf_hand/SF_SB3_SAC_3/last_model.zip
deleted file mode 100644
index 26b5780..0000000
Binary files a/darm_training/results/darm_sf_hand/SF_SB3_SAC_3/last_model.zip and /dev/null differ
diff --git a/darm_training/results/darm_sf_hand/SF_SB3_SAC_3/model.zip b/darm_training/results/darm_sf_hand/SF_SB3_SAC_3/model.zip
index 26b5780..1c14cd5 100644
Binary files a/darm_training/results/darm_sf_hand/SF_SB3_SAC_3/model.zip and b/darm_training/results/darm_sf_hand/SF_SB3_SAC_3/model.zip differ
diff --git a/darm_training/sb3_sac_darm_sf_hand-nb2.ipynb b/darm_training/sb3_sac_darm_sf_hand-nb2.ipynb
index efc2d4f..4d00475 100644
--- a/darm_training/sb3_sac_darm_sf_hand-nb2.ipynb
+++ b/darm_training/sb3_sac_darm_sf_hand-nb2.ipynb
@@ -44,6 +44,32 @@
   {
    "cell_type": "code",
    "execution_count": 3,
+   "id": "fea4d0f1-e0e8-41d0-9c7c-006cb9a9926a",
+   "metadata": {},
+   "outputs": [
+    {
+     "name": "stdout",
+     "output_type": "stream",
+     "text": [
+      "Running generate_darm_xml.sh\n",
+      "Single Finger: true\n",
+      "No Wrist: true\n",
+      "\n",
+      "\n",
+      "\n",
+      "\n"
+     ]
+    }
+   ],
+   "source": [
+    "%%bash\n",
+    "cd ../mujoco_env\n",
+    "bash generate_darm_xml.sh true true"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 5,
    "id": "8a6733be-66a7-4aa7-81b5-b1b28b0a0efd",
    "metadata": {},
    "outputs": [
@@ -78,17 +104,130 @@
   },
   {
    "cell_type": "code",
-   "execution_count": null,
+   "execution_count": 6,
    "id": "3c027ec5-151e-4986-a444-2107e98fb741",
-   "metadata": {},
-   "outputs": [],
+   "metadata": {
+    "collapsed": true,
+    "jupyter": {
+     "outputs_hidden": true
+    },
+    "tags": []
+   },
+   "outputs": [
+    {
+     "name": "stdout",
+     "output_type": "stream",
+     "text": [
+      "running install\n",
+      "running bdist_egg\n",
+      "running egg_info\n",
+      "writing darm_gym_env.egg-info/PKG-INFO\n",
+      "writing dependency_links to darm_gym_env.egg-info/dependency_links.txt\n",
+      "writing requirements to darm_gym_env.egg-info/requires.txt\n",
+      "writing top-level names to darm_gym_env.egg-info/top_level.txt\n",
+      "reading manifest file 'darm_gym_env.egg-info/SOURCES.txt'\n",
+      "writing manifest file 'darm_gym_env.egg-info/SOURCES.txt'\n",
+      "installing library code to build/bdist.linux-x86_64/egg\n",
+      "running install_lib\n",
+      "running build_py\n",
+      "creating build/bdist.linux-x86_64/egg\n",
+      "creating build/bdist.linux-x86_64/egg/darm_gym_env\n",
+      "copying build/lib/darm_gym_env/darm_sf_gym.py -> build/bdist.linux-x86_64/egg/darm_gym_env\n",
+      "copying build/lib/darm_gym_env/__init__.py -> build/bdist.linux-x86_64/egg/darm_gym_env\n",
+      "copying build/lib/darm_gym_env/multi_darm_gym.py -> build/bdist.linux-x86_64/egg/darm_gym_env\n",
+      "copying build/lib/darm_gym_env/darm_gym.py -> build/bdist.linux-x86_64/egg/darm_gym_env\n",
+      "copying build/lib/darm_gym_env/env_test.py -> build/bdist.linux-x86_64/egg/darm_gym_env\n",
+      "byte-compiling build/bdist.linux-x86_64/egg/darm_gym_env/darm_sf_gym.py to darm_sf_gym.cpython-38.pyc\n",
+      "byte-compiling build/bdist.linux-x86_64/egg/darm_gym_env/__init__.py to __init__.cpython-38.pyc\n",
+      "byte-compiling build/bdist.linux-x86_64/egg/darm_gym_env/multi_darm_gym.py to multi_darm_gym.cpython-38.pyc\n",
+      "byte-compiling build/bdist.linux-x86_64/egg/darm_gym_env/darm_gym.py to darm_gym.cpython-38.pyc\n",
+      "byte-compiling build/bdist.linux-x86_64/egg/darm_gym_env/env_test.py to env_test.cpython-38.pyc\n",
+      "creating build/bdist.linux-x86_64/egg/EGG-INFO\n",
+      "copying darm_gym_env.egg-info/PKG-INFO -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
+      "copying darm_gym_env.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
+      "copying darm_gym_env.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
+      "copying darm_gym_env.egg-info/requires.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
+      "copying darm_gym_env.egg-info/top_level.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
+      "creating 'dist/darm_gym_env-0.0.1-py3.8.egg' and adding 'build/bdist.linux-x86_64/egg' to it\n"
+     ]
+    },
+    {
+     "name": "stderr",
+     "output_type": "stream",
+     "text": [
+      "/home/daniel/miniconda3/lib/python3.8/site-packages/setuptools/command/install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools.\n",
+      "  warnings.warn(\n",
+      "/home/daniel/miniconda3/lib/python3.8/site-packages/setuptools/command/easy_install.py:144: EasyInstallDeprecationWarning: easy_install command is deprecated. Use build and pip and other standards-based tools.\n",
+      "  warnings.warn(\n",
+      "zip_safe flag not set; analyzing archive contents...\n",
+      "darm_gym_env.__pycache__.multi_darm_gym.cpython-38: module references __file__\n"
+     ]
+    },
+    {
+     "name": "stdout",
+     "output_type": "stream",
+     "text": [
+      "removing 'build/bdist.linux-x86_64/egg' (and everything under it)\n",
+      "Processing darm_gym_env-0.0.1-py3.8.egg\n",
+      "removing '/home/daniel/miniconda3/lib/python3.8/site-packages/darm_gym_env-0.0.1-py3.8.egg' (and everything under it)\n",
+      "creating /home/daniel/miniconda3/lib/python3.8/site-packages/darm_gym_env-0.0.1-py3.8.egg\n",
+      "Extracting darm_gym_env-0.0.1-py3.8.egg to /home/daniel/miniconda3/lib/python3.8/site-packages\n",
+      "darm-gym-env 0.0.1 is already the active version in easy-install.pth\n",
+      "\n",
+      "Installed /home/daniel/miniconda3/lib/python3.8/site-packages/darm_gym_env-0.0.1-py3.8.egg\n",
+      "Processing dependencies for darm-gym-env==0.0.1\n",
+      "Searching for gym==0.21.0\n",
+      "Best match: gym 0.21.0\n",
+      "Adding gym 0.21.0 to easy-install.pth file\n",
+      "\n",
+      "Using /home/daniel/miniconda3/lib/python3.8/site-packages\n",
+      "Searching for mujoco==2.2.2\n",
+      "Best match: mujoco 2.2.2\n",
+      "Processing mujoco-2.2.2-py3.8-linux-x86_64.egg\n",
+      "mujoco 2.2.2 is already the active version in easy-install.pth\n",
+      "\n",
+      "Using /home/daniel/miniconda3/lib/python3.8/site-packages/mujoco-2.2.2-py3.8-linux-x86_64.egg\n",
+      "Searching for cloudpickle==2.2.0\n",
+      "Best match: cloudpickle 2.2.0\n",
+      "Adding cloudpickle 2.2.0 to easy-install.pth file\n",
+      "\n",
+      "Using /home/daniel/miniconda3/lib/python3.8/site-packages\n",
+      "Searching for numpy==1.23.4\n",
+      "Best match: numpy 1.23.4\n",
+      "Adding numpy 1.23.4 to easy-install.pth file\n",
+      "Installing f2py script to /home/daniel/miniconda3/bin\n",
+      "Installing f2py3 script to /home/daniel/miniconda3/bin\n",
+      "Installing f2py3.8 script to /home/daniel/miniconda3/bin\n",
+      "\n",
+      "Using /home/daniel/miniconda3/lib/python3.8/site-packages\n",
+      "Searching for PyOpenGL==3.1.6\n",
+      "Best match: PyOpenGL 3.1.6\n",
+      "Adding PyOpenGL 3.1.6 to easy-install.pth file\n",
+      "\n",
+      "Using /home/daniel/miniconda3/lib/python3.8/site-packages\n",
+      "Searching for glfw==2.5.5\n",
+      "Best match: glfw 2.5.5\n",
+      "Adding glfw 2.5.5 to easy-install.pth file\n",
+      "\n",
+      "Using /home/daniel/miniconda3/lib/python3.8/site-packages\n",
+      "Searching for absl-py==1.2.0\n",
+      "Best match: absl-py 1.2.0\n",
+      "Adding absl-py 1.2.0 to easy-install.pth file\n",
+      "\n",
+      "Using /home/daniel/miniconda3/lib/python3.8/site-packages\n",
+      "Finished processing dependencies for darm-gym-env==0.0.1\n"
+     ]
+    }
+   ],
    "source": [
-    "!python setup.py install"
+    "%%bash\n",
+    "cd ..\n",
+    "python setup.py install"
    ]
   },
   {
    "cell_type": "code",
-   "execution_count": null,
+   "execution_count": 7,
    "id": "9e8b1cb4-49d2-416d-b23b-004c54cefbf1",
    "metadata": {},
    "outputs": [],
@@ -135,13 +274,13 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 3,
+   "execution_count": 4,
    "id": "ea66991b-3d7c-4e2e-8133-c5f358fa797e",
    "metadata": {},
    "outputs": [],
    "source": [
     "import gym\n",
-    "from darm_gym_env import DARMSFEnv\n",
+    "from darm_gym_env import DARMEnv\n",
     "from stable_baselines3 import SAC\n",
     "from stable_baselines3.common.vec_env.subproc_vec_env import SubprocVecEnv\n",
     "from stable_baselines3.common.vec_env.vec_monitor import VecMonitor\n",
@@ -159,13 +298,16 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 4,
+   "execution_count": 5,
    "id": "2d7b4bbe-9994-4dc5-b212-c0f5d6ca25b2",
    "metadata": {},
    "outputs": [],
    "source": [
+    "run_name = \"test1_SF_SB3_SAC_4\"\n",
+    "\n",
     "config = {\n",
-    "    \"env_id\": \"darm/DarmSFHand-v0\",\n",
+    "    \"env_id\": \"darm/DarmHand-v0\", # changed from SF\n",
+    "    \"single_finger_env\": True,\n",
     "    \"algo\": \"SAC\",\n",
     "    \"rl_lib\": \"SB3\",\n",
     "    \n",
@@ -182,13 +324,15 @@
     "    \"no_improvement_min_evals\": 20,\n",
     "    \n",
     "    \"log_interval\": 20, # episodes\n",
-    "    \"wandb_model_save_freq\": 2_000 #5_000 timesteps?\n",
+    "    \"wandb_model_save_freq\": 2_000, #5_000 timesteps?\n",
+    "    \n",
+    "    \"run_local_dir\": f\"{os.getenv('DARM_MUJOCO_PATH')}/darm_training/results/darm_sf_hand/{run_name}\"\n",
     "}"
    ]
   },
   {
    "cell_type": "code",
-   "execution_count": 5,
+   "execution_count": 6,
    "id": "c6bdd726-0edc-4e6b-8c4b-b679c56ce71f",
    "metadata": {},
    "outputs": [
@@ -214,7 +358,7 @@
     {
      "data": {
       "text/html": [
-       "Run data is saved locally in <code>/home/daniel/DARM/darm_mujoco/darm_training/wandb/run-20230221_162741-fwqpzibt</code>"
+       "Run data is saved locally in <code>/home/daniel/DARM/darm_mujoco/darm_training/wandb/run-20230302_145847-jfvnsuje</code>"
       ],
       "text/plain": [
        "<IPython.core.display.HTML object>"
@@ -226,7 +370,7 @@
     {
      "data": {
       "text/html": [
-       "Syncing run <strong><a href='https://wandb.ai/danieladejumo/DARM/runs/fwqpzibt' target=\"_blank\">test4_SF_SB3_SAC</a></strong> to <a href='https://wandb.ai/danieladejumo/DARM' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
+       "Syncing run <strong><a href='https://wandb.ai/danieladejumo/DARM/runs/jfvnsuje' target=\"_blank\">test1_SF_SB3_SAC_4</a></strong> to <a href='https://wandb.ai/danieladejumo/DARM' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
       ],
       "text/plain": [
        "<IPython.core.display.HTML object>"
@@ -250,7 +394,7 @@
     {
      "data": {
       "text/html": [
-       " View run at <a href='https://wandb.ai/danieladejumo/DARM/runs/fwqpzibt' target=\"_blank\">https://wandb.ai/danieladejumo/DARM/runs/fwqpzibt</a>"
+       " View run at <a href='https://wandb.ai/danieladejumo/DARM/runs/jfvnsuje' target=\"_blank\">https://wandb.ai/danieladejumo/DARM/runs/jfvnsuje</a>"
       ],
       "text/plain": [
        "<IPython.core.display.HTML object>"
@@ -261,8 +405,6 @@
     }
    ],
    "source": [
-    "run_name = \"test1_SF_SB3_SAC_3\"\n",
-    "\n",
     "notes = \"\"\"\n",
     "- The environment was updated such that the target is within a range from the start point\n",
     "- Velocity penalty was removed and only effort penalty was used\n",
@@ -273,8 +415,14 @@
     "- max_episode_steps was updated to 200.\n",
     "- Velocity vector (size [3,]) was added to observation. Observation size is now (9,)\n",
     "- Action range was increased to [0, 5]\n",
+    "<Changes: ID 3>\n",
     "- Observation warpper to scale observation from m and m/s to cm and cm/s was applied\n",
+    "<Changes: ID 4>\n",
+    "- Max Tension for Digitorum Extensor Communis was increased to 10\n",
+    "- FIXED: Velocity Observation from (prev_pos - new_pos)/time to (new_pos - prev_pos)/time\n",
+    "- FIXED: Removed weight of 1 from 'sparse', 'solved', and 'done' in reward weighting\n",
     "\n",
+    "- Single-Finger; No Wrist Environment\n",
     "- This run was trained on vast_ai using SB3's SAC algo.\n",
     "\"\"\"\n",
     "\n",
@@ -294,7 +442,7 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 5,
+   "execution_count": 7,
    "id": "376efc5a-273e-4074-ad31-329575675bdc",
    "metadata": {},
    "outputs": [],
@@ -302,12 +450,12 @@
     "from gym.wrappers import TransformObservation\n",
     "# from gym.wrappers import RescaleAction\n",
     "\n",
-    "create_env = lambda: TransformObservation(gym.make(config[\"env_id\"]), lambda obs: obs*100)"
+    "create_env = lambda: TransformObservation(gym.make(config[\"env_id\"], single_finger_env=config[\"single_finger_env\"]), lambda obs: obs*100)"
    ]
   },
   {
    "cell_type": "code",
-   "execution_count": 6,
+   "execution_count": 8,
    "id": "87846de9-eca5-498d-a18d-c41b9363b5ab",
    "metadata": {
     "tags": []
@@ -332,6 +480,8 @@
      "name": "stdout",
      "output_type": "stream",
      "text": [
+      "Loaded XML file successfully\n",
+      "Loaded XML file successfully\n",
       "Loaded XML file successfully\n",
       "Loaded XML file successfully\n",
       "Loaded XML file successfully\n",
@@ -352,12 +502,12 @@
     "            learning_starts=config[\"learning_starts\"],\n",
     "            gradient_steps=NUM_CPU, # num of envs\n",
     "            policy_kwargs=policy_kwargs,\n",
-    "            tensorboard_log=\"./results/darm_sf_hand\")"
+    "            tensorboard_log=config['run_local_dir'])"
    ]
   },
   {
    "cell_type": "code",
-   "execution_count": 7,
+   "execution_count": 9,
    "id": "665027f7-6606-4178-9ddb-614d3c1c4706",
    "metadata": {},
    "outputs": [
@@ -371,10 +521,10 @@
     {
      "data": {
       "text/plain": [
-       "<stable_baselines3.common.callbacks.CallbackList at 0x7f4920e50fa0>"
+       "<stable_baselines3.common.callbacks.CallbackList at 0x7fede6ed14f0>"
       ]
      },
-     "execution_count": 7,
+     "execution_count": 9,
      "metadata": {},
      "output_type": "execute_result"
     }
@@ -383,23 +533,23 @@
     "eval_env = make_vec_env(create_env, n_envs=1, seed=config[\"seed\"])\n",
     "\n",
     "# Stop training when the model reaches the reward threshold\n",
-    "reward_thresh_callback = StopTrainingOnRewardThreshold(reward_threshold=config[\"mean_reward_thresh\"], verbose=1)\n",
+    "# reward_thresh_callback = StopTrainingOnRewardThreshold(reward_threshold=config[\"mean_reward_thresh\"], verbose=1)\n",
     "\n",
     "# Stop training if there is no improvement after more than N evaluations\n",
-    "stop_train_callback = StopTrainingOnNoModelImprovement(\n",
-    "    max_no_improvement_evals=config[\"max_no_improvement_evals\"], \n",
-    "    min_evals=config[\"no_improvement_min_evals\"], \n",
-    "    verbose=1)\n",
+    "# stop_train_callback = StopTrainingOnNoModelImprovement(\n",
+    "#     max_no_improvement_evals=config[\"max_no_improvement_evals\"], \n",
+    "#     min_evals=config[\"no_improvement_min_evals\"], \n",
+    "#     verbose=1)\n",
     "\n",
     "eval_callback = EvalCallback(eval_env, \n",
-    "                             best_model_save_path=f\"./results/darm_sf_hand/{run_name}/models/best\",\n",
-    "                             log_path=f\"./results/darm_sf_hand/{run_name}/models/best/logs\", \n",
+    "                             best_model_save_path=f\"{config['run_local_dir']}/models/best\",\n",
+    "                             log_path=f\"{config['run_local_dir']}/models/best/logs\", \n",
     "                             eval_freq=config[\"eval_freq\"],\n",
-    "                             callback_on_new_best=reward_thresh_callback,\n",
+    "                             # callback_on_new_best=reward_thresh_callback,\n",
     "                             # callback_after_eval=stop_train_callback,\n",
     "                             deterministic=True, render=False, verbose=1)\n",
     "\n",
-    "wandb_callback=WandbCallback(model_save_path=f\"./results/darm_sf_hand/{run_name}/models\",\n",
+    "wandb_callback=WandbCallback(model_save_path=f\"{config['run_local_dir']}/models\",\n",
     "                             model_save_freq=config[\"wandb_model_save_freq\"],\n",
     "                             verbose=2)\n",
     "\n",
@@ -410,13 +560,9 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 8,
+   "execution_count": 10,
    "id": "2866ed92-c5e1-4179-a261-0378da2b421f",
    "metadata": {
-    "collapsed": true,
-    "jupyter": {
-     "outputs_hidden": true
-    },
     "tags": []
    },
    "outputs": [
@@ -424,7 +570,7 @@
      "name": "stdout",
      "output_type": "stream",
      "text": [
-      "Logging to ./results/darm_sf_hand/test4_SF_SB3_SAC_1\n"
+      "Logging to /home/daniel/DARM/darm_mujoco/darm_training/results/darm_sf_hand/test1_SF_SB3_SAC_4/test1_SF_SB3_SAC_4_1\n"
      ]
     },
     {
@@ -441,484 +587,909 @@
      "text": [
       "---------------------------------\n",
       "| rollout/           |          |\n",
-      "|    ep_len_mean     | 91.5     |\n",
-      "|    ep_rew_mean     | -32.1    |\n",
+      "|    ep_len_mean     | 5        |\n",
+      "|    ep_rew_mean     | -48      |\n",
       "| time/              |          |\n",
       "|    episodes        | 20       |\n",
-      "|    fps             | 500      |\n",
-      "|    time_elapsed    | 3        |\n",
-      "|    total_timesteps | 1968     |\n",
+      "|    fps             | 495      |\n",
+      "|    time_elapsed    | 0        |\n",
+      "|    total_timesteps | 210      |\n",
       "---------------------------------\n",
       "---------------------------------\n",
       "| rollout/           |          |\n",
-      "|    ep_len_mean     | 92.2     |\n",
-      "|    ep_rew_mean     | -27.9    |\n",
+      "|    ep_len_mean     | 22.6     |\n",
+      "|    ep_rew_mean     | -40      |\n",
       "| time/              |          |\n",
       "|    episodes        | 40       |\n",
-      "|    fps             | 502      |\n",
-      "|    time_elapsed    | 8        |\n",
-      "|    total_timesteps | 4020     |\n",
+      "|    fps             | 589      |\n",
+      "|    time_elapsed    | 2        |\n",
+      "|    total_timesteps | 1398     |\n",
       "---------------------------------\n",
       "---------------------------------\n",
       "| rollout/           |          |\n",
-      "|    ep_len_mean     | 85.2     |\n",
-      "|    ep_rew_mean     | -30.4    |\n",
+      "|    ep_len_mean     | 23.2     |\n",
+      "|    ep_rew_mean     | -36.4    |\n",
       "| time/              |          |\n",
       "|    episodes        | 60       |\n",
-      "|    fps             | 509      |\n",
-      "|    time_elapsed    | 10       |\n",
-      "|    total_timesteps | 5384     |\n",
-      "---------------------------------\n",
-      "Eval num_timesteps=8000, episode_reward=-50.95 +/- 3.88\n",
-      "Episode length: 17.00 +/- 5.33\n",
-      "---------------------------------\n",
-      "| eval/              |          |\n",
-      "|    mean_ep_length  | 17       |\n",
-      "|    mean_reward     | -50.9    |\n",
-      "| time/              |          |\n",
-      "|    total_timesteps | 8000     |\n",
+      "|    fps             | 577      |\n",
+      "|    time_elapsed    | 2        |\n",
+      "|    total_timesteps | 1620     |\n",
       "---------------------------------\n",
-      "New best mean reward!\n",
       "---------------------------------\n",
       "| rollout/           |          |\n",
-      "|    ep_len_mean     | 99.8     |\n",
-      "|    ep_rew_mean     | -31.9    |\n",
+      "|    ep_len_mean     | 19.1     |\n",
+      "|    ep_rew_mean     | -39.5    |\n",
       "| time/              |          |\n",
       "|    episodes        | 80       |\n",
-      "|    fps             | 488      |\n",
-      "|    time_elapsed    | 16       |\n",
-      "|    total_timesteps | 8228     |\n",
+      "|    fps             | 573      |\n",
+      "|    time_elapsed    | 3        |\n",
+      "|    total_timesteps | 1938     |\n",
       "---------------------------------\n",
       "---------------------------------\n",
       "| rollout/           |          |\n",
-      "|    ep_len_mean     | 99.7     |\n",
-      "|    ep_rew_mean     | -35      |\n",
+      "|    ep_len_mean     | 18.6     |\n",
+      "|    ep_rew_mean     | -40.3    |\n",
       "| time/              |          |\n",
       "|    episodes        | 100      |\n",
-      "|    fps             | 495      |\n",
-      "|    time_elapsed    | 20       |\n",
-      "|    total_timesteps | 10092    |\n",
+      "|    fps             | 576      |\n",
+      "|    time_elapsed    | 3        |\n",
+      "|    total_timesteps | 2208     |\n",
       "---------------------------------\n",
       "---------------------------------\n",
       "| rollout/           |          |\n",
-      "|    ep_len_mean     | 95.4     |\n",
-      "|    ep_rew_mean     | -38.3    |\n",
+      "|    ep_len_mean     | 18.5     |\n",
+      "|    ep_rew_mean     | -40.5    |\n",
       "| time/              |          |\n",
       "|    episodes        | 120      |\n",
-      "|    fps             | 499      |\n",
-      "|    time_elapsed    | 23       |\n",
-      "|    total_timesteps | 11580    |\n",
+      "|    fps             | 572      |\n",
+      "|    time_elapsed    | 4        |\n",
+      "|    total_timesteps | 2400     |\n",
       "---------------------------------\n",
       "---------------------------------\n",
       "| rollout/           |          |\n",
-      "|    ep_len_mean     | 92.5     |\n",
-      "|    ep_rew_mean     | -42.4    |\n",
+      "|    ep_len_mean     | 13.5     |\n",
+      "|    ep_rew_mean     | -40.4    |\n",
       "| time/              |          |\n",
       "|    episodes        | 140      |\n",
-      "|    fps             | 503      |\n",
-      "|    time_elapsed    | 26       |\n",
-      "|    total_timesteps | 13224    |\n",
+      "|    fps             | 576      |\n",
+      "|    time_elapsed    | 4        |\n",
+      "|    total_timesteps | 2604     |\n",
       "---------------------------------\n",
       "---------------------------------\n",
       "| rollout/           |          |\n",
-      "|    ep_len_mean     | 98.3     |\n",
-      "|    ep_rew_mean     | -44      |\n",
+      "|    ep_len_mean     | 11.6     |\n",
+      "|    ep_rew_mean     | -43.6    |\n",
       "| time/              |          |\n",
       "|    episodes        | 160      |\n",
-      "|    fps             | 494      |\n",
-      "|    time_elapsed    | 30       |\n",
-      "|    total_timesteps | 15104    |\n",
-      "---------------------------------\n",
-      "Eval num_timesteps=16000, episode_reward=-41.16 +/- 10.08\n",
-      "Episode length: 127.00 +/- 89.41\n",
-      "---------------------------------\n",
-      "| eval/              |          |\n",
-      "|    mean_ep_length  | 127      |\n",
-      "|    mean_reward     | -41.2    |\n",
-      "| time/              |          |\n",
-      "|    total_timesteps | 16000    |\n",
+      "|    fps             | 578      |\n",
+      "|    time_elapsed    | 4        |\n",
+      "|    total_timesteps | 2760     |\n",
       "---------------------------------\n",
-      "New best mean reward!\n",
       "---------------------------------\n",
       "| rollout/           |          |\n",
-      "|    ep_len_mean     | 90       |\n",
-      "|    ep_rew_mean     | -44.4    |\n",
+      "|    ep_len_mean     | 17.7     |\n",
+      "|    ep_rew_mean     | -41.7    |\n",
       "| time/              |          |\n",
       "|    episodes        | 180      |\n",
-      "|    fps             | 455      |\n",
-      "|    time_elapsed    | 37       |\n",
-      "|    total_timesteps | 17172    |\n",
+      "|    fps             | 594      |\n",
+      "|    time_elapsed    | 6        |\n",
+      "|    total_timesteps | 3948     |\n",
       "---------------------------------\n",
       "---------------------------------\n",
       "| rollout/           |          |\n",
-      "|    ep_len_mean     | 88.2     |\n",
-      "|    ep_rew_mean     | -44.1    |\n",
+      "|    ep_len_mean     | 19.7     |\n",
+      "|    ep_rew_mean     | -35.3    |\n",
       "| time/              |          |\n",
       "|    episodes        | 200      |\n",
-      "|    fps             | 454      |\n",
-      "|    time_elapsed    | 41       |\n",
-      "|    total_timesteps | 18992    |\n",
+      "|    fps             | 596      |\n",
+      "|    time_elapsed    | 7        |\n",
+      "|    total_timesteps | 4194     |\n",
       "---------------------------------\n",
       "---------------------------------\n",
       "| rollout/           |          |\n",
-      "|    ep_len_mean     | 90       |\n",
-      "|    ep_rew_mean     | -41.5    |\n",
+      "|    ep_len_mean     | 21.9     |\n",
+      "|    ep_rew_mean     | -26.5    |\n",
       "| time/              |          |\n",
       "|    episodes        | 220      |\n",
-      "|    fps             | 458      |\n",
-      "|    time_elapsed    | 44       |\n",
-      "|    total_timesteps | 20512    |\n",
+      "|    fps             | 598      |\n",
+      "|    time_elapsed    | 7        |\n",
+      "|    total_timesteps | 4470     |\n",
       "---------------------------------\n",
       "---------------------------------\n",
       "| rollout/           |          |\n",
-      "|    ep_len_mean     | 90.9     |\n",
-      "|    ep_rew_mean     | -38.7    |\n",
+      "|    ep_len_mean     | 22       |\n",
+      "|    ep_rew_mean     | -29.2    |\n",
       "| time/              |          |\n",
       "|    episodes        | 240      |\n",
-      "|    fps             | 461      |\n",
-      "|    time_elapsed    | 48       |\n",
-      "|    total_timesteps | 22248    |\n",
-      "---------------------------------\n",
-      "Eval num_timesteps=24000, episode_reward=-47.28 +/- 8.29\n",
-      "Episode length: 56.60 +/- 71.96\n",
-      "---------------------------------\n",
-      "| eval/              |          |\n",
-      "|    mean_ep_length  | 56.6     |\n",
-      "|    mean_reward     | -47.3    |\n",
-      "| time/              |          |\n",
-      "|    total_timesteps | 24000    |\n",
+      "|    fps             | 587      |\n",
+      "|    time_elapsed    | 8        |\n",
+      "|    total_timesteps | 4752     |\n",
       "---------------------------------\n",
       "---------------------------------\n",
       "| rollout/           |          |\n",
-      "|    ep_len_mean     | 96.2     |\n",
-      "|    ep_rew_mean     | -38.9    |\n",
+      "|    ep_len_mean     | 24       |\n",
+      "|    ep_rew_mean     | -28.4    |\n",
       "| time/              |          |\n",
       "|    episodes        | 260      |\n",
-      "|    fps             | 455      |\n",
-      "|    time_elapsed    | 54       |\n",
-      "|    total_timesteps | 24784    |\n",
+      "|    fps             | 579      |\n",
+      "|    time_elapsed    | 9        |\n",
+      "|    total_timesteps | 5484     |\n",
       "---------------------------------\n",
       "---------------------------------\n",
       "| rollout/           |          |\n",
-      "|    ep_len_mean     | 88.7     |\n",
-      "|    ep_rew_mean     | -40.5    |\n",
+      "|    ep_len_mean     | 19.7     |\n",
+      "|    ep_rew_mean     | -29.4    |\n",
       "| time/              |          |\n",
       "|    episodes        | 280      |\n",
-      "|    fps             | 456      |\n",
-      "|    time_elapsed    | 56       |\n",
-      "|    total_timesteps | 25936    |\n",
+      "|    fps             | 580      |\n",
+      "|    time_elapsed    | 9        |\n",
+      "|    total_timesteps | 5760     |\n",
       "---------------------------------\n",
       "---------------------------------\n",
       "| rollout/           |          |\n",
-      "|    ep_len_mean     | 87.9     |\n",
-      "|    ep_rew_mean     | -40.1    |\n",
+      "|    ep_len_mean     | 21.8     |\n",
+      "|    ep_rew_mean     | -29.2    |\n",
       "| time/              |          |\n",
       "|    episodes        | 300      |\n",
-      "|    fps             | 457      |\n",
-      "|    time_elapsed    | 61       |\n",
-      "|    total_timesteps | 27976    |\n",
+      "|    fps             | 580      |\n",
+      "|    time_elapsed    | 11       |\n",
+      "|    total_timesteps | 6468     |\n",
       "---------------------------------\n",
       "---------------------------------\n",
       "| rollout/           |          |\n",
-      "|    ep_len_mean     | 101      |\n",
-      "|    ep_rew_mean     | -40.6    |\n",
+      "|    ep_len_mean     | 21.3     |\n",
+      "|    ep_rew_mean     | -37.5    |\n",
       "| time/              |          |\n",
       "|    episodes        | 320      |\n",
-      "|    fps             | 459      |\n",
-      "|    time_elapsed    | 66       |\n",
-      "|    total_timesteps | 30712    |\n",
-      "---------------------------------\n",
-      "Eval num_timesteps=32000, episode_reward=-43.29 +/- 9.81\n",
-      "Episode length: 58.40 +/- 71.01\n",
-      "---------------------------------\n",
-      "| eval/              |          |\n",
-      "|    mean_ep_length  | 58.4     |\n",
-      "|    mean_reward     | -43.3    |\n",
-      "| time/              |          |\n",
-      "|    total_timesteps | 32000    |\n",
+      "|    fps             | 579      |\n",
+      "|    time_elapsed    | 11       |\n",
+      "|    total_timesteps | 6894     |\n",
       "---------------------------------\n",
       "---------------------------------\n",
       "| rollout/           |          |\n",
-      "|    ep_len_mean     | 105      |\n",
-      "|    ep_rew_mean     | -43.4    |\n",
+      "|    ep_len_mean     | 24.8     |\n",
+      "|    ep_rew_mean     | -36.5    |\n",
       "| time/              |          |\n",
       "|    episodes        | 340      |\n",
-      "|    fps             | 455      |\n",
-      "|    time_elapsed    | 71       |\n",
-      "|    total_timesteps | 32560    |\n",
+      "|    fps             | 579      |\n",
+      "|    time_elapsed    | 12       |\n",
+      "|    total_timesteps | 7194     |\n",
       "---------------------------------\n",
       "---------------------------------\n",
       "| rollout/           |          |\n",
-      "|    ep_len_mean     | 94.1     |\n",
-      "|    ep_rew_mean     | -44.1    |\n",
+      "|    ep_len_mean     | 23.1     |\n",
+      "|    ep_rew_mean     | -37.4    |\n",
       "| time/              |          |\n",
       "|    episodes        | 360      |\n",
-      "|    fps             | 456      |\n",
-      "|    time_elapsed    | 75       |\n",
-      "|    total_timesteps | 34276    |\n",
+      "|    fps             | 578      |\n",
+      "|    time_elapsed    | 12       |\n",
+      "|    total_timesteps | 7380     |\n",
       "---------------------------------\n",
       "---------------------------------\n",
       "| rollout/           |          |\n",
-      "|    ep_len_mean     | 96.7     |\n",
-      "|    ep_rew_mean     | -44      |\n",
+      "|    ep_len_mean     | 21.6     |\n",
+      "|    ep_rew_mean     | -38.3    |\n",
       "| time/              |          |\n",
       "|    episodes        | 380      |\n",
-      "|    fps             | 459      |\n",
-      "|    time_elapsed    | 77       |\n",
-      "|    total_timesteps | 35696    |\n",
+      "|    fps             | 577      |\n",
+      "|    time_elapsed    | 13       |\n",
+      "|    total_timesteps | 7812     |\n",
       "---------------------------------\n",
       "---------------------------------\n",
       "| rollout/           |          |\n",
-      "|    ep_len_mean     | 99.6     |\n",
-      "|    ep_rew_mean     | -44.3    |\n",
+      "|    ep_len_mean     | 21.6     |\n",
+      "|    ep_rew_mean     | -29.8    |\n",
       "| time/              |          |\n",
       "|    episodes        | 400      |\n",
-      "|    fps             | 460      |\n",
-      "|    time_elapsed    | 82       |\n",
-      "|    total_timesteps | 37876    |\n",
-      "---------------------------------\n",
-      "Eval num_timesteps=40000, episode_reward=-24.95 +/- 41.75\n",
-      "Episode length: 92.40 +/- 87.95\n",
-      "---------------------------------\n",
-      "| eval/              |          |\n",
-      "|    mean_ep_length  | 92.4     |\n",
-      "|    mean_reward     | -24.9    |\n",
-      "| time/              |          |\n",
-      "|    total_timesteps | 40000    |\n",
+      "|    fps             | 580      |\n",
+      "|    time_elapsed    | 14       |\n",
+      "|    total_timesteps | 8664     |\n",
       "---------------------------------\n",
-      "New best mean reward!\n",
       "---------------------------------\n",
       "| rollout/           |          |\n",
-      "|    ep_len_mean     | 94.9     |\n",
-      "|    ep_rew_mean     | -36.7    |\n",
+      "|    ep_len_mean     | 24.4     |\n",
+      "|    ep_rew_mean     | -27.9    |\n",
       "| time/              |          |\n",
       "|    episodes        | 420      |\n",
-      "|    fps             | 416      |\n",
-      "|    time_elapsed    | 96       |\n",
-      "|    total_timesteps | 40360    |\n",
-      "| train/             |          |\n",
-      "|    actor_loss      | -7.7     |\n",
-      "|    critic_loss     | 15.7     |\n",
-      "|    ent_coef        | 0.899    |\n",
-      "|    ent_coef_loss   | -0.89    |\n",
-      "|    learning_rate   | 0.0003   |\n",
-      "|    n_updates       | 356      |\n",
+      "|    fps             | 580      |\n",
+      "|    time_elapsed    | 15       |\n",
+      "|    total_timesteps | 9000     |\n",
       "---------------------------------\n",
       "---------------------------------\n",
       "| rollout/           |          |\n",
-      "|    ep_len_mean     | 91.6     |\n",
-      "|    ep_rew_mean     | -30.8    |\n",
+      "|    ep_len_mean     | 24.6     |\n",
+      "|    ep_rew_mean     | -27.3    |\n",
       "| time/              |          |\n",
       "|    episodes        | 440      |\n",
-      "|    fps             | 304      |\n",
-      "|    time_elapsed    | 137      |\n",
-      "|    total_timesteps | 41960    |\n",
-      "| train/             |          |\n",
-      "|    actor_loss      | -17.6    |\n",
-      "|    critic_loss     | 25.1     |\n",
-      "|    ent_coef        | 0.557    |\n",
-      "|    ent_coef_loss   | -4.92    |\n",
-      "|    learning_rate   | 0.0003   |\n",
-      "|    n_updates       | 1956     |\n",
+      "|    fps             | 582      |\n",
+      "|    time_elapsed    | 17       |\n",
+      "|    total_timesteps | 9906     |\n",
       "---------------------------------\n",
       "---------------------------------\n",
       "| rollout/           |          |\n",
-      "|    ep_len_mean     | 95.7     |\n",
-      "|    ep_rew_mean     | -30.9    |\n",
+      "|    ep_len_mean     | 27.4     |\n",
+      "|    ep_rew_mean     | -18.9    |\n",
       "| time/              |          |\n",
       "|    episodes        | 460      |\n",
-      "|    fps             | 233      |\n",
-      "|    time_elapsed    | 188      |\n",
-      "|    total_timesteps | 43876    |\n",
-      "| train/             |          |\n",
-      "|    actor_loss      | -22.4    |\n",
-      "|    critic_loss     | 25       |\n",
-      "|    ent_coef        | 0.317    |\n",
-      "|    ent_coef_loss   | -8.85    |\n",
-      "|    learning_rate   | 0.0003   |\n",
-      "|    n_updates       | 3872     |\n",
+      "|    fps             | 582      |\n",
+      "|    time_elapsed    | 17       |\n",
+      "|    total_timesteps | 10206    |\n",
       "---------------------------------\n",
       "---------------------------------\n",
       "| rollout/           |          |\n",
-      "|    ep_len_mean     | 100      |\n",
-      "|    ep_rew_mean     | -12.7    |\n",
+      "|    ep_len_mean     | 27.5     |\n",
+      "|    ep_rew_mean     | -18.4    |\n",
       "| time/              |          |\n",
       "|    episodes        | 480      |\n",
-      "|    fps             | 191      |\n",
-      "|    time_elapsed    | 238      |\n",
-      "|    total_timesteps | 45808    |\n",
-      "| train/             |          |\n",
-      "|    actor_loss      | -23.8    |\n",
-      "|    critic_loss     | 4.18     |\n",
-      "|    ent_coef        | 0.182    |\n",
-      "|    ent_coef_loss   | -10.4    |\n",
-      "|    learning_rate   | 0.0003   |\n",
-      "|    n_updates       | 5804     |\n",
+      "|    fps             | 580      |\n",
+      "|    time_elapsed    | 18       |\n",
+      "|    total_timesteps | 10458    |\n",
+      "---------------------------------\n",
       "---------------------------------\n",
-      "Eval num_timesteps=48000, episode_reward=-37.06 +/- 13.40\n",
-      "Episode length: 200.00 +/- 0.00\n",
+      "| rollout/           |          |\n",
+      "|    ep_len_mean     | 23.3     |\n",
+      "|    ep_rew_mean     | -33.9    |\n",
+      "| time/              |          |\n",
+      "|    episodes        | 500      |\n",
+      "|    fps             | 580      |\n",
+      "|    time_elapsed    | 18       |\n",
+      "|    total_timesteps | 10740    |\n",
+      "---------------------------------\n",
+      "---------------------------------\n",
+      "| rollout/           |          |\n",
+      "|    ep_len_mean     | 22.2     |\n",
+      "|    ep_rew_mean     | -35.5    |\n",
+      "| time/              |          |\n",
+      "|    episodes        | 520      |\n",
+      "|    fps             | 581      |\n",
+      "|    time_elapsed    | 19       |\n",
+      "|    total_timesteps | 11388    |\n",
+      "---------------------------------\n",
+      "Eval num_timesteps=12000, episode_reward=-42.82 +/- 14.96\n",
+      "Episode length: 43.00 +/- 78.51\n",
       "---------------------------------\n",
       "| eval/              |          |\n",
-      "|    mean_ep_length  | 200      |\n",
-      "|    mean_reward     | -37.1    |\n",
+      "|    mean_ep_length  | 43       |\n",
+      "|    mean_reward     | -42.8    |\n",
       "| time/              |          |\n",
-      "|    total_timesteps | 48000    |\n",
-      "| train/             |          |\n",
-      "|    actor_loss      | -20.1    |\n",
-      "|    critic_loss     | 8.23     |\n",
-      "|    ent_coef        | 0.0977   |\n",
-      "|    ent_coef_loss   | -13.9    |\n",
-      "|    learning_rate   | 0.0003   |\n",
-      "|    n_updates       | 7996     |\n",
+      "|    total_timesteps | 12000    |\n",
       "---------------------------------\n",
+      "New best mean reward!\n",
       "---------------------------------\n",
       "| rollout/           |          |\n",
-      "|    ep_len_mean     | 114      |\n",
-      "|    ep_rew_mean     | -2.58    |\n",
+      "|    ep_len_mean     | 22.6     |\n",
+      "|    ep_rew_mean     | -27.3    |\n",
       "| time/              |          |\n",
-      "|    episodes        | 500      |\n",
-      "|    fps             | 148      |\n",
-      "|    time_elapsed    | 330      |\n",
-      "|    total_timesteps | 49132    |\n",
-      "| train/             |          |\n",
-      "|    actor_loss      | -19.4    |\n",
-      "|    critic_loss     | 6.48     |\n",
-      "|    ent_coef        | 0.0746   |\n",
-      "|    ent_coef_loss   | -10.9    |\n",
-      "|    learning_rate   | 0.0003   |\n",
-      "|    n_updates       | 9128     |\n",
+      "|    episodes        | 540      |\n",
+      "|    fps             | 553      |\n",
+      "|    time_elapsed    | 21       |\n",
+      "|    total_timesteps | 12042    |\n",
       "---------------------------------\n",
       "---------------------------------\n",
       "| rollout/           |          |\n",
-      "|    ep_len_mean     | 117      |\n",
-      "|    ep_rew_mean     | -9.55    |\n",
+      "|    ep_len_mean     | 21.9     |\n",
+      "|    ep_rew_mean     | -31.4    |\n",
       "| time/              |          |\n",
-      "|    episodes        | 520      |\n",
-      "|    fps             | 128      |\n",
-      "|    time_elapsed    | 402      |\n",
-      "|    total_timesteps | 51840    |\n",
-      "| train/             |          |\n",
-      "|    actor_loss      | -17.6    |\n",
-      "|    critic_loss     | 2.88     |\n",
-      "|    ent_coef        | 0.048    |\n",
-      "|    ent_coef_loss   | -3.41    |\n",
-      "|    learning_rate   | 0.0003   |\n",
-      "|    n_updates       | 11836    |\n",
+      "|    episodes        | 560      |\n",
+      "|    fps             | 553      |\n",
+      "|    time_elapsed    | 22       |\n",
+      "|    total_timesteps | 12528    |\n",
       "---------------------------------\n",
-      "Saving last checkpoint\n",
-      "Last checkpoint saved in: ./results/darm_sf_hand/test4_SF_SB3_SAC/models/last_model\n"
-     ]
-    },
-    {
-     "ename": "KeyboardInterrupt",
-     "evalue": "",
-     "output_type": "error",
-     "traceback": [
-      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
-      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
-      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m----> 2\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtotal_timesteps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlog_interval\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m                \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mException caught:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
-      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/stable_baselines3/sac/sac.py:309\u001b[0m, in \u001b[0;36mSAC.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    295\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[1;32m    296\u001b[0m     \u001b[38;5;28mself\u001b[39m: SACSelf,\n\u001b[1;32m    297\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    306\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    307\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SACSelf:\n\u001b[0;32m--> 309\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_env\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_eval_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_eval_episodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_log_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_log_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
-      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/stable_baselines3/common/off_policy_algorithm.py:375\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    373\u001b[0m         \u001b[38;5;66;03m# Special case when the user passes `gradient_steps=0`\u001b[39;00m\n\u001b[1;32m    374\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m gradient_steps \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 375\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgradient_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    377\u001b[0m callback\u001b[38;5;241m.\u001b[39mon_training_end()\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
-      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/stable_baselines3/sac/sac.py:265\u001b[0m, in \u001b[0;36mSAC.train\u001b[0;34m(self, gradient_steps, batch_size)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcritic\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m    264\u001b[0m critic_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m--> 265\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcritic\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[38;5;66;03m# Compute actor loss\u001b[39;00m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;66;03m# Alternative: actor_loss = th.mean(log_prob - qf1_pi)\u001b[39;00m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;66;03m# Min over all critic networks\u001b[39;00m\n\u001b[1;32m    270\u001b[0m q_values_pi \u001b[38;5;241m=\u001b[39m th\u001b[38;5;241m.\u001b[39mcat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcritic(replay_data\u001b[38;5;241m.\u001b[39mobservations, actions_pi), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
-      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/optim/optimizer.py:140\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m profile_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimizer.step#\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.step\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m--> 140\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m     obj\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
-      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/optim/optimizer.py:23\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     22\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m---> 23\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     25\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(prev_grad)\n",
-      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/optim/adam.py:234\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure, grad_scaler)\u001b[0m\n\u001b[1;32m    231\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`requires_grad` is not supported for `step` in differentiable mode\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    232\u001b[0m             state_steps\u001b[38;5;241m.\u001b[39mappend(state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstep\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m--> 234\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[43m         \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[43m         \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m         \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[43m         \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[43m         \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    240\u001b[0m \u001b[43m         \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    241\u001b[0m \u001b[43m         \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    242\u001b[0m \u001b[43m         \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[43m         \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    244\u001b[0m \u001b[43m         \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[43m         \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[43m         \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[43m         \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[43m         \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[43m         \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[43m         \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m         \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m         \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
-      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/optim/adam.py:300\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    298\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 300\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    301\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    302\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    303\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    304\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[43m     \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    306\u001b[0m \u001b[43m     \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    307\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    308\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m     \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[43m     \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m     \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m     \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m     \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m)\u001b[49m\n",
-      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/optim/adam.py:412\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    409\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    410\u001b[0m     denom \u001b[38;5;241m=\u001b[39m (exp_avg_sq\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[0;32m--> 412\u001b[0m \u001b[43mparam\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maddcdiv_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexp_avg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdenom\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mstep_size\u001b[49m\u001b[43m)\u001b[49m\n",
-      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
-     ]
-    }
-   ],
-   "source": [
-    "try:\n",
-    "    model.learn(total_timesteps=config[\"total_timesteps\"], \n",
-    "                log_interval=config[\"log_interval\"], \n",
-    "                tb_log_name=run_name,\n",
-    "                callback=callback)\n",
-    "except Exception as e:\n",
-    "    print(\"Exception caught:\")\n",
-    "    print(e)\n",
-    "finally:\n",
-    "    # timestamp = f\"{datetime.now().date()}__{datetime.now().time()}\"\n",
-    "    print(\"Saving last checkpoint\")\n",
-    "    model_name = f\"./results/darm_sf_hand/{run_name}/models/last_model\"\n",
-    "    env_norm_name = f\"./results/darm_sf_hand/{run_name}/env_norm\"\n",
-    "    model.save(model_name)\n",
-    "    print(f\"Last checkpoint saved in: {model_name}\")\n",
-    "    # env.save(env_norm_name) # FIXME: Remember to save norm params if using VecNorm env\n",
-    "    "
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": 9,
-   "id": "139daf70-71ae-4b08-8798-a9e86b0a87a0",
-   "metadata": {
-    "collapsed": true,
-    "jupyter": {
-     "outputs_hidden": true
-    },
-    "tags": []
-   },
-   "outputs": [
-    {
-     "data": {
-      "text/html": [
-       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
-      ],
-      "text/plain": [
-       "<IPython.core.display.HTML object>"
-      ]
-     },
-     "metadata": {},
-     "output_type": "display_data"
-    },
-    {
-     "data": {
-      "text/html": [
-       "<style>\n",
-       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
-       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
-       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
-       "    </style>\n",
-       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/mean_ep_length</td><td>▁▅▃▃▄█</td></tr><tr><td>eval/mean_reward</td><td>▁▄▂▃█▅</td></tr><tr><td>global_step</td><td>▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇██</td></tr><tr><td>rollout/ep_len_mean</td><td>▂▃▁▄▄▃▃▄▂▂▂▂▃▂▂▅▅▃▄▄▃▂▃▄▇█</td></tr><tr><td>rollout/ep_rew_mean</td><td>▃▄▃▃▃▂▁▁▁▁▁▂▂▂▂▂▁▁▁▁▂▃▃▆█▇</td></tr><tr><td>time/fps</td><td>████████▇▇▇▇▇▇▇▇▇▇▇▇▆▄▃▂▁▁</td></tr><tr><td>train/actor_loss</td><td>█▄▂▁▃▃▄</td></tr><tr><td>train/critic_loss</td><td>▅██▁▃▂▁</td></tr><tr><td>train/ent_coef</td><td>█▅▃▂▁▁▁</td></tr><tr><td>train/ent_coef_loss</td><td>█▆▄▃▁▃▇</td></tr><tr><td>train/learning_rate</td><td>▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/mean_ep_length</td><td>200.0</td></tr><tr><td>eval/mean_reward</td><td>-37.05579</td></tr><tr><td>global_step</td><td>51840</td></tr><tr><td>rollout/ep_len_mean</td><td>116.95</td></tr><tr><td>rollout/ep_rew_mean</td><td>-9.54759</td></tr><tr><td>time/fps</td><td>128.0</td></tr><tr><td>train/actor_loss</td><td>-17.60356</td></tr><tr><td>train/critic_loss</td><td>2.88043</td></tr><tr><td>train/ent_coef</td><td>0.04804</td></tr><tr><td>train/ent_coef_loss</td><td>-3.41416</td></tr><tr><td>train/learning_rate</td><td>0.0003</td></tr></table><br/></div></div>"
-      ],
-      "text/plain": [
-       "<IPython.core.display.HTML object>"
-      ]
-     },
-     "metadata": {},
-     "output_type": "display_data"
-    },
-    {
-     "data": {
-      "text/html": [
-       " View run <strong style=\"color:#cdcd00\">test4_SF_SB3_SAC</strong> at: <a href='https://wandb.ai/danieladejumo/DARM/runs/fwqpzibt' target=\"_blank\">https://wandb.ai/danieladejumo/DARM/runs/fwqpzibt</a><br/>Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 3 other file(s)"
-      ],
-      "text/plain": [
-       "<IPython.core.display.HTML object>"
-      ]
-     },
-     "metadata": {},
-     "output_type": "display_data"
-    },
-    {
-     "data": {
-      "text/html": [
-       "Find logs at: <code>./wandb/run-20230221_162741-fwqpzibt/logs</code>"
-      ],
-      "text/plain": [
-       "<IPython.core.display.HTML object>"
-      ]
-     },
-     "metadata": {},
-     "output_type": "display_data"
-    },
+      "---------------------------------\n",
+      "| rollout/           |          |\n",
+      "|    ep_len_mean     | 31.2     |\n",
+      "|    ep_rew_mean     | -0.488   |\n",
+      "| time/              |          |\n",
+      "|    episodes        | 580      |\n",
+      "|    fps             | 545      |\n",
+      "|    time_elapsed    | 24       |\n",
+      "|    total_timesteps | 13620    |\n",
+      "---------------------------------\n",
+      "---------------------------------\n",
+      "| rollout/           |          |\n",
+      "|    ep_len_mean     | 35       |\n",
+      "|    ep_rew_mean     | 0.27     |\n",
+      "| time/              |          |\n",
+      "|    episodes        | 600      |\n",
+      "|    fps             | 548      |\n",
+      "|    time_elapsed    | 26       |\n",
+      "|    total_timesteps | 14520    |\n",
+      "---------------------------------\n",
+      "---------------------------------\n",
+      "| rollout/           |          |\n",
+      "|    ep_len_mean     | 35.5     |\n",
+      "|    ep_rew_mean     | 9.08     |\n",
+      "| time/              |          |\n",
+      "|    episodes        | 620      |\n",
+      "|    fps             | 548      |\n",
+      "|    time_elapsed    | 27       |\n",
+      "|    total_timesteps | 14898    |\n",
+      "---------------------------------\n",
+      "---------------------------------\n",
+      "| rollout/           |          |\n",
+      "|    ep_len_mean     | 30.3     |\n",
+      "|    ep_rew_mean     | 2.21     |\n",
+      "| time/              |          |\n",
+      "|    episodes        | 640      |\n",
+      "|    fps             | 549      |\n",
+      "|    time_elapsed    | 27       |\n",
+      "|    total_timesteps | 15150    |\n",
+      "---------------------------------\n",
+      "---------------------------------\n",
+      "| rollout/           |          |\n",
+      "|    ep_len_mean     | 30.6     |\n",
+      "|    ep_rew_mean     | 1.14     |\n",
+      "| time/              |          |\n",
+      "|    episodes        | 660      |\n",
+      "|    fps             | 549      |\n",
+      "|    time_elapsed    | 28       |\n",
+      "|    total_timesteps | 15690    |\n",
+      "---------------------------------\n",
+      "---------------------------------\n",
+      "| rollout/           |          |\n",
+      "|    ep_len_mean     | 26.6     |\n",
+      "|    ep_rew_mean     | -18.6    |\n",
+      "| time/              |          |\n",
+      "|    episodes        | 680      |\n",
+      "|    fps             | 549      |\n",
+      "|    time_elapsed    | 29       |\n",
+      "|    total_timesteps | 16386    |\n",
+      "---------------------------------\n",
+      "---------------------------------\n",
+      "| rollout/           |          |\n",
+      "|    ep_len_mean     | 23       |\n",
+      "|    ep_rew_mean     | -17.2    |\n",
+      "| time/              |          |\n",
+      "|    episodes        | 700      |\n",
+      "|    fps             | 549      |\n",
+      "|    time_elapsed    | 30       |\n",
+      "|    total_timesteps | 16854    |\n",
+      "---------------------------------\n",
+      "---------------------------------\n",
+      "| rollout/           |          |\n",
+      "|    ep_len_mean     | 24.1     |\n",
+      "|    ep_rew_mean     | -26.2    |\n",
+      "| time/              |          |\n",
+      "|    episodes        | 720      |\n",
+      "|    fps             | 548      |\n",
+      "|    time_elapsed    | 31       |\n",
+      "|    total_timesteps | 17130    |\n",
+      "---------------------------------\n",
+      "---------------------------------\n",
+      "| rollout/           |          |\n",
+      "|    ep_len_mean     | 25.2     |\n",
+      "|    ep_rew_mean     | -29.5    |\n",
+      "| time/              |          |\n",
+      "|    episodes        | 740      |\n",
+      "|    fps             | 549      |\n",
+      "|    time_elapsed    | 31       |\n",
+      "|    total_timesteps | 17436    |\n",
+      "---------------------------------\n",
+      "---------------------------------\n",
+      "| rollout/           |          |\n",
+      "|    ep_len_mean     | 23.2     |\n",
+      "|    ep_rew_mean     | -32.6    |\n",
+      "| time/              |          |\n",
+      "|    episodes        | 760      |\n",
+      "|    fps             | 549      |\n",
+      "|    time_elapsed    | 32       |\n",
+      "|    total_timesteps | 18114    |\n",
+      "---------------------------------\n",
+      "---------------------------------\n",
+      "| rollout/           |          |\n",
+      "|    ep_len_mean     | 22.6     |\n",
+      "|    ep_rew_mean     | -43      |\n",
+      "| time/              |          |\n",
+      "|    episodes        | 780      |\n",
+      "|    fps             | 550      |\n",
+      "|    time_elapsed    | 33       |\n",
+      "|    total_timesteps | 18666    |\n",
+      "---------------------------------\n",
+      "---------------------------------\n",
+      "| rollout/           |          |\n",
+      "|    ep_len_mean     | 24.4     |\n",
+      "|    ep_rew_mean     | -34.8    |\n",
+      "| time/              |          |\n",
+      "|    episodes        | 800      |\n",
+      "|    fps             | 551      |\n",
+      "|    time_elapsed    | 34       |\n",
+      "|    total_timesteps | 19014    |\n",
+      "---------------------------------\n",
+      "---------------------------------\n",
+      "| rollout/           |          |\n",
+      "|    ep_len_mean     | 23.9     |\n",
+      "|    ep_rew_mean     | -34.8    |\n",
+      "| time/              |          |\n",
+      "|    episodes        | 820      |\n",
+      "|    fps             | 552      |\n",
+      "|    time_elapsed    | 36       |\n",
+      "|    total_timesteps | 19926    |\n",
+      "---------------------------------\n",
+      "---------------------------------\n",
+      "| rollout/           |          |\n",
+      "|    ep_len_mean     | 26.7     |\n",
+      "|    ep_rew_mean     | -22.9    |\n",
+      "| time/              |          |\n",
+      "|    episodes        | 840      |\n",
+      "|    fps             | 552      |\n",
+      "|    time_elapsed    | 37       |\n",
+      "|    total_timesteps | 20496    |\n",
+      "---------------------------------\n",
+      "---------------------------------\n",
+      "| rollout/           |          |\n",
+      "|    ep_len_mean     | 32.7     |\n",
+      "|    ep_rew_mean     | -19.5    |\n",
+      "| time/              |          |\n",
+      "|    episodes        | 860      |\n",
+      "|    fps             | 553      |\n",
+      "|    time_elapsed    | 38       |\n",
+      "|    total_timesteps | 21138    |\n",
+      "---------------------------------\n",
+      "---------------------------------\n",
+      "| rollout/           |          |\n",
+      "|    ep_len_mean     | 29.6     |\n",
+      "|    ep_rew_mean     | -13      |\n",
+      "| time/              |          |\n",
+      "|    episodes        | 880      |\n",
+      "|    fps             | 554      |\n",
+      "|    time_elapsed    | 38       |\n",
+      "|    total_timesteps | 21486    |\n",
+      "---------------------------------\n",
+      "---------------------------------\n",
+      "| rollout/           |          |\n",
+      "|    ep_len_mean     | 31.4     |\n",
+      "|    ep_rew_mean     | -20.5    |\n",
+      "| time/              |          |\n",
+      "|    episodes        | 900      |\n",
+      "|    fps             | 552      |\n",
+      "|    time_elapsed    | 40       |\n",
+      "|    total_timesteps | 22434    |\n",
+      "---------------------------------\n",
+      "---------------------------------\n",
+      "| rollout/           |          |\n",
+      "|    ep_len_mean     | 31       |\n",
+      "|    ep_rew_mean     | -20.2    |\n",
+      "| time/              |          |\n",
+      "|    episodes        | 920      |\n",
+      "|    fps             | 552      |\n",
+      "|    time_elapsed    | 41       |\n",
+      "|    total_timesteps | 22650    |\n",
+      "---------------------------------\n",
+      "---------------------------------\n",
+      "| rollout/           |          |\n",
+      "|    ep_len_mean     | 28.7     |\n",
+      "|    ep_rew_mean     | -32.2    |\n",
+      "| time/              |          |\n",
+      "|    episodes        | 940      |\n",
+      "|    fps             | 550      |\n",
+      "|    time_elapsed    | 41       |\n",
+      "|    total_timesteps | 22908    |\n",
+      "---------------------------------\n",
+      "---------------------------------\n",
+      "| rollout/           |          |\n",
+      "|    ep_len_mean     | 21.6     |\n",
+      "|    ep_rew_mean     | -36      |\n",
+      "| time/              |          |\n",
+      "|    episodes        | 960      |\n",
+      "|    fps             | 548      |\n",
+      "|    time_elapsed    | 42       |\n",
+      "|    total_timesteps | 23100    |\n",
+      "---------------------------------\n",
+      "---------------------------------\n",
+      "| rollout/           |          |\n",
+      "|    ep_len_mean     | 19.4     |\n",
+      "|    ep_rew_mean     | -43.5    |\n",
+      "| time/              |          |\n",
+      "|    episodes        | 980      |\n",
+      "|    fps             | 548      |\n",
+      "|    time_elapsed    | 43       |\n",
+      "|    total_timesteps | 23658    |\n",
+      "---------------------------------\n",
+      "Eval num_timesteps=24000, episode_reward=-47.14 +/- 3.87\n",
+      "Episode length: 4.40 +/- 1.36\n",
+      "---------------------------------\n",
+      "| eval/              |          |\n",
+      "|    mean_ep_length  | 4.4      |\n",
+      "|    mean_reward     | -47.1    |\n",
+      "| time/              |          |\n",
+      "|    total_timesteps | 24000    |\n",
+      "---------------------------------\n",
+      "---------------------------------\n",
+      "| rollout/           |          |\n",
+      "|    ep_len_mean     | 23.4     |\n",
+      "|    ep_rew_mean     | -25      |\n",
+      "| time/              |          |\n",
+      "|    episodes        | 1000     |\n",
+      "|    fps             | 546      |\n",
+      "|    time_elapsed    | 44       |\n",
+      "|    total_timesteps | 24510    |\n",
+      "---------------------------------\n",
+      "---------------------------------\n",
+      "| rollout/           |          |\n",
+      "|    ep_len_mean     | 19.5     |\n",
+      "|    ep_rew_mean     | -25.6    |\n",
+      "| time/              |          |\n",
+      "|    episodes        | 1020     |\n",
+      "|    fps             | 545      |\n",
+      "|    time_elapsed    | 45       |\n",
+      "|    total_timesteps | 24852    |\n",
+      "---------------------------------\n",
+      "---------------------------------\n",
+      "| rollout/           |          |\n",
+      "|    ep_len_mean     | 20.1     |\n",
+      "|    ep_rew_mean     | -24.6    |\n",
+      "| time/              |          |\n",
+      "|    episodes        | 1040     |\n",
+      "|    fps             | 545      |\n",
+      "|    time_elapsed    | 46       |\n",
+      "|    total_timesteps | 25194    |\n",
+      "---------------------------------\n",
+      "---------------------------------\n",
+      "| rollout/           |          |\n",
+      "|    ep_len_mean     | 23.1     |\n",
+      "|    ep_rew_mean     | -24      |\n",
+      "| time/              |          |\n",
+      "|    episodes        | 1060     |\n",
+      "|    fps             | 546      |\n",
+      "|    time_elapsed    | 46       |\n",
+      "|    total_timesteps | 25614    |\n",
+      "---------------------------------\n",
+      "---------------------------------\n",
+      "| rollout/           |          |\n",
+      "|    ep_len_mean     | 24       |\n",
+      "|    ep_rew_mean     | -23.6    |\n",
+      "| time/              |          |\n",
+      "|    episodes        | 1080     |\n",
+      "|    fps             | 546      |\n",
+      "|    time_elapsed    | 47       |\n",
+      "|    total_timesteps | 25974    |\n",
+      "---------------------------------\n",
+      "---------------------------------\n",
+      "| rollout/           |          |\n",
+      "|    ep_len_mean     | 20.6     |\n",
+      "|    ep_rew_mean     | -44.1    |\n",
+      "| time/              |          |\n",
+      "|    episodes        | 1100     |\n",
+      "|    fps             | 547      |\n",
+      "|    time_elapsed    | 48       |\n",
+      "|    total_timesteps | 26424    |\n",
+      "---------------------------------\n",
+      "---------------------------------\n",
+      "| rollout/           |          |\n",
+      "|    ep_len_mean     | 28.4     |\n",
+      "|    ep_rew_mean     | -36      |\n",
+      "| time/              |          |\n",
+      "|    episodes        | 1120     |\n",
+      "|    fps             | 550      |\n",
+      "|    time_elapsed    | 49       |\n",
+      "|    total_timesteps | 27504    |\n",
+      "---------------------------------\n",
+      "---------------------------------\n",
+      "| rollout/           |          |\n",
+      "|    ep_len_mean     | 26.4     |\n",
+      "|    ep_rew_mean     | -31.4    |\n",
+      "| time/              |          |\n",
+      "|    episodes        | 1140     |\n",
+      "|    fps             | 550      |\n",
+      "|    time_elapsed    | 50       |\n",
+      "|    total_timesteps | 27768    |\n",
+      "---------------------------------\n",
+      "---------------------------------\n",
+      "| rollout/           |          |\n",
+      "|    ep_len_mean     | 34.4     |\n",
+      "|    ep_rew_mean     | -6.9     |\n",
+      "| time/              |          |\n",
+      "|    episodes        | 1160     |\n",
+      "|    fps             | 554      |\n",
+      "|    time_elapsed    | 52       |\n",
+      "|    total_timesteps | 29034    |\n",
+      "---------------------------------\n",
+      "---------------------------------\n",
+      "| rollout/           |          |\n",
+      "|    ep_len_mean     | 43.3     |\n",
+      "|    ep_rew_mean     | -4.62    |\n",
+      "| time/              |          |\n",
+      "|    episodes        | 1180     |\n",
+      "|    fps             | 550      |\n",
+      "|    time_elapsed    | 54       |\n",
+      "|    total_timesteps | 30264    |\n",
+      "---------------------------------\n",
+      "---------------------------------\n",
+      "| rollout/           |          |\n",
+      "|    ep_len_mean     | 38.8     |\n",
+      "|    ep_rew_mean     | -5.32    |\n",
+      "| time/              |          |\n",
+      "|    episodes        | 1200     |\n",
+      "|    fps             | 550      |\n",
+      "|    time_elapsed    | 55       |\n",
+      "|    total_timesteps | 30516    |\n",
+      "---------------------------------\n",
+      "---------------------------------\n",
+      "| rollout/           |          |\n",
+      "|    ep_len_mean     | 33.2     |\n",
+      "|    ep_rew_mean     | -13.2    |\n",
+      "| time/              |          |\n",
+      "|    episodes        | 1220     |\n",
+      "|    fps             | 550      |\n",
+      "|    time_elapsed    | 55       |\n",
+      "|    total_timesteps | 30732    |\n",
+      "---------------------------------\n",
+      "---------------------------------\n",
+      "| rollout/           |          |\n",
+      "|    ep_len_mean     | 33.1     |\n",
+      "|    ep_rew_mean     | -14.3    |\n",
+      "| time/              |          |\n",
+      "|    episodes        | 1240     |\n",
+      "|    fps             | 551      |\n",
+      "|    time_elapsed    | 56       |\n",
+      "|    total_timesteps | 31026    |\n",
+      "---------------------------------\n",
+      "---------------------------------\n",
+      "| rollout/           |          |\n",
+      "|    ep_len_mean     | 23.3     |\n",
+      "|    ep_rew_mean     | -39.4    |\n",
+      "| time/              |          |\n",
+      "|    episodes        | 1260     |\n",
+      "|    fps             | 551      |\n",
+      "|    time_elapsed    | 57       |\n",
+      "|    total_timesteps | 31662    |\n",
+      "---------------------------------\n",
+      "---------------------------------\n",
+      "| rollout/           |          |\n",
+      "|    ep_len_mean     | 19.8     |\n",
+      "|    ep_rew_mean     | -34.1    |\n",
+      "| time/              |          |\n",
+      "|    episodes        | 1280     |\n",
+      "|    fps             | 552      |\n",
+      "|    time_elapsed    | 58       |\n",
+      "|    total_timesteps | 32226    |\n",
+      "---------------------------------\n",
+      "---------------------------------\n",
+      "| rollout/           |          |\n",
+      "|    ep_len_mean     | 21.9     |\n",
+      "|    ep_rew_mean     | -30.2    |\n",
+      "| time/              |          |\n",
+      "|    episodes        | 1300     |\n",
+      "|    fps             | 552      |\n",
+      "|    time_elapsed    | 58       |\n",
+      "|    total_timesteps | 32538    |\n",
+      "---------------------------------\n",
+      "---------------------------------\n",
+      "| rollout/           |          |\n",
+      "|    ep_len_mean     | 22.1     |\n",
+      "|    ep_rew_mean     | -28.3    |\n",
+      "| time/              |          |\n",
+      "|    episodes        | 1320     |\n",
+      "|    fps             | 550      |\n",
+      "|    time_elapsed    | 60       |\n",
+      "|    total_timesteps | 33336    |\n",
+      "---------------------------------\n",
+      "---------------------------------\n",
+      "| rollout/           |          |\n",
+      "|    ep_len_mean     | 30.5     |\n",
+      "|    ep_rew_mean     | -22.9    |\n",
+      "| time/              |          |\n",
+      "|    episodes        | 1340     |\n",
+      "|    fps             | 549      |\n",
+      "|    time_elapsed    | 62       |\n",
+      "|    total_timesteps | 34146    |\n",
+      "---------------------------------\n",
+      "---------------------------------\n",
+      "| rollout/           |          |\n",
+      "|    ep_len_mean     | 38.4     |\n",
+      "|    ep_rew_mean     | -4.81    |\n",
+      "| time/              |          |\n",
+      "|    episodes        | 1360     |\n",
+      "|    fps             | 549      |\n",
+      "|    time_elapsed    | 63       |\n",
+      "|    total_timesteps | 35088    |\n",
+      "---------------------------------\n",
+      "---------------------------------\n",
+      "| rollout/           |          |\n",
+      "|    ep_len_mean     | 31.9     |\n",
+      "|    ep_rew_mean     | -12.4    |\n",
+      "| time/              |          |\n",
+      "|    episodes        | 1380     |\n",
+      "|    fps             | 549      |\n",
+      "|    time_elapsed    | 64       |\n",
+      "|    total_timesteps | 35262    |\n",
+      "---------------------------------\n",
+      "---------------------------------\n",
+      "| rollout/           |          |\n",
+      "|    ep_len_mean     | 30.1     |\n",
+      "|    ep_rew_mean     | -15      |\n",
+      "| time/              |          |\n",
+      "|    episodes        | 1400     |\n",
+      "|    fps             | 549      |\n",
+      "|    time_elapsed    | 64       |\n",
+      "|    total_timesteps | 35694    |\n",
+      "---------------------------------\n",
+      "Eval num_timesteps=36000, episode_reward=235.63 +/- 501.15\n",
+      "Episode length: 83.60 +/- 95.04\n",
+      "---------------------------------\n",
+      "| eval/              |          |\n",
+      "|    mean_ep_length  | 83.6     |\n",
+      "|    mean_reward     | 236      |\n",
+      "| time/              |          |\n",
+      "|    total_timesteps | 36000    |\n",
+      "---------------------------------\n",
+      "New best mean reward!\n",
+      "---------------------------------\n",
+      "| rollout/           |          |\n",
+      "|    ep_len_mean     | 34.3     |\n",
+      "|    ep_rew_mean     | -4.42    |\n",
+      "| time/              |          |\n",
+      "|    episodes        | 1420     |\n",
+      "|    fps             | 538      |\n",
+      "|    time_elapsed    | 67       |\n",
+      "|    total_timesteps | 36396    |\n",
+      "---------------------------------\n",
+      "---------------------------------\n",
+      "| rollout/           |          |\n",
+      "|    ep_len_mean     | 28.8     |\n",
+      "|    ep_rew_mean     | -12.8    |\n",
+      "| time/              |          |\n",
+      "|    episodes        | 1440     |\n",
+      "|    fps             | 539      |\n",
+      "|    time_elapsed    | 69       |\n",
+      "|    total_timesteps | 37242    |\n",
+      "---------------------------------\n",
+      "---------------------------------\n",
+      "| rollout/           |          |\n",
+      "|    ep_len_mean     | 25.6     |\n",
+      "|    ep_rew_mean     | -28.5    |\n",
+      "| time/              |          |\n",
+      "|    episodes        | 1460     |\n",
+      "|    fps             | 539      |\n",
+      "|    time_elapsed    | 70       |\n",
+      "|    total_timesteps | 37836    |\n",
+      "---------------------------------\n",
+      "---------------------------------\n",
+      "| rollout/           |          |\n",
+      "|    ep_len_mean     | 31.5     |\n",
+      "|    ep_rew_mean     | -27.7    |\n",
+      "| time/              |          |\n",
+      "|    episodes        | 1480     |\n",
+      "|    fps             | 538      |\n",
+      "|    time_elapsed    | 72       |\n",
+      "|    total_timesteps | 38922    |\n",
+      "---------------------------------\n",
+      "---------------------------------\n",
+      "| rollout/           |          |\n",
+      "|    ep_len_mean     | 39.4     |\n",
+      "|    ep_rew_mean     | -15.2    |\n",
+      "| time/              |          |\n",
+      "|    episodes        | 1500     |\n",
+      "|    fps             | 517      |\n",
+      "|    time_elapsed    | 77       |\n",
+      "|    total_timesteps | 40104    |\n",
+      "| train/             |          |\n",
+      "|    actor_loss      | -5.3     |\n",
+      "|    critic_loss     | 77.1     |\n",
+      "|    ent_coef        | 0.971    |\n",
+      "|    ent_coef_loss   | -0.248   |\n",
+      "|    learning_rate   | 0.0003   |\n",
+      "|    n_updates       | 102      |\n",
+      "---------------------------------\n",
+      "---------------------------------\n",
+      "| rollout/           |          |\n",
+      "|    ep_len_mean     | 40.8     |\n",
+      "|    ep_rew_mean     | -15.5    |\n",
+      "| time/              |          |\n",
+      "|    episodes        | 1520     |\n",
+      "|    fps             | 456      |\n",
+      "|    time_elapsed    | 88       |\n",
+      "|    total_timesteps | 40446    |\n",
+      "| train/             |          |\n",
+      "|    actor_loss      | -8.63    |\n",
+      "|    critic_loss     | 52.7     |\n",
+      "|    ent_coef        | 0.88     |\n",
+      "|    ent_coef_loss   | -0.869   |\n",
+      "|    learning_rate   | 0.0003   |\n",
+      "|    n_updates       | 444      |\n",
+      "---------------------------------\n",
+      "Saving last checkpoint\n",
+      "Last checkpoint saved in: /home/daniel/DARM/darm_mujoco/darm_training/results/darm_sf_hand/test1_SF_SB3_SAC_4/models/last_model\n"
+     ]
+    },
+    {
+     "ename": "KeyboardInterrupt",
+     "evalue": "",
+     "output_type": "error",
+     "traceback": [
+      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
+      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
+      "Input \u001b[0;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m----> 2\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtotal_timesteps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlog_interval\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m                \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mException caught:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
+      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/stable_baselines3/sac/sac.py:309\u001b[0m, in \u001b[0;36mSAC.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    295\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[1;32m    296\u001b[0m     \u001b[38;5;28mself\u001b[39m: SACSelf,\n\u001b[1;32m    297\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    306\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    307\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SACSelf:\n\u001b[0;32m--> 309\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_env\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_eval_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_eval_episodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_log_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_log_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
+      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/stable_baselines3/common/off_policy_algorithm.py:375\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    373\u001b[0m         \u001b[38;5;66;03m# Special case when the user passes `gradient_steps=0`\u001b[39;00m\n\u001b[1;32m    374\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m gradient_steps \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 375\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgradient_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    377\u001b[0m callback\u001b[38;5;241m.\u001b[39mon_training_end()\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
+      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/stable_baselines3/sac/sac.py:271\u001b[0m, in \u001b[0;36mSAC.train\u001b[0;34m(self, gradient_steps, batch_size)\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[38;5;66;03m# Compute actor loss\u001b[39;00m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;66;03m# Alternative: actor_loss = th.mean(log_prob - qf1_pi)\u001b[39;00m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;66;03m# Min over all critic networks\u001b[39;00m\n\u001b[1;32m    270\u001b[0m q_values_pi \u001b[38;5;241m=\u001b[39m th\u001b[38;5;241m.\u001b[39mcat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcritic(replay_data\u001b[38;5;241m.\u001b[39mobservations, actions_pi), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 271\u001b[0m min_qf_pi, _ \u001b[38;5;241m=\u001b[39m \u001b[43mth\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq_values_pi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    272\u001b[0m actor_loss \u001b[38;5;241m=\u001b[39m (ent_coef \u001b[38;5;241m*\u001b[39m log_prob \u001b[38;5;241m-\u001b[39m min_qf_pi)\u001b[38;5;241m.\u001b[39mmean()\n\u001b[1;32m    273\u001b[0m actor_losses\u001b[38;5;241m.\u001b[39mappend(actor_loss\u001b[38;5;241m.\u001b[39mitem())\n",
+      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
+     ]
+    }
+   ],
+   "source": [
+    "try:\n",
+    "    model.learn(total_timesteps=config[\"total_timesteps\"], \n",
+    "                log_interval=config[\"log_interval\"], \n",
+    "                tb_log_name=run_name,\n",
+    "                callback=callback)\n",
+    "except Exception as e:\n",
+    "    print(\"Exception caught:\")\n",
+    "    print(e)\n",
+    "finally:\n",
+    "    # timestamp = f\"{datetime.now().date()}__{datetime.now().time()}\"\n",
+    "    print(\"Saving last checkpoint\")\n",
+    "    model_name = f\"{config['run_local_dir']}/models/last_model\"\n",
+    "    model.save(model_name)\n",
+    "    print(f\"Last checkpoint saved in: {model_name}\")"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 11,
+   "id": "139daf70-71ae-4b08-8798-a9e86b0a87a0",
+   "metadata": {
+    "tags": []
+   },
+   "outputs": [
+    {
+     "data": {
+      "text/html": [
+       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
+      ],
+      "text/plain": [
+       "<IPython.core.display.HTML object>"
+      ]
+     },
+     "metadata": {},
+     "output_type": "display_data"
+    },
+    {
+     "data": {
+      "text/html": [
+       "<style>\n",
+       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
+       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
+       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
+       "    </style>\n",
+       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/mean_ep_length</td><td>▄▁█</td></tr><tr><td>eval/mean_reward</td><td>▁▁█</td></tr><tr><td>global_step</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇██</td></tr><tr><td>rollout/ep_len_mean</td><td>▁▄▄▄▂▄▄▄▄▅▄▅▅▄▄▆▇▆▅▅▄▅▆▆▆▄▄▄▅▆▇█▆▄▄█▆▆▆█</td></tr><tr><td>rollout/ep_rew_mean</td><td>▁▂▂▂▂▃▃▃▂▂▃▄▅▃▄▇█▇▅▃▂▃▄▄▃▂▄▄▄▂▆▆▅▃▃▆▅▅▃▅</td></tr><tr><td>time/fps</td><td>▃█▇▇▇██▇▇▇▇▇▇▇▆▅▆▆▆▆▆▆▆▆▆▆▅▅▅▆▆▆▆▆▆▆▆▅▅▁</td></tr><tr><td>train/actor_loss</td><td>█▁</td></tr><tr><td>train/critic_loss</td><td>█▁</td></tr><tr><td>train/ent_coef</td><td>█▁</td></tr><tr><td>train/ent_coef_loss</td><td>█▁</td></tr><tr><td>train/learning_rate</td><td>▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/mean_ep_length</td><td>83.6</td></tr><tr><td>eval/mean_reward</td><td>235.63208</td></tr><tr><td>global_step</td><td>40446</td></tr><tr><td>rollout/ep_len_mean</td><td>40.81</td></tr><tr><td>rollout/ep_rew_mean</td><td>-15.53837</td></tr><tr><td>time/fps</td><td>456.0</td></tr><tr><td>train/actor_loss</td><td>-8.62987</td></tr><tr><td>train/critic_loss</td><td>52.72518</td></tr><tr><td>train/ent_coef</td><td>0.88031</td></tr><tr><td>train/ent_coef_loss</td><td>-0.86934</td></tr><tr><td>train/learning_rate</td><td>0.0003</td></tr></table><br/></div></div>"
+      ],
+      "text/plain": [
+       "<IPython.core.display.HTML object>"
+      ]
+     },
+     "metadata": {},
+     "output_type": "display_data"
+    },
+    {
+     "data": {
+      "text/html": [
+       " View run <strong style=\"color:#cdcd00\">test1_SF_SB3_SAC_4</strong> at: <a href='https://wandb.ai/danieladejumo/DARM/runs/jfvnsuje' target=\"_blank\">https://wandb.ai/danieladejumo/DARM/runs/jfvnsuje</a><br/>Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 3 other file(s)"
+      ],
+      "text/plain": [
+       "<IPython.core.display.HTML object>"
+      ]
+     },
+     "metadata": {},
+     "output_type": "display_data"
+    },
+    {
+     "data": {
+      "text/html": [
+       "Find logs at: <code>./wandb/run-20230302_145847-jfvnsuje/logs</code>"
+      ],
+      "text/plain": [
+       "<IPython.core.display.HTML object>"
+      ]
+     },
+     "metadata": {},
+     "output_type": "display_data"
+    },
     {
      "name": "stdout",
      "output_type": "stream",
      "text": [
-      "Finished run test4_SF_SB3_SAC\n"
+      "Finished run test1_SF_SB3_SAC_4\n"
      ]
     }
    ],
@@ -928,6 +1499,19 @@
     "print(f\"Finished run {run_name}\")"
    ]
   },
+  {
+   "cell_type": "code",
+   "execution_count": 12,
+   "id": "6801db30-7721-45e3-af29-a2bc557d4fe8",
+   "metadata": {
+    "tags": []
+   },
+   "outputs": [],
+   "source": [
+    "env.close()\n",
+    "eval_env.close()"
+   ]
+  },
   {
    "cell_type": "code",
    "execution_count": null,
@@ -941,22 +1525,23 @@
     "\n",
     "# LOAD TRAINED MODEL\n",
     "\n",
-    "try:\n",
-    "    model.learn(total_timesteps=10_000_000, log_interval=8, tb_log_name=\"PlainDarmEnv\",\n",
-    "                    callback=WandbCallback(model_save_path=f\"checkpoints/wandb/{run.id}\",\n",
-    "                                           model_save_freq=10, verbose=2)\n",
-    "               )\n",
-    "    # Add calbacks\n",
-    "except Exception as e:\n",
-    "    print(\"Exception caught:\")\n",
-    "    print(e)\n",
-    "finally:\n",
-    "    timestamp = f\"{datetime.now().date()}__{datetime.now().time()}\"\n",
-    "    print(f\"Saving checkpoint {timestamp}\")\n",
-    "    model_name = f\"./checkpoints/darm_sf_hand_{timestamp}\"\n",
-    "    env_norm_name = f\"./checkpoints/darm_sf_hand_env_norm_{timestamp}\"\n",
-    "    model.save(model_name)\n",
-    "    # env.save(env_norm_name) # FIXME: Remember to save norm params if using VecNorm env\n",
+    "# ########### PATHS NEED TO BE UPDATED\n",
+    "# try:\n",
+    "#     model.learn(total_timesteps=10_000_000, log_interval=8, tb_log_name=\"PlainDarmEnv\",\n",
+    "#                     callback=WandbCallback(model_save_path=f\"checkpoints/wandb/{run.id}\",\n",
+    "#                                            model_save_freq=10, verbose=2)\n",
+    "#                )\n",
+    "#     # Add calbacks\n",
+    "# except Exception as e:\n",
+    "#     print(\"Exception caught:\")\n",
+    "#     print(e)\n",
+    "# finally:\n",
+    "#     timestamp = f\"{datetime.now().date()}__{datetime.now().time()}\"\n",
+    "#     print(f\"Saving checkpoint {timestamp}\")\n",
+    "#     model_name = f\"./checkpoints/darm_sf_hand_{timestamp}\"\n",
+    "#     env_norm_name = f\"./checkpoints/darm_sf_hand_env_norm_{timestamp}\"\n",
+    "#     model.save(model_name)\n",
+    "#     # env.save(env_norm_name) # FIXME: Remember to save norm params if using VecNorm env\n",
     "    "
    ]
   },
@@ -969,7 +1554,8 @@
    },
    "outputs": [],
    "source": [
-    "env.close()"
+    "env.close()\n",
+    "eval_env.close()"
    ]
   },
   {
@@ -982,7 +1568,7 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 6,
+   "execution_count": 7,
    "id": "c54a6abe-1b86-4682-9d7d-456dc1364ba8",
    "metadata": {},
    "outputs": [
@@ -1000,7 +1586,7 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 7,
+   "execution_count": 13,
    "id": "0e4596f3-355b-4200-9eb5-128405d6b914",
    "metadata": {
     "tags": []
@@ -1013,32 +1599,22 @@
       "Loaded XML file successfully\n"
      ]
     },
-    {
-     "name": "stderr",
-     "output_type": "stream",
-     "text": [
-      "/home/daniel/miniconda3/lib/python3.8/site-packages/gym/spaces/box.py:73: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
-      "  logger.warn(\n"
-     ]
-    },
     {
      "data": {
       "text/plain": [
-       "<stable_baselines3.sac.sac.SAC at 0x7f9da5ae2f10>"
+       "<stable_baselines3.sac.sac.SAC at 0x7fede49427f0>"
       ]
      },
-     "execution_count": 7,
+     "execution_count": 13,
      "metadata": {},
      "output_type": "execute_result"
     }
    ],
    "source": [
-    "model_name = \"./results/darm_sf_hand/SF_SB3_SAC_3/best_model\"\n",
+    "model_name = f\"{config['run_local_dir']}/models/best/best_model\"\n",
     "# env_norm_name = \"./checkpoints/darm_sf_hand_env_norm_2022-12-28__10:10:05.637581\"\n",
     "\n",
     "eval_env = make_vec_env(create_env, n_envs=1, seed=config[\"seed\"])\n",
-    "# eval_env = DummyVecEnv([lambda: gym.make(\"darm/DarmSFHand-v0\", render_mode=\"human\", hand_name=\"hand1\")])\n",
-    "# eval_env = gym.make(\"darm/DarmSFHand-v0\", render_mode=\"human\", hand_name=\"hand1\")\n",
     "\n",
     "eval_model = SAC.load(model_name, env=eval_env)\n",
     "eval_model"
@@ -1046,7 +1622,7 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 10,
+   "execution_count": 14,
    "id": "759192eb-a5d0-43e5-825a-8f550c09ad5e",
    "metadata": {},
    "outputs": [
@@ -1054,7 +1630,7 @@
      "name": "stdout",
      "output_type": "stream",
      "text": [
-      "1189.5793403000002 741.351242315577\n"
+      "-6.5527834 119.55361281297256\n"
      ]
     }
    ],
@@ -1071,27 +1647,9 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 141,
-   "id": "c1818f92-c4f6-4af5-a1d3-1752af7e1806",
-   "metadata": {},
-   "outputs": [],
-   "source": [
-    "# env = DummyVecEnv([lambda: gym.make(\"darm/DarmSFHand-v0\", render_mode=\"human\", hand_name=\"hand1\")])\n",
-    "\n",
-    "# env = VecNormalize.load(env_norm_name, env)\n",
-    "# env.training = False\n",
-    "# print(\"Zero Norm: \", env.unnormalize_reward(-0.47959065))"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": 18,
-   "id": "cbec922f-40a9-4f6f-b7f8-99ebd5113835",
+   "execution_count": 16,
+   "id": "89fa3c73-6a78-4b4f-93d3-f4cd855fdb2f",
    "metadata": {
-    "collapsed": true,
-    "jupyter": {
-     "outputs_hidden": true
-    },
     "tags": []
    },
    "outputs": [
@@ -1099,221 +1657,62 @@
      "name": "stdout",
      "output_type": "stream",
      "text": [
-      "Loaded XML file successfully\n",
-      "Episode Return: 770.678056485951 Episode Length: 200\n",
-      "{'TimeLimit.truncated': True,\n",
-      " 'action': array([0.1272218 , 0.38648322, 0.41795075, 0.96957695, 0.17801568],\n",
-      "      dtype=float32),\n",
-      " 'episode': {'l': 200, 'r': 770.678058, 't': 4.201306},\n",
-      " 'model_action': array([[-0.9491113, -0.8454067, -0.8328197, -0.6121692, -0.9287937]],\n",
-      "      dtype=float32),\n",
-      " 'reward': {'act_reg': -0.22908571362495422,\n",
-      "            'bonus': array([1.]),\n",
-      "            'dense': array([3.96524759]),\n",
-      "            'done': array([False]),\n",
-      "            'penalty': array([-0.]),\n",
-      "            'reach': array([-0.00592192]),\n",
-      "            'solved': array([False]),\n",
-      "            'sparse': array([-0.00592192])},\n",
-      " 'sim_time': 16.154000000001925,\n",
-      " 'terminal_observation': array([-1.43847659, -7.91392647,  0.31345961, -1.34039728, -7.80854287,\n",
-      "       -0.26096704,  0.01737392,  0.05786671, -0.02924779])}\n",
-      "Episode Return: 635.6054340600967 Episode Length: 200\n",
-      "{'TimeLimit.truncated': True,\n",
-      " 'action': array([0.01671374, 0.03772706, 4.880125  , 2.3247674 , 0.07127702],\n",
-      "      dtype=float32),\n",
-      " 'episode': {'l': 200, 'r': 635.605434, 't': 8.174224},\n",
-      " 'model_action': array([[-0.9933145, -0.9849092,  0.95205  , -0.0700931, -0.9714892]],\n",
-      "      dtype=float32),\n",
-      " 'reward': {'act_reg': -1.081239104270935,\n",
-      "            'bonus': array([0.]),\n",
-      "            'dense': array([-0.13392945]),\n",
-      "            'done': array([False]),\n",
-      "            'penalty': array([-0.]),\n",
-      "            'reach': array([-0.01290277]),\n",
-      "            'solved': array([False]),\n",
-      "            'sparse': array([-0.01290277])},\n",
-      " 'sim_time': 16.385999999991878,\n",
-      " 'terminal_observation': array([-0.97768326, -7.2132605 , -1.03575665, -1.136482  , -6.26464382,\n",
-      "       -0.17568471, -0.01834931,  0.08324079,  0.1292913 ])}\n",
-      "Episode Return: 698.4176086336374 Episode Length: 200\n",
-      "{'TimeLimit.truncated': True,\n",
-      " 'action': array([1.4136398 , 1.9245778 , 0.25722325, 2.4980981 , 0.18945813],\n",
-      "      dtype=float32),\n",
-      " 'episode': {'l': 200, 'r': 698.417607, 't': 12.202491},\n",
-      " 'model_action': array([[-4.3454409e-01, -2.3016888e-01, -8.9711070e-01, -7.6073408e-04,\n",
-      "        -9.2421675e-01]], dtype=float32),\n",
-      " 'reward': {'act_reg': -0.6941161155700684,\n",
-      "            'bonus': array([1.]),\n",
-      "            'dense': array([3.91971437]),\n",
-      "            'done': array([False]),\n",
-      "            'penalty': array([-0.]),\n",
-      "            'reach': array([-0.00543701]),\n",
-      "            'solved': array([False]),\n",
-      "            'sparse': array([-0.00543701])},\n",
-      " 'sim_time': 16.000000000019554,\n",
-      " 'terminal_observation': array([ 0.74869396, -3.39518286,  8.58578734,  0.5289286 , -3.84832334,\n",
-      "        8.38090363,  0.01507195,  0.10874858,  0.04706846])}\n",
-      "Episode Return: 762.3988908529282 Episode Length: 200\n",
-      "{'TimeLimit.truncated': True,\n",
-      " 'action': array([4.8835716, 0.6218907, 4.532805 , 1.0692017, 0.3363648],\n",
-      "      dtype=float32),\n",
-      " 'episode': {'l': 200, 'r': 762.398892, 't': 16.093096},\n",
-      " 'model_action': array([[ 0.9534286 , -0.7512437 ,  0.81312203, -0.5723193 , -0.8654541 ]],\n",
-      "      dtype=float32),\n",
-      " 'reward': {'act_reg': -1.3570361137390137,\n",
-      "            'bonus': array([1.]),\n",
-      "            'dense': array([3.85199221]),\n",
-      "            'done': array([False]),\n",
-      "            'penalty': array([-0.]),\n",
-      "            'reach': array([-0.00615209]),\n",
-      "            'solved': array([False]),\n",
-      "            'sparse': array([-0.00615209])},\n",
-      " 'sim_time': 16.014000000017596,\n",
-      " 'terminal_observation': array([-2.17373708e+00, -7.50087186e+00, -3.53731302e+00, -2.20807879e+00,\n",
-      "       -7.81725214e+00, -3.01080938e+00,  2.33116142e-03, -7.95696272e-03,\n",
-      "        1.00434933e-02])}\n",
-      "Episode Return: 274.17127342522144 Episode Length: 200\n",
-      "{'TimeLimit.truncated': True,\n",
-      " 'action': array([0.41142836, 0.07208899, 3.8438308 , 2.905532  , 0.53034574],\n",
-      "      dtype=float32),\n",
-      " 'episode': {'l': 200, 'r': 274.171275, 't': 20.10166},\n",
-      " 'model_action': array([[-0.83542866, -0.9711644 ,  0.5375323 ,  0.16221273, -0.7878617 ]],\n",
-      "      dtype=float32),\n",
-      " 'reward': {'act_reg': -0.9730958342552185,\n",
-      "            'bonus': array([0.]),\n",
-      "            'dense': array([-0.13675444]),\n",
-      "            'done': array([False]),\n",
-      "            'penalty': array([-0.]),\n",
-      "            'reach': array([-0.01972243]),\n",
-      "            'solved': array([False]),\n",
-      "            'sparse': array([-0.01972243])},\n",
-      " 'sim_time': 16.39999999996178,\n",
-      " 'terminal_observation': array([-1.41096584, -7.04316762,  0.76866325, -0.97992386, -7.20407405,\n",
-      "        2.68648856, -0.05202748,  0.02185201, -0.00837738])}\n",
-      "Episode Return: 790.0150463581085 Episode Length: 200\n",
-      "{'TimeLimit.truncated': True,\n",
-      " 'action': array([0.626176 , 0.230418 , 3.0357594, 2.8480678, 1.5895853],\n",
-      "      dtype=float32),\n",
-      " 'episode': {'l': 200, 'r': 790.015047, 't': 24.183044},\n",
-      " 'model_action': array([[-0.7495296 , -0.9078328 ,  0.21430373,  0.13922715, -0.3641659 ]],\n",
-      "      dtype=float32),\n",
-      " 'reward': {'act_reg': -0.9010948538780212,\n",
-      "            'bonus': array([1.]),\n",
-      "            'dense': array([3.8982463]),\n",
-      "            'done': array([False]),\n",
-      "            'penalty': array([-0.]),\n",
-      "            'reach': array([-0.00582211]),\n",
-      "            'solved': array([False]),\n",
-      "            'sparse': array([-0.00582211])},\n",
-      " 'sim_time': 16.39999999996178,\n",
-      " 'terminal_observation': array([-1.69379596e+00, -6.41159166e+00,  2.34630250e+00, -1.40258968e+00,\n",
-      "       -6.58174560e+00,  1.87173357e+00,  1.13313858e-02,  7.53286189e-04,\n",
-      "       -4.42717139e-02])}\n",
-      "Episode Return: -17.83931428194046 Episode Length: 200\n",
-      "{'TimeLimit.truncated': True,\n",
-      " 'action': array([0.01443997, 4.0789003 , 4.928274  , 1.3416407 , 0.08007079],\n",
-      "      dtype=float32),\n",
-      " 'episode': {'l': 200, 'r': -17.839314, 't': 28.122158},\n",
-      " 'model_action': array([[-0.994224  ,  0.6315601 ,  0.97130966, -0.46334374, -0.9679717 ]],\n",
-      "      dtype=float32),\n",
-      " 'reward': {'act_reg': -1.3073933124542236,\n",
-      "            'bonus': array([0.]),\n",
-      "            'dense': array([-0.15065615]),\n",
-      "            'done': array([False]),\n",
-      "            'penalty': array([-0.]),\n",
-      "            'reach': array([-0.00995841]),\n",
-      "            'solved': array([False]),\n",
-      "            'sparse': array([-0.00995841])},\n",
-      " 'sim_time': 16.39999999996178,\n",
-      " 'terminal_observation': array([ 2.31332325e+00,  3.58036822e+00,  8.62615753e+00,  1.32623710e+00,\n",
-      "        3.71192344e+00,  8.63344786e+00, -2.32576980e-01,  1.39302252e-01,\n",
-      "        8.62924880e-03])}\n",
-      "Episode Return: 774.3884239196777 Episode Length: 200\n",
-      "{'TimeLimit.truncated': True,\n",
-      " 'action': array([0.78777623, 0.1897496 , 3.7601388 , 1.480814  , 4.1308985 ],\n",
-      "      dtype=float32),\n",
-      " 'episode': {'l': 200, 'r': 774.388425, 't': 32.212381},\n",
-      " 'model_action': array([[-0.6848895 , -0.92410016,  0.5040555 , -0.40767443,  0.6523595 ]],\n",
-      "      dtype=float32),\n",
-      " 'reward': {'act_reg': -1.1670881509780884,\n",
-      "            'bonus': array([1.]),\n",
-      "            'dense': array([3.87202517]),\n",
-      "            'done': array([False]),\n",
-      "            'penalty': array([-0.]),\n",
-      "            'reach': array([-0.00563301]),\n",
-      "            'solved': array([False]),\n",
-      "            'sparse': array([-0.00563301])},\n",
-      " 'sim_time': 16.34799999997682,\n",
-      " 'terminal_observation': array([-2.33427851e+00, -4.14350529e+00,  6.42536776e+00, -2.37802387e+00,\n",
-      "       -4.58753966e+00,  6.08152511e+00, -1.63520234e-05, -8.45316575e-06,\n",
-      "        5.89392780e-05])}\n",
-      "Episode Return: 769.3141285777092 Episode Length: 200\n",
-      "{'TimeLimit.truncated': True,\n",
-      " 'action': array([0.5708501, 1.8720514, 1.7804887, 1.2069921, 0.5779172],\n",
-      "      dtype=float32),\n",
-      " 'episode': {'l': 200, 'r': 769.31413, 't': 36.213876},\n",
-      " 'model_action': array([[-0.77166   , -0.25117946, -0.28780448, -0.51720315, -0.7688331 ]],\n",
-      "      dtype=float32),\n",
-      " 'reward': {'act_reg': -0.5930060744285583,\n",
-      "            'bonus': array([1.]),\n",
-      "            'dense': array([3.92649667]),\n",
-      "            'done': array([False]),\n",
-      "            'penalty': array([-0.]),\n",
-      "            'reach': array([-0.00710136]),\n",
-      "            'solved': array([False]),\n",
-      "            'sparse': array([-0.00710136])},\n",
-      " 'sim_time': 16.000000000076398,\n",
-      " 'terminal_observation': array([-1.39438631e+00, -8.90305365e+00, -7.95455178e-01, -1.45342848e+00,\n",
-      "       -9.00528122e+00, -1.49571028e+00,  1.13871858e-05, -2.33744274e-08,\n",
-      "       -4.36979276e-06])}\n",
-      "Episode Return: 703.4400441199541 Episode Length: 200\n",
-      "{'TimeLimit.truncated': True,\n",
-      " 'action': array([4.6980705 , 0.33982232, 4.484064  , 3.5164433 , 1.7755698 ],\n",
-      "      dtype=float32),\n",
-      " 'episode': {'l': 200, 'r': 703.440043, 't': 40.214405},\n",
-      " 'model_action': array([[ 0.8792281 , -0.8640711 ,  0.7936256 ,  0.40657735, -0.2897721 ]],\n",
-      "      dtype=float32),\n",
-      " 'reward': {'act_reg': -1.5206867456436157,\n",
-      "            'bonus': array([1.]),\n",
-      "            'dense': array([3.83444965]),\n",
-      "            'done': array([False]),\n",
-      "            'penalty': array([-0.]),\n",
-      "            'reach': array([-0.00674084]),\n",
-      "            'solved': array([False]),\n",
-      "            'sparse': array([-0.00674084])},\n",
-      " 'sim_time': 16.000000000076398,\n",
-      " 'terminal_observation': array([ 3.03725062,  1.24027421,  6.26230394,  3.00486633,  0.6216548 ,\n",
-      "        6.52810497,  0.14403881, -0.09292099,  0.35840393])}\n"
+      "Loaded XML file successfully\n"
+     ]
+    },
+    {
+     "name": "stderr",
+     "output_type": "stream",
+     "text": [
+      "100%|██████████| 10/10 [00:20<00:00,  2.02s/it]"
+     ]
+    },
+    {
+     "name": "stdout",
+     "output_type": "stream",
+     "text": [
+      "0/10 Solved\n"
+     ]
+    },
+    {
+     "name": "stderr",
+     "output_type": "stream",
+     "text": [
+      "\n"
      ]
     }
    ],
    "source": [
     "import pprint\n",
+    "import numpy as np\n",
+    "from tqdm import tqdm\n",
     "\n",
-    "create_env_render = lambda: TransformObservation(gym.make(config[\"env_id\"], render_mode=\"human\"), lambda obs: obs*100)\n",
+    "create_env_render = lambda: TransformObservation(gym.make(config[\"env_id\"], render_mode=\"human\", single_finger_env=config[\"single_finger_env\"]), lambda obs: obs*100)\n",
     "env = make_vec_env(create_env_render, n_envs=1, seed=config[\"seed\"])\n",
-    "# env = DummyVecEnv([lambda: gym.make(\"darm/DarmSFHand-v0\", render_mode=\"human\", hand_name=\"hand1\")])\n",
-    "# env = gym.make(\"darm/DarmSFHand-v0\", render_mode=\"human\", hand_name=\"hand1\")\n",
     "\n",
     "obs = env.reset()\n",
     "episode_return = 0\n",
     "episode_length = 0\n",
     "N_EPISODES = 10\n",
+    "solved = 0\n",
+    "actions = []\n",
     "\n",
-    "for i in range(N_EPISODES):\n",
+    "for i in tqdm(range(N_EPISODES)):\n",
     "    done = False\n",
     "    while not done:\n",
     "        # env.render()\n",
     "        action, _states = eval_model.predict(obs, deterministic=True)\n",
     "        obs, reward, done, info = env.step(action)\n",
+    "        actions.append(info[0][\"action\"])\n",
     "        episode_return += reward[0]\n",
     "        episode_length += 1\n",
     "        done = done[0]\n",
     "        \n",
-    "    print(f\"Episode Return: {episode_return} Episode Length: {episode_length}\")\n",
+    "  #  print(f\"Episode Return: {episode_return} Episode Length: {episode_length}\")\n",
     "    info[0][\"model_action\"] = action\n",
-    "    pprint.pprint(info[0])\n",
+    "    # pprint.pprint(info[0])\n",
+    "  # print(f\"Solved: {info[0]['reward']['solved']}\")\n",
+    "    solved += info[0]['reward']['solved'][0]\n",
     "    # info[\"model_action\"] = action\n",
     "    # pprint.pprint(info)\n",
     "    \n",
@@ -1321,7 +1720,317 @@
     "    episode_return = 0\n",
     "    episode_length = 0\n",
     "\n",
-    "env.close()"
+    "env.close()\n",
+    "print(f\"{solved}/{N_EPISODES} Solved\")"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 101,
+   "id": "b2f714a1-e583-4c5a-a7bd-f9936df07ba8",
+   "metadata": {
+    "tags": []
+   },
+   "outputs": [
+    {
+     "data": {
+      "text/plain": [
+       "(5, 195072)"
+      ]
+     },
+     "execution_count": 101,
+     "metadata": {},
+     "output_type": "execute_result"
+    }
+   ],
+   "source": [
+    "actions = np.asarray(actions)\n",
+    "\n",
+    "tendon_tensions = []\n",
+    "tendon_names = [\"Dorsal Interossei\",\n",
+    "                \"Palmar Interossei\",\n",
+    "                \"Extensor Digitorum Communis\",\n",
+    "                \"Flexor Digitorum Profundus\",\n",
+    "                \"Flexor Digitorum Superficialis\"]\n",
+    "\n",
+    "\n",
+    "for i in range(5):\n",
+    "    tendon_tensions.append(actions[:, i])\n",
+    "\n",
+    "tendon_tensions = np.asarray(tendon_tensions)\n",
+    "tendon_tensions.shape"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 102,
+   "id": "4af34a7e-25f2-4d1a-bcaa-19f2f5fa0d59",
+   "metadata": {
+    "tags": []
+   },
+   "outputs": [
+    {
+     "data": {
+      "text/html": [
+       "<div>\n",
+       "<style scoped>\n",
+       "    .dataframe tbody tr th:only-of-type {\n",
+       "        vertical-align: middle;\n",
+       "    }\n",
+       "\n",
+       "    .dataframe tbody tr th {\n",
+       "        vertical-align: top;\n",
+       "    }\n",
+       "\n",
+       "    .dataframe thead th {\n",
+       "        text-align: right;\n",
+       "    }\n",
+       "</style>\n",
+       "<table border=\"1\" class=\"dataframe\">\n",
+       "  <thead>\n",
+       "    <tr style=\"text-align: right;\">\n",
+       "      <th></th>\n",
+       "      <th>Dorsal Interossei</th>\n",
+       "      <th>Palmar Interossei</th>\n",
+       "      <th>Extensor Digitorum Communis</th>\n",
+       "      <th>Flexor Digitorum Profundus</th>\n",
+       "      <th>Flexor Digitorum Superficialis</th>\n",
+       "    </tr>\n",
+       "  </thead>\n",
+       "  <tbody>\n",
+       "    <tr>\n",
+       "      <th>count</th>\n",
+       "      <td>195072.000000</td>\n",
+       "      <td>195072.000000</td>\n",
+       "      <td>195072.000000</td>\n",
+       "      <td>195072.000000</td>\n",
+       "      <td>195072.000000</td>\n",
+       "    </tr>\n",
+       "    <tr>\n",
+       "      <th>mean</th>\n",
+       "      <td>0.950422</td>\n",
+       "      <td>0.767148</td>\n",
+       "      <td>3.509327</td>\n",
+       "      <td>1.329884</td>\n",
+       "      <td>0.429009</td>\n",
+       "    </tr>\n",
+       "    <tr>\n",
+       "      <th>std</th>\n",
+       "      <td>1.014455</td>\n",
+       "      <td>0.936925</td>\n",
+       "      <td>1.026986</td>\n",
+       "      <td>0.747989</td>\n",
+       "      <td>0.561555</td>\n",
+       "    </tr>\n",
+       "    <tr>\n",
+       "      <th>min</th>\n",
+       "      <td>0.001007</td>\n",
+       "      <td>0.000811</td>\n",
+       "      <td>0.000827</td>\n",
+       "      <td>0.000035</td>\n",
+       "      <td>0.000511</td>\n",
+       "    </tr>\n",
+       "    <tr>\n",
+       "      <th>25%</th>\n",
+       "      <td>0.244022</td>\n",
+       "      <td>0.115738</td>\n",
+       "      <td>2.881964</td>\n",
+       "      <td>0.832723</td>\n",
+       "      <td>0.155681</td>\n",
+       "    </tr>\n",
+       "    <tr>\n",
+       "      <th>50%</th>\n",
+       "      <td>0.590215</td>\n",
+       "      <td>0.373623</td>\n",
+       "      <td>3.788050</td>\n",
+       "      <td>1.291511</td>\n",
+       "      <td>0.288769</td>\n",
+       "    </tr>\n",
+       "    <tr>\n",
+       "      <th>75%</th>\n",
+       "      <td>1.207549</td>\n",
+       "      <td>1.124003</td>\n",
+       "      <td>4.289469</td>\n",
+       "      <td>1.689200</td>\n",
+       "      <td>0.432691</td>\n",
+       "    </tr>\n",
+       "    <tr>\n",
+       "      <th>max</th>\n",
+       "      <td>4.999928</td>\n",
+       "      <td>5.000000</td>\n",
+       "      <td>4.999597</td>\n",
+       "      <td>4.999947</td>\n",
+       "      <td>4.991437</td>\n",
+       "    </tr>\n",
+       "  </tbody>\n",
+       "</table>\n",
+       "</div>"
+      ],
+      "text/plain": [
+       "       Dorsal Interossei  Palmar Interossei  Extensor Digitorum Communis  \\\n",
+       "count      195072.000000      195072.000000                195072.000000   \n",
+       "mean            0.950422           0.767148                     3.509327   \n",
+       "std             1.014455           0.936925                     1.026986   \n",
+       "min             0.001007           0.000811                     0.000827   \n",
+       "25%             0.244022           0.115738                     2.881964   \n",
+       "50%             0.590215           0.373623                     3.788050   \n",
+       "75%             1.207549           1.124003                     4.289469   \n",
+       "max             4.999928           5.000000                     4.999597   \n",
+       "\n",
+       "       Flexor Digitorum Profundus  Flexor Digitorum Superficialis  \n",
+       "count               195072.000000                   195072.000000  \n",
+       "mean                     1.329884                        0.429009  \n",
+       "std                      0.747989                        0.561555  \n",
+       "min                      0.000035                        0.000511  \n",
+       "25%                      0.832723                        0.155681  \n",
+       "50%                      1.291511                        0.288769  \n",
+       "75%                      1.689200                        0.432691  \n",
+       "max                      4.999947                        4.991437  "
+      ]
+     },
+     "execution_count": 102,
+     "metadata": {},
+     "output_type": "execute_result"
+    }
+   ],
+   "source": [
+    "import pandas as pd\n",
+    "\n",
+    "tendon_tensions_dict = {}\n",
+    "\n",
+    "for tendon_name, tensions in zip(tendon_names, tendon_tensions):\n",
+    "    tendon_tensions_dict[tendon_name] = tensions\n",
+    "\n",
+    "tensions_df = pd.DataFrame(tendon_tensions_dict)\n",
+    "tensions_df.describe()"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 126,
+   "id": "b82ad6cc-b70e-45c0-8f5a-503381b6372a",
+   "metadata": {
+    "tags": []
+   },
+   "outputs": [
+    {
+     "data": {
+      "text/plain": [
+       "count    195072.000000\n",
+       "mean          0.950422\n",
+       "std           1.014455\n",
+       "min           0.001007\n",
+       "25%           0.244022\n",
+       "50%           0.590215\n",
+       "75%           1.207549\n",
+       "max           4.999928\n",
+       "Name: Dorsal Interossei, dtype: float64"
+      ]
+     },
+     "execution_count": 126,
+     "metadata": {},
+     "output_type": "execute_result"
+    },
+    {
+     "data": {
+      "image/png": "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\n",
+      "text/plain": [
+       "<Figure size 432x288 with 1 Axes>"
+      ]
+     },
+     "metadata": {
+      "needs_background": "light"
+     },
+     "output_type": "display_data"
+    }
+   ],
+   "source": [
+    "tendon_idx = 0\n",
+    "# tensions_df[tensions_df.columns[tendon_idx]].plot(kind='density', xlim=(-0.5,5.5))\n",
+    "tensions_df[tensions_df.columns[tendon_idx]].plot(kind='hist', bins=50)\n",
+    "plt.title(f\"{tensions_df.columns[tendon_idx]} Tension Histogram plot\")\n",
+    "_ = plt.xlabel(\"Tendon Tension [N]\")\n",
+    "# _ = plt.ylabel(\"Count\")\n",
+    "\n",
+    "tensions_df[tensions_df.columns[tendon_idx]].describe()"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "id": "4d02519f-98c7-48d8-a9af-e266565e5758",
+   "metadata": {},
+   "outputs": [],
+   "source": []
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "id": "fac02162-fb76-4e93-a1f9-373253dc4ccd",
+   "metadata": {},
+   "outputs": [],
+   "source": []
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "id": "dd03e71f-e842-45a8-9638-5477d0efd2c7",
+   "metadata": {},
+   "outputs": [],
+   "source": []
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "id": "e4d81643-ff6f-441b-a92f-d6b48df9de74",
+   "metadata": {},
+   "outputs": [],
+   "source": []
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "id": "ac0254e0-8fa4-4b67-a708-0f9e9e56e5a3",
+   "metadata": {},
+   "outputs": [],
+   "source": []
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 48,
+   "id": "b181d28b-35c8-4620-acde-49eaa01ea458",
+   "metadata": {
+    "tags": []
+   },
+   "outputs": [
+    {
+     "data": {
+      "text/plain": [
+       "<AxesSubplot: >"
+      ]
+     },
+     "execution_count": 48,
+     "metadata": {},
+     "output_type": "execute_result"
+    },
+    {
+     "data": {
+      "image/png": "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\n",
+      "text/plain": [
+       "<Figure size 432x288 with 1 Axes>"
+      ]
+     },
+     "metadata": {
+      "needs_background": "light"
+     },
+     "output_type": "display_data"
+    }
+   ],
+   "source": [
+    "# tensions_df[\"Flexor Digitorum Superficialis\"].hist()\n",
+    "# tensions_df[\"Extensor Digitorum Communis\"].hist()\n",
+    "# tensions_df[\"Dorsal Interossei\"].hist()\n",
+    "# tensions_df[\"Palmar Interossei\"].hist()"
    ]
   },
   {
diff --git a/dist/darm_gym_env-0.0.1-py3.8.egg b/dist/darm_gym_env-0.0.1-py3.8.egg
index 0d6f447..1815bd3 100644
Binary files a/dist/darm_gym_env-0.0.1-py3.8.egg and b/dist/darm_gym_env-0.0.1-py3.8.egg differ
diff --git a/gym_temp_test.py b/gym_temp_test.py
deleted file mode 100644
index 1d7b272..0000000
--- a/gym_temp_test.py
+++ /dev/null
@@ -1,20 +0,0 @@
-import gym
-from darm_gym_env import DARMEnv
-import time
-import random
-
-env = gym.make("Ant-v3")
-
-done = False
-obs = env.reset()
-
-start = time.time()
-while not done:
-    ac = env.action_space.sample()
-    obs, rew, done, info = env.step(ac)
-    env.render()
-
-    if done:
-        env.reset()
-        done = False
-env.close()
\ No newline at end of file
diff --git a/mujoco_env/actuator.xml b/mujoco_env/actuator.xml
index 3b94109..8356691 100644
--- a/mujoco_env/actuator.xml
+++ b/mujoco_env/actuator.xml
@@ -5,11 +5,36 @@
 <!-- =================================================================================== -->
 <actuator>
   <!-- A total of 34 muscles -->
+  <general biasprm="0" biastype="none" ctrllimited="true" ctrlrange="0 5" gainprm="-1" gaintype="fixed" name="hand1_adductor_pollicis_oblique_actuator" tendon="hand1_adductor_pollicis_oblique"/>
+  <general biasprm="0" biastype="none" ctrllimited="true" ctrlrange="0 5" gainprm="-1" gaintype="fixed" name="hand1_adductor_pollicis_transverse_actuator" tendon="hand1_adductor_pollicis_transverse"/>
+  <general biasprm="0" biastype="none" ctrllimited="true" ctrlrange="0 5" gainprm="-1" gaintype="fixed" name="hand1_abductor_pollicis_brevis_actuator" tendon="hand1_abductor_pollicis_brevis"/>
+  <general biasprm="0" biastype="none" ctrllimited="true" ctrlrange="0 5" gainprm="-1" gaintype="fixed" name="hand1_abductor_pollicis_longus_actuator" tendon="hand1_abductor_pollicis_longus"/>
+  <general biasprm="0" biastype="none" ctrllimited="true" ctrlrange="0 5" gainprm="-1" gaintype="fixed" name="hand1_flexor_pollicis_brevis_actuator" tendon="hand1_flexor_pollicis_brevis"/>
+  <general biasprm="0" biastype="none" ctrllimited="true" ctrlrange="0 5" gainprm="-1" gaintype="fixed" name="hand1_flexor_pollicis_longus_actuator" tendon="hand1_flexor_pollicis_longus"/>
+  <general biasprm="0" biastype="none" ctrllimited="true" ctrlrange="0 5" gainprm="-1" gaintype="fixed" name="hand1_extensor_pollicis_brevis_actuator" tendon="hand1_extensor_pollicis_brevis"/>
+  <general biasprm="0" biastype="none" ctrllimited="true" ctrlrange="0 5" gainprm="-1" gaintype="fixed" name="hand1_extensor_pollicis_longus_actuator" tendon="hand1_extensor_pollicis_longus"/>
+  <general biasprm="0" biastype="none" ctrllimited="true" ctrlrange="0 5" gainprm="-1" gaintype="fixed" name="hand1_opponens_pollicis_actuator" tendon="hand1_opponens_pollicis"/>
   <general biasprm="0" biastype="none" ctrllimited="true" ctrlrange="0 5" gainprm="-1" gaintype="fixed" name="hand1_dorsal_interossei_ii_actuator" tendon="hand1_dorsal_interossei_ii"/>
   <general biasprm="0" biastype="none" ctrllimited="true" ctrlrange="0 5" gainprm="-1" gaintype="fixed" name="hand1_palmar_interossei_ii_actuator" tendon="hand1_palmar_interossei_ii"/>
-  <general biasprm="0" biastype="none" ctrllimited="true" ctrlrange="0 5" gainprm="-1" gaintype="fixed" name="hand1_extensor_communis_ii_actuator" tendon="hand1_extensor_communis_ii"/>
   <general biasprm="0" biastype="none" ctrllimited="true" ctrlrange="0 5" gainprm="-1" gaintype="fixed" name="hand1_flexor_profundus_ii_actuator" tendon="hand1_flexor_profundus_ii"/>
   <general biasprm="0" biastype="none" ctrllimited="true" ctrlrange="0 5" gainprm="-1" gaintype="fixed" name="hand1_flexor_superficialis_ii_actuator" tendon="hand1_flexor_superficialis_ii"/>
+  <general biasprm="0" biastype="none" ctrllimited="true" ctrlrange="0 10" gainprm="-1" gaintype="fixed" name="hand1_extensor_communis_ii_actuator" tendon="hand1_extensor_communis_ii"/>
+  <general biasprm="0" biastype="none" ctrllimited="true" ctrlrange="0 5" gainprm="-1" gaintype="fixed" name="hand1_dorsal_interossei_iii_actuator" tendon="hand1_dorsal_interossei_iii"/>
+  <general biasprm="0" biastype="none" ctrllimited="true" ctrlrange="0 5" gainprm="-1" gaintype="fixed" name="hand1_palmar_interossei_iii_actuator" tendon="hand1_palmar_interossei_iii"/>
+  <general biasprm="0" biastype="none" ctrllimited="true" ctrlrange="0 5" gainprm="-1" gaintype="fixed" name="hand1_flexor_profundus_iii_actuator" tendon="hand1_flexor_profundus_iii"/>
+  <general biasprm="0" biastype="none" ctrllimited="true" ctrlrange="0 5" gainprm="-1" gaintype="fixed" name="hand1_flexor_superficialis_iii_actuator" tendon="hand1_flexor_superficialis_iii"/>
+  <general biasprm="0" biastype="none" ctrllimited="true" ctrlrange="0 10" gainprm="-1" gaintype="fixed" name="hand1_extensor_communis_iii_actuator" tendon="hand1_extensor_communis_iii"/>
+  <general biasprm="0" biastype="none" ctrllimited="true" ctrlrange="0 5" gainprm="-1" gaintype="fixed" name="hand1_dorsal_interossei_iv_actuator" tendon="hand1_dorsal_interossei_iv"/>
+  <general biasprm="0" biastype="none" ctrllimited="true" ctrlrange="0 5" gainprm="-1" gaintype="fixed" name="hand1_palmar_interossei_iv_actuator" tendon="hand1_palmar_interossei_iv"/>
+  <general biasprm="0" biastype="none" ctrllimited="true" ctrlrange="0 5" gainprm="-1" gaintype="fixed" name="hand1_flexor_profundus_iv_actuator" tendon="hand1_flexor_profundus_iv"/>
+  <general biasprm="0" biastype="none" ctrllimited="true" ctrlrange="0 5" gainprm="-1" gaintype="fixed" name="hand1_flexor_superficialis_iv_actuator" tendon="hand1_flexor_superficialis_iv"/>
+  <general biasprm="0" biastype="none" ctrllimited="true" ctrlrange="0 10" gainprm="-1" gaintype="fixed" name="hand1_extensor_communis_iv_actuator" tendon="hand1_extensor_communis_iv"/>
+  <general biasprm="0" biastype="none" ctrllimited="true" ctrlrange="0 5" gainprm="-1" gaintype="fixed" name="hand1_dorsal_interossei_v_actuator" tendon="hand1_dorsal_interossei_v"/>
+  <general biasprm="0" biastype="none" ctrllimited="true" ctrlrange="0 5" gainprm="-1" gaintype="fixed" name="hand1_palmar_interossei_v_actuator" tendon="hand1_palmar_interossei_v"/>
+  <general biasprm="0" biastype="none" ctrllimited="true" ctrlrange="0 5" gainprm="-1" gaintype="fixed" name="hand1_flexor_profundus_v_actuator" tendon="hand1_flexor_profundus_v"/>
+  <general biasprm="0" biastype="none" ctrllimited="true" ctrlrange="0 5" gainprm="-1" gaintype="fixed" name="hand1_flexor_superficialis_v_actuator" tendon="hand1_flexor_superficialis_v"/>
+  <general biasprm="0" biastype="none" ctrllimited="true" ctrlrange="0 10" gainprm="-1" gaintype="fixed" name="hand1_extensor_communis_v_actuator" tendon="hand1_extensor_communis_v"/>
+  <general biasprm="0" biastype="none" ctrllimited="true" ctrlrange="0 5" gainprm="-1" gaintype="fixed" name="hand1_opponens_digiti_minimi_actuator" tendon="hand1_opponens_digiti_minimi"/>
   <!-- <xacro:hand_actuators hand_name="hand2" />
     <xacro:hand_actuators hand_name="hand3" /> -->
 </actuator>
diff --git a/mujoco_env/actuator.xml.xacro b/mujoco_env/actuator.xml.xacro
index f9eccef..d37ac9f 100644
--- a/mujoco_env/actuator.xml.xacro
+++ b/mujoco_env/actuator.xml.xacro
@@ -24,14 +24,21 @@
     'flexor_pollicis_brevis', 'flexor_pollicis_longus',
     'extensor_pollicis_brevis', 'extensor_pollicis_longus',
     'opponens_pollicis'
-    ]}" />
-
-    <xacro:property name="digiti_minimi_specific_tendons" value="${['opponens_digiti_minimi']}" />
+    ]}" /> 
 
     <xacro:property name="digitorum_tendons" value="${[
         'dorsal_interossei', 'palmar_interossei',
-        'extensor_communis', 'flexor_profundus',
-        'flexor_superficialis']}" />
+        'flexor_profundus', 'flexor_superficialis']}" />
+
+    <xacro:property name="digitorum_extensor" value="${['extensor_communis']}" />
+
+    <xacro:property name="digiti_minimi_specific_tendons" value="${['opponens_digiti_minimi']}" />
+
+    <xacro:property name="carpal_tendons_max_tension" value="10" /> 
+    <xacro:property name="pollicis_tendons_max_tension" value="5" /> 
+    <xacro:property name="digitorum_tendons_max_tension" value="5" /> 
+    <xacro:property name="digitorum_extensor_max_tension" value="10" /> 
+    <xacro:property name="digiti_minimi_specific_tendons_max_tension" value="5" /> 
 
 
     <xacro:macro name="hand_actuators" params="hand_name">
@@ -62,25 +69,35 @@
 
 
         <!-- Create a List of Muscles -->
-        <!-- Carpal Tendon Actuators -->
         <xacro:property name="s_finger" value="$(arg single_finger)" />
         <xacro:property name="n_wrist" value="$(arg no_wrist)" />
+        
+        <!-- Carpal Tendon Actuators -->
         <xacro:unless value="${s_finger or n_wrist}">
-            <xacro:make_actuators tendons="${list(carpal_tendons)}" ctrlrange="0 10"/>
+            <xacro:make_actuators tendons="${list(carpal_tendons)}" ctrlrange="0 ${carpal_tendons_max_tension}"/>
+        </xacro:unless>
+
+        <!-- Digit I (Pollicis) Tendon Actuators -->
+        <xacro:unless value="$(arg single_finger)">
+            <xacro:make_actuators tendons="${list(pollicis_tendons)}" ctrlrange="0 ${pollicis_tendons_max_tension}"/>
         </xacro:unless>
 
         <!-- Digit II -->
-        <xacro:make_actuators tendons="${list(digitorum_tendons)}" index="ii" ctrlrange="0 5"/>
+        <xacro:make_actuators tendons="${list(digitorum_tendons)}" index="ii" ctrlrange="0 ${digitorum_tendons_max_tension}"/>
+        <xacro:make_actuators tendons="${list(digitorum_extensor)}" index="ii" ctrlrange="0 ${digitorum_extensor_max_tension}"/>
 
-        <!-- Pollicis, Digit III-V, Digiti Minimi Specific Tendon Actuators -->
+        <!-- Digit III-V, Digiti Minimi Specific Tendon Actuators -->
         <xacro:unless value="$(arg single_finger)">
-            <xacro:make_actuators tendons="${list(pollicis_tendons)}" ctrlrange="0 5"/>
+            <xacro:make_actuators tendons="${list(digitorum_tendons)}" index="iii" ctrlrange="0 ${digitorum_tendons_max_tension}"/>
+            <xacro:make_actuators tendons="${list(digitorum_extensor)}" index="iii" ctrlrange="0 ${digitorum_extensor_max_tension}"/>
 
-            <xacro:make_actuators tendons="${list(digitorum_tendons)}" index="iii" ctrlrange="0 5"/>
-            <xacro:make_actuators tendons="${list(digitorum_tendons)}" index="iv" ctrlrange="0 5"/>
-            <xacro:make_actuators tendons="${list(digitorum_tendons)}" index="v" ctrlrange="0 5"/>
+            <xacro:make_actuators tendons="${list(digitorum_tendons)}" index="iv" ctrlrange="0 ${digitorum_tendons_max_tension}"/>
+            <xacro:make_actuators tendons="${list(digitorum_extensor)}" index="iv" ctrlrange="0 ${digitorum_extensor_max_tension}"/>
+            
+            <xacro:make_actuators tendons="${list(digitorum_tendons)}" index="v" ctrlrange="0 ${digitorum_tendons_max_tension}"/>
+            <xacro:make_actuators tendons="${list(digitorum_extensor)}" index="v" ctrlrange="0 ${digitorum_extensor_max_tension}"/>
 
-            <xacro:make_actuators tendons="${list(digiti_minimi_specific_tendons)}" ctrlrange="0 5"/>
+            <xacro:make_actuators tendons="${list(digiti_minimi_specific_tendons)}" ctrlrange="0 ${digiti_minimi_specific_tendons_max_tension}"/>
         </xacro:unless>
     </xacro:macro>
 
diff --git a/mujoco_env/contact_exclusions.xml b/mujoco_env/contact_exclusions.xml
index 8c466d3..7e64440 100644
--- a/mujoco_env/contact_exclusions.xml
+++ b/mujoco_env/contact_exclusions.xml
@@ -16,6 +16,39 @@
   <exclude body1="hand1_proximal_phalanx_ii" body2="hand1_carpals_metacarpals"/>
   <exclude body1="hand1_middle_phalanx_ii" body2="hand1_proximal_phalanx_ii"/>
   <exclude body1="hand1_distal_phalanx_ii" body2="hand1_middle_phalanx_ii"/>
+  <!-- PHALANGES I -->
+  <exclude body1="hand1_proximal_cm_i_centre_block" body2="hand1_carpals_metacarpals"/>
+  <exclude body1="hand1_distal_cm_i_centre_block" body2="hand1_proximal_cm_i_centre_block"/>
+  <exclude body1="hand1_distal_cm_i_centre_block" body2="hand1_carpals_metacarpals"/>
+  <exclude body1="hand1_metacarpal_i" body2="hand1_distal_cm_i_centre_block"/>
+  <exclude body1="hand1_metacarpal_i" body2="hand1_proximal_cm_i_centre_block"/>
+  <exclude body1="hand1_metacarpal_i" body2="hand1_carpals_metacarpals"/>
+  <exclude body1="hand1_proximal_phalanx_i" body2="hand1_metacarpal_i"/>
+  <exclude body1="hand1_distal_phalanx_i" body2="hand1_proximal_phalanx_i"/>
+  <!-- PHALANGES III -->
+  <exclude body1="hand1_mcp_centre_block_iii" body2="hand1_carpals_metacarpals"/>
+  <exclude body1="hand1_proximal_phalanx_iii" body2="hand1_mcp_centre_block_iii"/>
+  <exclude body1="hand1_proximal_phalanx_iii" body2="hand1_carpals_metacarpals"/>
+  <exclude body1="hand1_middle_phalanx_iii" body2="hand1_proximal_phalanx_iii"/>
+  <exclude body1="hand1_distal_phalanx_iii" body2="hand1_middle_phalanx_iii"/>
+  <!-- PHALANGES IV -->
+  <exclude body1="hand1_mcp_centre_block_iv" body2="hand1_carpals_metacarpals"/>
+  <exclude body1="hand1_proximal_phalanx_iv" body2="hand1_mcp_centre_block_iv"/>
+  <exclude body1="hand1_proximal_phalanx_iv" body2="hand1_carpals_metacarpals"/>
+  <exclude body1="hand1_middle_phalanx_iv" body2="hand1_proximal_phalanx_iv"/>
+  <exclude body1="hand1_distal_phalanx_iv" body2="hand1_middle_phalanx_iv"/>
+  <!-- PHALANGES V -->
+  <exclude body1="hand1_proximal_cm_v_centre_block" body2="hand1_carpals_metacarpals"/>
+  <exclude body1="hand1_distal_cm_v_centre_block" body2="hand1_proximal_cm_v_centre_block"/>
+  <exclude body1="hand1_distal_cm_v_centre_block" body2="hand1_carpals_metacarpals"/>
+  <exclude body1="hand1_metacarpal_v" body2="hand1_distal_cm_v_centre_block"/>
+  <exclude body1="hand1_metacarpal_v" body2="hand1_proximal_cm_v_centre_block"/>
+  <exclude body1="hand1_metacarpal_v" body2="hand1_carpals_metacarpals"/>
+  <exclude body1="hand1_mcp_centre_block_v" body2="hand1_metacarpal_v"/>
+  <exclude body1="hand1_proximal_phalanx_v" body2="hand1_mcp_centre_block_v"/>
+  <exclude body1="hand1_proximal_phalanx_v" body2="hand1_metacarpal_v"/>
+  <exclude body1="hand1_middle_phalanx_v" body2="hand1_proximal_phalanx_v"/>
+  <exclude body1="hand1_distal_phalanx_v" body2="hand1_middle_phalanx_v"/>
   <!-- <xacro:hand_contacts hand_name="hand2" />
     <xacro:hand_contacts hand_name="hand3" /> -->
 </contact>
diff --git a/mujoco_env/darm_hand.xml b/mujoco_env/darm_hand.xml
index 6eb1391..51b7112 100644
--- a/mujoco_env/darm_hand.xml
+++ b/mujoco_env/darm_hand.xml
@@ -5,9 +5,21 @@
 <!-- =================================================================================== -->
 <worldbody>
   <!-- Phalanges -->
+  <body mocap="true" name="hand1_fingertip_i_target" pos="0 0.25 0">
+    <geom rgba="1 0 0 1" size="0.004" type="sphere"/>
+  </body>
   <body mocap="true" name="hand1_fingertip_ii_target" pos="0 0.25 0">
     <geom rgba="0 1 0 1" size="0.004" type="sphere"/>
   </body>
+  <body mocap="true" name="hand1_fingertip_iii_target" pos="0 0.25 0">
+    <geom rgba="0 0 1 1" size="0.004" type="sphere"/>
+  </body>
+  <body mocap="true" name="hand1_fingertip_iv_target" pos="0 0.25 0">
+    <geom rgba="1 1 1 1" size="0.004" type="sphere"/>
+  </body>
+  <body mocap="true" name="hand1_fingertip_v_target" pos="0 0.25 0">
+    <geom rgba="0.7 0.3 0 1" size="0.004" type="sphere"/>
+  </body>
   <body childclass="darm_meshes" euler="0 0 0" name="hand1_forearm" pos="0 0 0.12">
     <geom mesh="forearm_mesh" type="mesh"/>
     <site name="hand1_rc_joint_wrap_dorsal_sidesite" pos="0 0.013 0.032" rgba="1 1 1 0.2" size="0.001"/>
@@ -124,6 +136,184 @@
             </body>
           </body>
         </body>
+        <body euler="0 0 0" name="hand1_mcp_centre_block_iii" pos="0.0082 0.0087 0.0933">
+          <joint axis="0 1 0" limited="true" range="-20 20" type="hinge"/>
+          <geom mesh="mcp_centre_block_iii_mesh" type="mesh"/>
+          <site name="hand1_mcp_iii_wrap_dorsal_sidesite" pos="0 0.012 0" rgba="1 1 1 0.2" size="0.001"/>
+          <site name="hand1_mcp_iii_wrap_palmar_sidesite" pos="0 -0.012 0" rgba="1 1 1 0.2" size="0.001"/>
+          <body euler="0 0 0" name="hand1_proximal_phalanx_iii" pos="0 0 0">
+            <joint axis="1 0 0" limited="true" range="-45 90" type="hinge"/>
+            <geom mesh="proximal_phalanx_iii_mesh" rgba="1 0 0 1" type="mesh"/>
+            <site name="hand1_dorsal_interossei_iii_insertion" pos="0.007 -0.007 0.012" rgba="0.1 0.1 0.1 1" size="0.002"/>
+            <site name="hand1_palmar_interossei_iii_insertion" pos="-0.007 -0.007 0.012" rgba="0.1 0.1 0.1 1" size="0.002"/>
+            <site name="hand1_pp_iii_extensor_communis_insertion" pos="0 0.009 0.012" rgba="0.1 0.1 0.1 1" size="0.002"/>
+            <site name="hand1_pp_iii_dorsal_interossei_tunnel" pos="0.007 0.0063 0.045" rgba="1 0 0 1" size="0.0015"/>
+            <site name="hand1_pp_iii_palmar_interossei_tunnel" pos="-0.007 0.0063 0.045" rgba="1 0 0 1" size="0.0015"/>
+            <site name="hand1_pp_iii_flexor_tunnel" pos="0 -0.00595 0.026" rgba="1 0 0 1" size="0.0015"/>
+            <site name="hand1_pip_iii_wrap_dorsal_sidesite" pos="0 0.008 0.052" rgba="1 1 1 0.2" size="0.001"/>
+            <site name="hand1_pip_iii_wrap_palmar_sidesite" pos="0 -0.008 0.052" rgba="1 1 1 0.2" size="0.001"/>
+            <body euler="0 0 0" name="hand1_middle_phalanx_iii" pos="0 0 0.052">
+              <joint axis="1 0 0" limited="true" range="-10 90" type="hinge"/>
+              <geom mesh="middle_phalanx_iii_mesh" rgba="0 1 0 1" type="mesh"/>
+              <geom class="wrap_geom" euler="0 90 0" name="hand1_pip_iii_wrap" rgba="0 0 1 0.2" size="0.007 0.007" type="cylinder"/>
+              <site name="hand1_flexor_superficialis_iii_insertion" pos="0 -0.005 0.007" rgba="0.1 0.1 0.1 1" size="0.002"/>
+              <site name="hand1_mp_iii_extensor_hood_insertion" pos="0 0.005 0.007" rgba="0.1 0.1 0.1 1" size="0.002"/>
+              <site name="hand1_mp_iii_flexor_tunnel" pos="0 -0.0045 0.0191428571429" rgba="0 1 0 1" size="0.0015"/>
+              <site name="hand1_dip_iii_wrap_dorsal_sidesite" pos="0 0.007 0.0335" rgba="1 1 1 0.2" size="0.001"/>
+              <site name="hand1_dip_iii_wrap_palmar_sidesite" pos="0 -0.007 0.0335" rgba="1 1 1 0.2" size="0.001"/>
+              <body euler="0 0 0" name="hand1_distal_phalanx_iii" pos="0 0 0.0335">
+                <joint axis="1 0 0" limited="true" range="-10 90" type="hinge"/>
+                <geom mesh="distal_phalanx_iii_mesh" rgba="0 0 1 1" type="mesh"/>
+                <geom class="wrap_geom" euler="0 90 0" name="hand1_dip_iii_wrap" rgba="0 0 1 0.2" size="0.006 0.006" type="cylinder"/>
+                <site name="hand1_flexor_profundus_iii_insertion" pos="0 -0.005 0.007" rgba="0.1 0.1 0.1 1" size="0.002"/>
+                <site name="hand1_dp_iii_extensor_hood_insertion" pos="0 0.005 0.007" rgba="0.1 0.1 0.1 1" size="0.002"/>
+                <site name="hand1_fingertip_iii" pos="0 0 0.0204" rgba="1 0 0 0.5" size="0.004"/>
+              </body>
+            </body>
+          </body>
+        </body>
+        <body euler="0 -5 5" name="hand1_mcp_centre_block_iv" pos="-0.0103 0.0021 0.0889">
+          <joint axis="0 1 0" limited="true" range="-20 20" type="hinge"/>
+          <geom mesh="mcp_centre_block_iv_mesh" type="mesh"/>
+          <site name="hand1_mcp_iv_wrap_dorsal_sidesite" pos="0 0.0105 0" rgba="1 1 1 0.2" size="0.001"/>
+          <site name="hand1_mcp_iv_wrap_palmar_sidesite" pos="0 -0.0105 0" rgba="1 1 1 0.2" size="0.001"/>
+          <body euler="0 0 0" name="hand1_proximal_phalanx_iv" pos="0 0 0">
+            <joint axis="1 0 0" limited="true" range="-45 90" type="hinge"/>
+            <geom mesh="proximal_phalanx_iv_mesh" rgba="1 0 0 1" type="mesh"/>
+            <site name="hand1_dorsal_interossei_iv_insertion" pos="0.007 -0.007 0.012" rgba="0.1 0.1 0.1 1" size="0.002"/>
+            <site name="hand1_palmar_interossei_iv_insertion" pos="-0.007 -0.007 0.012" rgba="0.1 0.1 0.1 1" size="0.002"/>
+            <site name="hand1_pp_iv_extensor_communis_insertion" pos="0 0.007 0.012" rgba="0.1 0.1 0.1 1" size="0.002"/>
+            <site name="hand1_pp_iv_dorsal_interossei_tunnel" pos="0.0065 0.00585 0.0422" rgba="1 0 0 1" size="0.0015"/>
+            <site name="hand1_pp_iv_palmar_interossei_tunnel" pos="-0.0065 0.00585 0.0422" rgba="1 0 0 1" size="0.0015"/>
+            <site name="hand1_pp_iv_flexor_tunnel" pos="0 -0.005525 0.02435" rgba="1 0 0 1" size="0.0015"/>
+            <site name="hand1_pip_iv_wrap_dorsal_sidesite" pos="0 0.0075 0.0487" rgba="1 1 1 0.2" size="0.001"/>
+            <site name="hand1_pip_iv_wrap_palmar_sidesite" pos="0 -0.0075 0.0487" rgba="1 1 1 0.2" size="0.001"/>
+            <body euler="0 0 0" name="hand1_middle_phalanx_iv" pos="0 0 0.0487">
+              <joint axis="1 0 0" limited="true" range="-10 90" type="hinge"/>
+              <geom mesh="middle_phalanx_iv_mesh" rgba="0 1 0 1" type="mesh"/>
+              <geom class="wrap_geom" euler="0 90 0" name="hand1_pip_iv_wrap" rgba="0 0 1 0.2" size="0.0065 0.0065" type="cylinder"/>
+              <site name="hand1_flexor_superficialis_iv_insertion" pos="0 -0.005 0.007" rgba="0.1 0.1 0.1 1" size="0.002"/>
+              <site name="hand1_mp_iv_extensor_hood_insertion" pos="0 0.005 0.007" rgba="0.1 0.1 0.1 1" size="0.002"/>
+              <site name="hand1_mp_iv_flexor_tunnel" pos="0 -0.004125 0.0188571428571" rgba="0 1 0 1" size="0.0015"/>
+              <site name="hand1_dip_iv_wrap_dorsal_sidesite" pos="0 0.0065 0.033" rgba="1 1 1 0.2" size="0.001"/>
+              <site name="hand1_dip_iv_wrap_palmar_sidesite" pos="0 -0.0065 0.033" rgba="1 1 1 0.2" size="0.001"/>
+              <body euler="0 0 0" name="hand1_distal_phalanx_iv" pos="0 0 0.033">
+                <joint axis="1 0 0" limited="true" range="-10 90" type="hinge"/>
+                <geom mesh="distal_phalanx_iv_mesh" rgba="0 0 1 1" type="mesh"/>
+                <geom class="wrap_geom" euler="0 90 0" name="hand1_dip_iv_wrap" rgba="0 0 1 0.2" size="0.0055 0.0055" type="cylinder"/>
+                <site name="hand1_flexor_profundus_iv_insertion" pos="0 -0.004 0.007" rgba="0.1 0.1 0.1 1" size="0.002"/>
+                <site name="hand1_dp_iv_extensor_hood_insertion" pos="0 0.004 0.007" rgba="0.1 0.1 0.1 1" size="0.002"/>
+                <site name="hand1_fingertip_iv" pos="0 0 0.0202" rgba="1 0 0 0.5" size="0.004"/>
+              </body>
+            </body>
+          </body>
+        </body>
+        <body euler="24 -15 -75" name="hand1_proximal_cm_i_centre_block" pos="0.0223607 -0.0073321 0.0254833">
+          <joint axis="1 0 0" limited="true" range="-50 -10" type="hinge"/>
+          <geom mesh="proximal_cm_i_centre_block_mesh" type="mesh"/>
+          <geom class="wrap_geom" euler="0 90 0" name="hand1_proximal_cm_i_wrap" rgba="0 0 1 0.2" size="0.0085 0.010" type="cylinder"/>
+          <site name="hand1_distal_cm_i_wrap_dorsal_sidesite" pos="0.0085 0 0.013" rgba="1 1 1 0.2" size="0.001"/>
+          <site name="hand1_distal_cm_i_wrap_palmar_sidesite" pos="-0.0085 0 0.013" rgba="1 1 1 0.2" size="0.001"/>
+          <body euler="0 0 0" name="hand1_distal_cm_i_centre_block" pos="0 0 0.013">
+            <joint axis="0 0 1" limited="true" range="-15 15" type="hinge"/>
+            <geom mesh="distal_cm_i_centre_block_mesh" type="mesh"/>
+            <geom class="wrap_geom" euler="90 0 0" name="hand1_distal_cm_i_wrap" rgba="0 0 1 0.2" size="0.0075 0.009" type="cylinder"/>
+            <body euler="0 0 0" name="hand1_metacarpal_i" pos="0 0 0">
+              <joint axis="0 1 0" limited="true" pos="0 0 0" range="-20 20" type="hinge"/>
+              <geom mesh="metacarpal_i_mesh" type="mesh"/>
+              <geom class="wrap_geom" euler="0 90 0" name="hand1_mcp_i_wrap" pos="0 0 0.0395" rgba="0 0 1 0.2" size="0.0075 0.008" type="cylinder"/>
+              <site name="hand1_opponens_pollicis_insertion_i" pos="0.0055 0.0048 0.009875" rgba="0.5 0.5 0.5 1" size="0.0015"/>
+              <site name="hand1_opponens_pollicis_insertion_ii" pos="0.00543 0.005595 0.03379" rgba="0.5 0.5 0.5 1" size="0.0015"/>
+              <site name="hand1_abductor_pollicis_longus_insertion" pos="0.0055 0.0 0.008" rgba="0.5 0.5 0.5 1" size="0.0015"/>
+              <site name="hand1_mcp_i_wrap_dorsal_sidesite" pos="0 0.0085 0.0395" rgba="1 1 1 0.2" size="0.001"/>
+              <site name="hand1_mcp_i_wrap_palmar_sidesite" pos="0 -0.0085 0.0395" rgba="1 1 1 0.2" size="0.001"/>
+              <site name="hand1_metacarpal_i_flexor_tunnel" pos="0.0 -0.0065 0.01975" rgba="0.5 0.5 0.5 1" size="0.0015"/>
+              <site name="hand1_metacarpal_i_extensor_tunnel" pos="0.0 0.0065 0.01975" rgba="0.5 0.5 0.5 1" size="0.0015"/>
+              <body euler="0 0 0" name="hand1_proximal_phalanx_i" pos="0 0 0.0395">
+                <joint axis="1 0 0" limited="true" range="-10 90" type="hinge"/>
+                <geom mesh="proximal_phalanx_i_mesh" rgba="1 0 0 1" type="mesh"/>
+                <geom class="wrap_geom" euler="0 90 0" name="hand1_ip_i_wrap" pos="0 0 0.0335" rgba="0 0 1 0.2" size="0.0065 0.008" type="cylinder"/>
+                <site name="hand1_pp_i_flexor_tunnel" pos="0.0 -0.005 0.01675" rgba="1 0 0 1" size="0.0015"/>
+                <site name="hand1_pp_i_extensor_tunnel" pos="0.0 0.005 0.01675" rgba="1 0 0 1" size="0.0015"/>
+                <site name="hand1_abductor_pollicis_brevis_insertion" pos="0.008 0.0 0.0" rgba="0.5 0.5 0.5 1" size="0.0015"/>
+                <site name="hand1_adductor_pollicis_insertion" pos="-0.008 0.0 0.0" rgba="0.5 0.5 0.5 1" size="0.0015"/>
+                <site name="hand1_flexor_pollicis_brevis_insertion" pos="0.005 -0.005598 0.008406" rgba="0.5 0.5 0.5 1" size="0.0015"/>
+                <site name="hand1_extensor_pollicis_brevis_insertion" pos="0.0 0.006 0.008" rgba="0.1 0.1 0.1 1" size="0.002"/>
+                <site name="hand1_ip_i_wrap_dorsal_sidesite" pos="0 0.0075 0.0335" rgba="1 1 1 0.2" size="0.001"/>
+                <site name="hand1_ip_i_wrap_palmar_sidesite" pos="0 -0.0075 0.0335" rgba="1 1 1 0.2" size="0.001"/>
+                <body euler="0 0 0" name="hand1_distal_phalanx_i" pos="0 0 0.0335">
+                  <joint axis="1 0 0" limited="true" range="-60 90" type="hinge"/>
+                  <geom mesh="distal_phalanx_i_mesh" rgba="0 0 1 1" type="mesh"/>
+                  <site name="hand1_flexor_pollicis_longus_insertion" pos="0.0 -0.005 0.007" rgba="0.1 0.1 0.1 1" size="0.002"/>
+                  <site name="hand1_extensor_pollicis_longus_insertion" pos="0.0 0.005 0.007" rgba="0.1 0.1 0.1 1" size="0.002"/>
+                  <site name="hand1_fingertip_i" pos="0 0 0.027" rgba="1 0 0 0.5" size="0.004"/>
+                </body>
+              </body>
+            </body>
+          </body>
+        </body>
+        <body euler="0 -12 5" name="hand1_proximal_cm_v_centre_block" pos="-0.0171 0.0038 0.0360">
+          <joint axis="1 0 0" limited="true" range="-10 10" type="hinge"/>
+          <geom mesh="proximal_cm_v_centre_block_mesh" type="mesh"/>
+          <geom class="wrap_geom" euler="0 90 0" name="hand1_proximal_cm_v_wrap" rgba="0 0 1 0.2" size="0.007 0.009" type="cylinder"/>
+          <site name="hand1_proximal_cm_v_wrap_dorsal_sidesite" pos="0 0.008 0" rgba="1 1 1 0.2" size="0.001"/>
+          <site name="hand1_proximal_cm_v_wrap_palmar_sidesite" pos="0 -0.008 0" rgba="1 1 1 0.2" size="0.001"/>
+          <body euler="0 0 0" name="hand1_distal_cm_v_centre_block" pos="0 0 0.014">
+            <joint axis="0 0 1" limited="true" range="-10 10" type="hinge"/>
+            <geom mesh="distal_cm_v_centre_block_mesh" type="mesh"/>
+            <geom class="wrap_geom" euler="90 0 0" name="hand1_distal_cm_v_wrap" rgba="0 0 1 0.2" size="0.007 0.008" type="cylinder"/>
+            <site name="hand1_distal_cm_v_wrap_ulnar_sidesite" pos="-0.008 0 0" rgba="1 1 1 0.2" size="0.001"/>
+            <site name="hand1_distal_cm_v_wrap_radial_sidesite" pos="0.008 0 0" rgba="1 1 1 0.2" size="0.001"/>
+            <site name="hand1_opponens_digiti_minimi_distal_cm_v_tunnel" pos="-0.0065 -0.0035 -0.007" rgba="0.5 0.5 0.5 1" size="0.0015"/>
+            <body euler="0 0 0" name="hand1_metacarpal_v" pos="0 0 0.0349">
+              <joint axis="0 1 0" limited="true" pos="0 0 -0.0349" range="-5 5" type="hinge"/>
+              <geom mesh="metacarpal_v_mesh" type="mesh"/>
+              <geom class="wrap_geom" euler="0 90 0" name="hand1_mcp_v_wrap" pos="0 0 0" rgba="0 0 1 0.2" size="0.0095 0.0095" type="cylinder"/>
+              <site name="hand1_flexor_v_metacarpals_tunnel" pos="0 -0.009 -0.0349" rgba="0.5 0.5 0.5 1" size="0.0015"/>
+              <site name="hand1_extensor_communis_v_metacarpals_tunnel" pos="0 0.009 -0.0349" rgba="0.5 0.5 0.5 1" size="0.0015"/>
+              <site name="hand1_dorsal_interossei_v_metacarpals_tunnel" pos="0.006 -0.006 -0.013" rgba="0.5 0.5 0.5 1" size="0.0015"/>
+              <site name="hand1_palmar_interossei_v_metacarpals_tunnel" pos="-0.006 -0.006 -0.013" rgba="0.5 0.5 0.5 1" size="0.0015"/>
+              <site name="hand1_opponens_digiti_minimi_insertion_i" pos="-0.0065 0.005506 -0.026175" rgba="0.5 0.5 0.5 1" size="0.0015"/>
+              <site name="hand1_opponens_digiti_minimi_insertion_ii" pos="-0.0045 0.00665 -0.013738" rgba="0.5 0.5 0.5 1" size="0.0015"/>
+              <body euler="0 0 0" name="hand1_mcp_centre_block_v" pos="0 0 0">
+                <joint axis="0 1 0" limited="true" range="-20 20" type="hinge"/>
+                <geom mesh="mcp_centre_block_v_mesh" type="mesh"/>
+                <site name="hand1_mcp_v_wrap_dorsal_sidesite" pos="0 0.0105 0" rgba="1 1 1 0.2" size="0.001"/>
+                <site name="hand1_mcp_v_wrap_palmar_sidesite" pos="0 -0.0105 0" rgba="1 1 1 0.2" size="0.001"/>
+                <body euler="0 0 0" name="hand1_proximal_phalanx_v" pos="0 0 0">
+                  <joint axis="1 0 0" limited="true" range="-45 90" type="hinge"/>
+                  <geom mesh="proximal_phalanx_v_mesh" rgba="1 0 0 1" type="mesh"/>
+                  <site name="hand1_dorsal_interossei_v_insertion" pos="0.0055 -0.006 0.012" rgba="0.1 0.1 0.1 1" size="0.002"/>
+                  <site name="hand1_palmar_interossei_v_insertion" pos="-0.0055 -0.006 0.012" rgba="0.1 0.1 0.1 1" size="0.002"/>
+                  <site name="hand1_pp_v_extensor_communis_insertion" pos="0 0.0075 0.012" rgba="0.1 0.1 0.1 1" size="0.002"/>
+                  <site name="hand1_pp_v_dorsal_interossei_tunnel" pos="0.006 0.0054 0.0356" rgba="1 0 0 1" size="0.0015"/>
+                  <site name="hand1_pp_v_palmar_interossei_tunnel" pos="-0.006 0.0054 0.0356" rgba="1 0 0 1" size="0.0015"/>
+                  <site name="hand1_pp_v_flexor_tunnel" pos="0 -0.0051 0.0208" rgba="1 0 0 1" size="0.0015"/>
+                  <site name="hand1_pip_v_wrap_dorsal_sidesite" pos="0 0.007 0.0416" rgba="1 1 1 0.2" size="0.001"/>
+                  <site name="hand1_pip_v_wrap_palmar_sidesite" pos="0 -0.007 0.0416" rgba="1 1 1 0.2" size="0.001"/>
+                  <body euler="0 0 0" name="hand1_middle_phalanx_v" pos="0 0 0.0416">
+                    <joint axis="1 0 0" limited="true" range="-10 90" type="hinge"/>
+                    <geom mesh="middle_phalanx_v_mesh" rgba="0 1 0 1" type="mesh"/>
+                    <geom class="wrap_geom" euler="0 90 0" name="hand1_pip_v_wrap" rgba="0 0 1 0.2" size="0.006 0.006" type="cylinder"/>
+                    <site name="hand1_flexor_superficialis_v_insertion" pos="0 -0.005 0.007" rgba="0.1 0.1 0.1 1" size="0.002"/>
+                    <site name="hand1_mp_v_extensor_hood_insertion" pos="0 0.005 0.007" rgba="0.1 0.1 0.1 1" size="0.002"/>
+                    <site name="hand1_mp_v_flexor_tunnel" pos="0 -0.00375 0.0141142857143" rgba="0 1 0 1" size="0.0015"/>
+                    <site name="hand1_dip_v_wrap_dorsal_sidesite" pos="0 0.006 0.0247" rgba="1 1 1 0.2" size="0.001"/>
+                    <site name="hand1_dip_v_wrap_palmar_sidesite" pos="0 -0.006 0.0247" rgba="1 1 1 0.2" size="0.001"/>
+                    <body euler="0 0 0" name="hand1_distal_phalanx_v" pos="0 0 0.0247">
+                      <joint axis="1 0 0" limited="true" range="-10 90" type="hinge"/>
+                      <geom mesh="distal_phalanx_v_mesh" rgba="0 0 1 1" type="mesh"/>
+                      <geom class="wrap_geom" euler="0 90 0" name="hand1_dip_v_wrap" rgba="0 0 1 0.2" size="0.005 0.005" type="cylinder"/>
+                      <site name="hand1_flexor_profundus_v_insertion" pos="0 -0.004 0.007" rgba="0.1 0.1 0.1 1" size="0.002"/>
+                      <site name="hand1_dp_v_extensor_hood_insertion" pos="0 0.004 0.007" rgba="0.1 0.1 0.1 1" size="0.002"/>
+                      <site name="hand1_fingertip_v" pos="0 0 0.0185" rgba="1 0 0 0.5" size="0.004"/>
+                    </body>
+                  </body>
+                </body>
+              </body>
+            </body>
+          </body>
+        </body>
       </body>
     </body>
   </body>
diff --git a/mujoco_env/darm_hand.xml.xacro b/mujoco_env/darm_hand.xml.xacro
index 3c8c215..3e4eb24 100644
--- a/mujoco_env/darm_hand.xml.xacro
+++ b/mujoco_env/darm_hand.xml.xacro
@@ -249,9 +249,13 @@
     <!-- DARM Hand -->
     <xacro:macro name="darm_hand" params="hand_name pos">
         <!-- NOTE: Only the required mocap bodies should be exposed -->
-        <xacro:fingertip_target hand_name="${hand_name}" name="fingertip_ii_target" rgba="0 1 0 1" />
         <xacro:unless value="$(arg single_finger)">
             <xacro:fingertip_target hand_name="${hand_name}" name="fingertip_i_target" rgba="1 0 0 1" />
+        </xacro:unless> 
+
+        <xacro:fingertip_target hand_name="${hand_name}" name="fingertip_ii_target" rgba="0 1 0 1" />
+        
+        <xacro:unless value="$(arg single_finger)">
             <xacro:fingertip_target hand_name="${hand_name}" name="fingertip_iii_target" rgba="0 0 1 1" />
             <xacro:fingertip_target hand_name="${hand_name}" name="fingertip_iv_target" rgba="1 1 1 1" />
             <xacro:fingertip_target hand_name="${hand_name}" name="fingertip_v_target" rgba="0.7 0.3 0 1" />
diff --git a/mujoco_env/tendon.xml b/mujoco_env/tendon.xml
index 019759c..2673af9 100644
--- a/mujoco_env/tendon.xml
+++ b/mujoco_env/tendon.xml
@@ -106,6 +106,396 @@
     <geom geom="hand1_pip_ii_wrap" sidesite="hand1_pip_ii_wrap_palmar_sidesite"/>
     <site site="hand1_flexor_superficialis_ii_insertion"/>
   </spatial>
+  <spatial name="hand1_dorsal_interossei_iii" width="0.0004">
+    <site site="hand1_dorsal_interossei_iii_origin"/>
+    <geom geom="hand1_rc_joint_wrap" sidesite="hand1_rc_joint_wrap_palmar_sidesite"/>
+    <site site="hand1_flexor_iii_carpals_tunnel"/>
+    <site site="hand1_flexor_iii_metacarpals_tunnel"/>
+    <site site="hand1_dorsal_interossei_iii_metacarpals_tunnel"/>
+    <pulley divisor="3"/>
+    <site site="hand1_dorsal_interossei_iii_metacarpals_tunnel"/>
+    <geom geom="hand1_mcp_iii_wrap" sidesite="hand1_mcp_iii_wrap_palmar_sidesite"/>
+    <site site="hand1_dorsal_interossei_iii_insertion"/>
+    <pulley divisor="3"/>
+    <site site="hand1_dorsal_interossei_iii_metacarpals_tunnel"/>
+    <geom geom="hand1_mcp_iii_wrap" sidesite="hand1_mcp_iii_wrap_palmar_sidesite"/>
+    <site site="hand1_dorsal_interossei_iii_insertion"/>
+    <site site="hand1_pp_iii_dorsal_interossei_tunnel"/>
+    <geom geom="hand1_pip_iii_wrap" sidesite="hand1_pip_iii_wrap_dorsal_sidesite"/>
+    <site site="hand1_mp_iii_extensor_hood_insertion"/>
+    <pulley divisor="3"/>
+    <site site="hand1_dorsal_interossei_iii_metacarpals_tunnel"/>
+    <geom geom="hand1_mcp_iii_wrap" sidesite="hand1_mcp_iii_wrap_palmar_sidesite"/>
+    <site site="hand1_dorsal_interossei_iii_insertion"/>
+    <site site="hand1_pp_iii_dorsal_interossei_tunnel"/>
+    <geom geom="hand1_pip_iii_wrap" sidesite="hand1_pip_iii_wrap_dorsal_sidesite"/>
+    <site site="hand1_mp_iii_extensor_hood_insertion"/>
+    <geom geom="hand1_dip_iii_wrap" sidesite="hand1_dip_iii_wrap_dorsal_sidesite"/>
+    <site site="hand1_dp_iii_extensor_hood_insertion"/>
+  </spatial>
+  <spatial name="hand1_palmar_interossei_iii" width="0.0004">
+    <site site="hand1_palmar_interossei_iii_origin"/>
+    <geom geom="hand1_rc_joint_wrap" sidesite="hand1_rc_joint_wrap_palmar_sidesite"/>
+    <site site="hand1_flexor_iii_carpals_tunnel"/>
+    <site site="hand1_flexor_iii_metacarpals_tunnel"/>
+    <site site="hand1_palmar_interossei_iii_metacarpals_tunnel"/>
+    <pulley divisor="3"/>
+    <site site="hand1_palmar_interossei_iii_metacarpals_tunnel"/>
+    <geom geom="hand1_mcp_iii_wrap" sidesite="hand1_mcp_iii_wrap_palmar_sidesite"/>
+    <site site="hand1_palmar_interossei_iii_insertion"/>
+    <pulley divisor="3"/>
+    <site site="hand1_palmar_interossei_iii_metacarpals_tunnel"/>
+    <geom geom="hand1_mcp_iii_wrap" sidesite="hand1_mcp_iii_wrap_palmar_sidesite"/>
+    <site site="hand1_palmar_interossei_iii_insertion"/>
+    <site site="hand1_pp_iii_palmar_interossei_tunnel"/>
+    <geom geom="hand1_pip_iii_wrap" sidesite="hand1_pip_iii_wrap_dorsal_sidesite"/>
+    <site site="hand1_mp_iii_extensor_hood_insertion"/>
+    <pulley divisor="3"/>
+    <site site="hand1_palmar_interossei_iii_metacarpals_tunnel"/>
+    <geom geom="hand1_mcp_iii_wrap" sidesite="hand1_mcp_iii_wrap_palmar_sidesite"/>
+    <site site="hand1_palmar_interossei_iii_insertion"/>
+    <site site="hand1_pp_iii_palmar_interossei_tunnel"/>
+    <geom geom="hand1_pip_iii_wrap" sidesite="hand1_pip_iii_wrap_dorsal_sidesite"/>
+    <site site="hand1_mp_iii_extensor_hood_insertion"/>
+    <geom geom="hand1_dip_iii_wrap" sidesite="hand1_dip_iii_wrap_dorsal_sidesite"/>
+    <site site="hand1_dp_iii_extensor_hood_insertion"/>
+  </spatial>
+  <spatial name="hand1_extensor_communis_iii" width="0.0004">
+    <site site="hand1_extensor_communis_iii_origin"/>
+    <geom geom="hand1_rc_joint_wrap" sidesite="hand1_rc_joint_wrap_dorsal_sidesite"/>
+    <site site="hand1_extensor_communis_iii_carpals_tunnel"/>
+    <site site="hand1_extensor_communis_iii_metacarpals_tunnel"/>
+    <pulley divisor="3"/>
+    <site site="hand1_extensor_communis_iii_metacarpals_tunnel"/>
+    <geom geom="hand1_mcp_iii_wrap" sidesite="hand1_mcp_iii_wrap_dorsal_sidesite"/>
+    <site site="hand1_pp_iii_extensor_communis_insertion"/>
+    <pulley divisor="3"/>
+    <site site="hand1_extensor_communis_iii_metacarpals_tunnel"/>
+    <geom geom="hand1_mcp_iii_wrap" sidesite="hand1_mcp_iii_wrap_dorsal_sidesite"/>
+    <site site="hand1_pp_iii_extensor_communis_insertion"/>
+    <geom geom="hand1_pip_iii_wrap" sidesite="hand1_pip_iii_wrap_dorsal_sidesite"/>
+    <site site="hand1_mp_iii_extensor_hood_insertion"/>
+    <pulley divisor="3"/>
+    <site site="hand1_extensor_communis_iii_metacarpals_tunnel"/>
+    <geom geom="hand1_mcp_iii_wrap" sidesite="hand1_mcp_iii_wrap_dorsal_sidesite"/>
+    <site site="hand1_pp_iii_extensor_communis_insertion"/>
+    <geom geom="hand1_pip_iii_wrap" sidesite="hand1_pip_iii_wrap_dorsal_sidesite"/>
+    <site site="hand1_mp_iii_extensor_hood_insertion"/>
+    <geom geom="hand1_dip_iii_wrap" sidesite="hand1_dip_iii_wrap_dorsal_sidesite"/>
+    <site site="hand1_dp_iii_extensor_hood_insertion"/>
+  </spatial>
+  <spatial name="hand1_flexor_profundus_iii" width="0.0004">
+    <site site="hand1_flexor_profundus_iii_origin"/>
+    <geom geom="hand1_rc_joint_wrap" sidesite="hand1_rc_joint_wrap_palmar_sidesite"/>
+    <site site="hand1_flexor_iii_carpals_tunnel"/>
+    <site site="hand1_flexor_iii_metacarpals_tunnel"/>
+    <geom geom="hand1_mcp_iii_wrap" sidesite="hand1_mcp_iii_wrap_palmar_sidesite"/>
+    <site site="hand1_pp_iii_flexor_tunnel"/>
+    <geom geom="hand1_pip_iii_wrap" sidesite="hand1_pip_iii_wrap_palmar_sidesite"/>
+    <!-- <site site="${hand_name}_flexor_superficialis_${index}_insertion" /> -->
+    <site site="hand1_mp_iii_flexor_tunnel"/>
+    <geom geom="hand1_dip_iii_wrap" sidesite="hand1_dip_iii_wrap_palmar_sidesite"/>
+    <site site="hand1_flexor_profundus_iii_insertion"/>
+  </spatial>
+  <spatial name="hand1_flexor_superficialis_iii" width="0.0004">
+    <site site="hand1_flexor_superficialis_iii_origin"/>
+    <geom geom="hand1_rc_joint_wrap" sidesite="hand1_rc_joint_wrap_palmar_sidesite"/>
+    <site site="hand1_flexor_iii_carpals_tunnel"/>
+    <site site="hand1_flexor_iii_metacarpals_tunnel"/>
+    <geom geom="hand1_mcp_iii_wrap" sidesite="hand1_mcp_iii_wrap_palmar_sidesite"/>
+    <site site="hand1_pp_iii_flexor_tunnel"/>
+    <geom geom="hand1_pip_iii_wrap" sidesite="hand1_pip_iii_wrap_palmar_sidesite"/>
+    <site site="hand1_flexor_superficialis_iii_insertion"/>
+  </spatial>
+  <spatial name="hand1_dorsal_interossei_iv" width="0.0004">
+    <site site="hand1_dorsal_interossei_iv_origin"/>
+    <geom geom="hand1_rc_joint_wrap" sidesite="hand1_rc_joint_wrap_palmar_sidesite"/>
+    <site site="hand1_flexor_iv_carpals_tunnel"/>
+    <site site="hand1_flexor_iv_metacarpals_tunnel"/>
+    <site site="hand1_dorsal_interossei_iv_metacarpals_tunnel"/>
+    <pulley divisor="3"/>
+    <site site="hand1_dorsal_interossei_iv_metacarpals_tunnel"/>
+    <geom geom="hand1_mcp_iv_wrap" sidesite="hand1_mcp_iv_wrap_palmar_sidesite"/>
+    <site site="hand1_dorsal_interossei_iv_insertion"/>
+    <pulley divisor="3"/>
+    <site site="hand1_dorsal_interossei_iv_metacarpals_tunnel"/>
+    <geom geom="hand1_mcp_iv_wrap" sidesite="hand1_mcp_iv_wrap_palmar_sidesite"/>
+    <site site="hand1_dorsal_interossei_iv_insertion"/>
+    <site site="hand1_pp_iv_dorsal_interossei_tunnel"/>
+    <geom geom="hand1_pip_iv_wrap" sidesite="hand1_pip_iv_wrap_dorsal_sidesite"/>
+    <site site="hand1_mp_iv_extensor_hood_insertion"/>
+    <pulley divisor="3"/>
+    <site site="hand1_dorsal_interossei_iv_metacarpals_tunnel"/>
+    <geom geom="hand1_mcp_iv_wrap" sidesite="hand1_mcp_iv_wrap_palmar_sidesite"/>
+    <site site="hand1_dorsal_interossei_iv_insertion"/>
+    <site site="hand1_pp_iv_dorsal_interossei_tunnel"/>
+    <geom geom="hand1_pip_iv_wrap" sidesite="hand1_pip_iv_wrap_dorsal_sidesite"/>
+    <site site="hand1_mp_iv_extensor_hood_insertion"/>
+    <geom geom="hand1_dip_iv_wrap" sidesite="hand1_dip_iv_wrap_dorsal_sidesite"/>
+    <site site="hand1_dp_iv_extensor_hood_insertion"/>
+  </spatial>
+  <spatial name="hand1_palmar_interossei_iv" width="0.0004">
+    <site site="hand1_palmar_interossei_iv_origin"/>
+    <geom geom="hand1_rc_joint_wrap" sidesite="hand1_rc_joint_wrap_palmar_sidesite"/>
+    <site site="hand1_flexor_iv_carpals_tunnel"/>
+    <site site="hand1_flexor_iv_metacarpals_tunnel"/>
+    <site site="hand1_palmar_interossei_iv_metacarpals_tunnel"/>
+    <pulley divisor="3"/>
+    <site site="hand1_palmar_interossei_iv_metacarpals_tunnel"/>
+    <geom geom="hand1_mcp_iv_wrap" sidesite="hand1_mcp_iv_wrap_palmar_sidesite"/>
+    <site site="hand1_palmar_interossei_iv_insertion"/>
+    <pulley divisor="3"/>
+    <site site="hand1_palmar_interossei_iv_metacarpals_tunnel"/>
+    <geom geom="hand1_mcp_iv_wrap" sidesite="hand1_mcp_iv_wrap_palmar_sidesite"/>
+    <site site="hand1_palmar_interossei_iv_insertion"/>
+    <site site="hand1_pp_iv_palmar_interossei_tunnel"/>
+    <geom geom="hand1_pip_iv_wrap" sidesite="hand1_pip_iv_wrap_dorsal_sidesite"/>
+    <site site="hand1_mp_iv_extensor_hood_insertion"/>
+    <pulley divisor="3"/>
+    <site site="hand1_palmar_interossei_iv_metacarpals_tunnel"/>
+    <geom geom="hand1_mcp_iv_wrap" sidesite="hand1_mcp_iv_wrap_palmar_sidesite"/>
+    <site site="hand1_palmar_interossei_iv_insertion"/>
+    <site site="hand1_pp_iv_palmar_interossei_tunnel"/>
+    <geom geom="hand1_pip_iv_wrap" sidesite="hand1_pip_iv_wrap_dorsal_sidesite"/>
+    <site site="hand1_mp_iv_extensor_hood_insertion"/>
+    <geom geom="hand1_dip_iv_wrap" sidesite="hand1_dip_iv_wrap_dorsal_sidesite"/>
+    <site site="hand1_dp_iv_extensor_hood_insertion"/>
+  </spatial>
+  <spatial name="hand1_extensor_communis_iv" width="0.0004">
+    <site site="hand1_extensor_communis_iv_origin"/>
+    <geom geom="hand1_rc_joint_wrap" sidesite="hand1_rc_joint_wrap_dorsal_sidesite"/>
+    <site site="hand1_extensor_communis_iv_carpals_tunnel"/>
+    <site site="hand1_extensor_communis_iv_metacarpals_tunnel"/>
+    <pulley divisor="3"/>
+    <site site="hand1_extensor_communis_iv_metacarpals_tunnel"/>
+    <geom geom="hand1_mcp_iv_wrap" sidesite="hand1_mcp_iv_wrap_dorsal_sidesite"/>
+    <site site="hand1_pp_iv_extensor_communis_insertion"/>
+    <pulley divisor="3"/>
+    <site site="hand1_extensor_communis_iv_metacarpals_tunnel"/>
+    <geom geom="hand1_mcp_iv_wrap" sidesite="hand1_mcp_iv_wrap_dorsal_sidesite"/>
+    <site site="hand1_pp_iv_extensor_communis_insertion"/>
+    <geom geom="hand1_pip_iv_wrap" sidesite="hand1_pip_iv_wrap_dorsal_sidesite"/>
+    <site site="hand1_mp_iv_extensor_hood_insertion"/>
+    <pulley divisor="3"/>
+    <site site="hand1_extensor_communis_iv_metacarpals_tunnel"/>
+    <geom geom="hand1_mcp_iv_wrap" sidesite="hand1_mcp_iv_wrap_dorsal_sidesite"/>
+    <site site="hand1_pp_iv_extensor_communis_insertion"/>
+    <geom geom="hand1_pip_iv_wrap" sidesite="hand1_pip_iv_wrap_dorsal_sidesite"/>
+    <site site="hand1_mp_iv_extensor_hood_insertion"/>
+    <geom geom="hand1_dip_iv_wrap" sidesite="hand1_dip_iv_wrap_dorsal_sidesite"/>
+    <site site="hand1_dp_iv_extensor_hood_insertion"/>
+  </spatial>
+  <spatial name="hand1_flexor_profundus_iv" width="0.0004">
+    <site site="hand1_flexor_profundus_iv_origin"/>
+    <geom geom="hand1_rc_joint_wrap" sidesite="hand1_rc_joint_wrap_palmar_sidesite"/>
+    <site site="hand1_flexor_iv_carpals_tunnel"/>
+    <site site="hand1_flexor_iv_metacarpals_tunnel"/>
+    <geom geom="hand1_mcp_iv_wrap" sidesite="hand1_mcp_iv_wrap_palmar_sidesite"/>
+    <site site="hand1_pp_iv_flexor_tunnel"/>
+    <geom geom="hand1_pip_iv_wrap" sidesite="hand1_pip_iv_wrap_palmar_sidesite"/>
+    <!-- <site site="${hand_name}_flexor_superficialis_${index}_insertion" /> -->
+    <site site="hand1_mp_iv_flexor_tunnel"/>
+    <geom geom="hand1_dip_iv_wrap" sidesite="hand1_dip_iv_wrap_palmar_sidesite"/>
+    <site site="hand1_flexor_profundus_iv_insertion"/>
+  </spatial>
+  <spatial name="hand1_flexor_superficialis_iv" width="0.0004">
+    <site site="hand1_flexor_superficialis_iv_origin"/>
+    <geom geom="hand1_rc_joint_wrap" sidesite="hand1_rc_joint_wrap_palmar_sidesite"/>
+    <site site="hand1_flexor_iv_carpals_tunnel"/>
+    <site site="hand1_flexor_iv_metacarpals_tunnel"/>
+    <geom geom="hand1_mcp_iv_wrap" sidesite="hand1_mcp_iv_wrap_palmar_sidesite"/>
+    <site site="hand1_pp_iv_flexor_tunnel"/>
+    <geom geom="hand1_pip_iv_wrap" sidesite="hand1_pip_iv_wrap_palmar_sidesite"/>
+    <site site="hand1_flexor_superficialis_iv_insertion"/>
+  </spatial>
+  <spatial name="hand1_dorsal_interossei_v" width="0.0004">
+    <site site="hand1_dorsal_interossei_v_origin"/>
+    <geom geom="hand1_rc_joint_wrap" sidesite="hand1_rc_joint_wrap_palmar_sidesite"/>
+    <site site="hand1_flexor_v_carpals_tunnel"/>
+    <site site="hand1_flexor_v_metacarpals_tunnel"/>
+    <site site="hand1_dorsal_interossei_v_metacarpals_tunnel"/>
+    <pulley divisor="3"/>
+    <site site="hand1_dorsal_interossei_v_metacarpals_tunnel"/>
+    <geom geom="hand1_mcp_v_wrap" sidesite="hand1_mcp_v_wrap_palmar_sidesite"/>
+    <site site="hand1_dorsal_interossei_v_insertion"/>
+    <pulley divisor="3"/>
+    <site site="hand1_dorsal_interossei_v_metacarpals_tunnel"/>
+    <geom geom="hand1_mcp_v_wrap" sidesite="hand1_mcp_v_wrap_palmar_sidesite"/>
+    <site site="hand1_dorsal_interossei_v_insertion"/>
+    <site site="hand1_pp_v_dorsal_interossei_tunnel"/>
+    <geom geom="hand1_pip_v_wrap" sidesite="hand1_pip_v_wrap_dorsal_sidesite"/>
+    <site site="hand1_mp_v_extensor_hood_insertion"/>
+    <pulley divisor="3"/>
+    <site site="hand1_dorsal_interossei_v_metacarpals_tunnel"/>
+    <geom geom="hand1_mcp_v_wrap" sidesite="hand1_mcp_v_wrap_palmar_sidesite"/>
+    <site site="hand1_dorsal_interossei_v_insertion"/>
+    <site site="hand1_pp_v_dorsal_interossei_tunnel"/>
+    <geom geom="hand1_pip_v_wrap" sidesite="hand1_pip_v_wrap_dorsal_sidesite"/>
+    <site site="hand1_mp_v_extensor_hood_insertion"/>
+    <geom geom="hand1_dip_v_wrap" sidesite="hand1_dip_v_wrap_dorsal_sidesite"/>
+    <site site="hand1_dp_v_extensor_hood_insertion"/>
+  </spatial>
+  <spatial name="hand1_palmar_interossei_v" width="0.0004">
+    <site site="hand1_palmar_interossei_v_origin"/>
+    <geom geom="hand1_rc_joint_wrap" sidesite="hand1_rc_joint_wrap_palmar_sidesite"/>
+    <site site="hand1_flexor_v_carpals_tunnel"/>
+    <site site="hand1_flexor_v_metacarpals_tunnel"/>
+    <site site="hand1_palmar_interossei_v_metacarpals_tunnel"/>
+    <pulley divisor="3"/>
+    <site site="hand1_palmar_interossei_v_metacarpals_tunnel"/>
+    <geom geom="hand1_mcp_v_wrap" sidesite="hand1_mcp_v_wrap_palmar_sidesite"/>
+    <site site="hand1_palmar_interossei_v_insertion"/>
+    <pulley divisor="3"/>
+    <site site="hand1_palmar_interossei_v_metacarpals_tunnel"/>
+    <geom geom="hand1_mcp_v_wrap" sidesite="hand1_mcp_v_wrap_palmar_sidesite"/>
+    <site site="hand1_palmar_interossei_v_insertion"/>
+    <site site="hand1_pp_v_palmar_interossei_tunnel"/>
+    <geom geom="hand1_pip_v_wrap" sidesite="hand1_pip_v_wrap_dorsal_sidesite"/>
+    <site site="hand1_mp_v_extensor_hood_insertion"/>
+    <pulley divisor="3"/>
+    <site site="hand1_palmar_interossei_v_metacarpals_tunnel"/>
+    <geom geom="hand1_mcp_v_wrap" sidesite="hand1_mcp_v_wrap_palmar_sidesite"/>
+    <site site="hand1_palmar_interossei_v_insertion"/>
+    <site site="hand1_pp_v_palmar_interossei_tunnel"/>
+    <geom geom="hand1_pip_v_wrap" sidesite="hand1_pip_v_wrap_dorsal_sidesite"/>
+    <site site="hand1_mp_v_extensor_hood_insertion"/>
+    <geom geom="hand1_dip_v_wrap" sidesite="hand1_dip_v_wrap_dorsal_sidesite"/>
+    <site site="hand1_dp_v_extensor_hood_insertion"/>
+  </spatial>
+  <spatial name="hand1_extensor_communis_v" width="0.0004">
+    <site site="hand1_extensor_communis_v_origin"/>
+    <geom geom="hand1_rc_joint_wrap" sidesite="hand1_rc_joint_wrap_dorsal_sidesite"/>
+    <site site="hand1_extensor_communis_v_carpals_tunnel"/>
+    <site site="hand1_extensor_communis_v_metacarpals_tunnel"/>
+    <pulley divisor="3"/>
+    <site site="hand1_extensor_communis_v_metacarpals_tunnel"/>
+    <geom geom="hand1_mcp_v_wrap" sidesite="hand1_mcp_v_wrap_dorsal_sidesite"/>
+    <site site="hand1_pp_v_extensor_communis_insertion"/>
+    <pulley divisor="3"/>
+    <site site="hand1_extensor_communis_v_metacarpals_tunnel"/>
+    <geom geom="hand1_mcp_v_wrap" sidesite="hand1_mcp_v_wrap_dorsal_sidesite"/>
+    <site site="hand1_pp_v_extensor_communis_insertion"/>
+    <geom geom="hand1_pip_v_wrap" sidesite="hand1_pip_v_wrap_dorsal_sidesite"/>
+    <site site="hand1_mp_v_extensor_hood_insertion"/>
+    <pulley divisor="3"/>
+    <site site="hand1_extensor_communis_v_metacarpals_tunnel"/>
+    <geom geom="hand1_mcp_v_wrap" sidesite="hand1_mcp_v_wrap_dorsal_sidesite"/>
+    <site site="hand1_pp_v_extensor_communis_insertion"/>
+    <geom geom="hand1_pip_v_wrap" sidesite="hand1_pip_v_wrap_dorsal_sidesite"/>
+    <site site="hand1_mp_v_extensor_hood_insertion"/>
+    <geom geom="hand1_dip_v_wrap" sidesite="hand1_dip_v_wrap_dorsal_sidesite"/>
+    <site site="hand1_dp_v_extensor_hood_insertion"/>
+  </spatial>
+  <spatial name="hand1_flexor_profundus_v" width="0.0004">
+    <site site="hand1_flexor_profundus_v_origin"/>
+    <geom geom="hand1_rc_joint_wrap" sidesite="hand1_rc_joint_wrap_palmar_sidesite"/>
+    <site site="hand1_flexor_v_carpals_tunnel"/>
+    <site site="hand1_flexor_v_metacarpals_tunnel"/>
+    <geom geom="hand1_mcp_v_wrap" sidesite="hand1_mcp_v_wrap_palmar_sidesite"/>
+    <site site="hand1_pp_v_flexor_tunnel"/>
+    <geom geom="hand1_pip_v_wrap" sidesite="hand1_pip_v_wrap_palmar_sidesite"/>
+    <!-- <site site="${hand_name}_flexor_superficialis_${index}_insertion" /> -->
+    <site site="hand1_mp_v_flexor_tunnel"/>
+    <geom geom="hand1_dip_v_wrap" sidesite="hand1_dip_v_wrap_palmar_sidesite"/>
+    <site site="hand1_flexor_profundus_v_insertion"/>
+  </spatial>
+  <spatial name="hand1_flexor_superficialis_v" width="0.0004">
+    <site site="hand1_flexor_superficialis_v_origin"/>
+    <geom geom="hand1_rc_joint_wrap" sidesite="hand1_rc_joint_wrap_palmar_sidesite"/>
+    <site site="hand1_flexor_v_carpals_tunnel"/>
+    <site site="hand1_flexor_v_metacarpals_tunnel"/>
+    <geom geom="hand1_mcp_v_wrap" sidesite="hand1_mcp_v_wrap_palmar_sidesite"/>
+    <site site="hand1_pp_v_flexor_tunnel"/>
+    <geom geom="hand1_pip_v_wrap" sidesite="hand1_pip_v_wrap_palmar_sidesite"/>
+    <site site="hand1_flexor_superficialis_v_insertion"/>
+  </spatial>
+  <!-- Pollicis Muscles -->
+  <spatial name="hand1_abductor_pollicis_brevis" width="0.0004">
+    <site site="hand1_abductor_pollicis_brevis_origin"/>
+    <geom geom="hand1_rc_joint_wrap" sidesite="hand1_rc_joint_wrap_palmar_sidesite"/>
+    <site site="hand1_abductor_pollicis_brevis_carpals_tunnel"/>
+    <geom geom="hand1_distal_cm_i_wrap" sidesite="hand1_distal_cm_i_wrap_dorsal_sidesite"/>
+    <site site="hand1_abductor_pollicis_brevis_insertion"/>
+  </spatial>
+  <spatial name="hand1_adductor_pollicis_oblique" width="0.0004">
+    <site site="hand1_adductor_pollicis_origin"/>
+    <geom geom="hand1_rc_joint_wrap" sidesite="hand1_rc_joint_wrap_palmar_sidesite"/>
+    <site site="hand1_adductor_pollicis_capitate_tunnel"/>
+    <geom geom="hand1_distal_cm_i_wrap" sidesite="hand1_proximal_cm_i_wrap_palmar_sidesite"/>
+    <site site="hand1_adductor_pollicis_insertion"/>
+  </spatial>
+  <spatial name="hand1_adductor_pollicis_transverse" width="0.0004">
+    <site site="hand1_adductor_pollicis_origin"/>
+    <geom geom="hand1_rc_joint_wrap" sidesite="hand1_rc_joint_wrap_palmar_sidesite"/>
+    <site site="hand1_adductor_pollicis_capitate_tunnel"/>
+    <site site="hand1_adductor_pollicis_metacarpals_iii_tunnel_i"/>
+    <site site="hand1_adductor_pollicis_metacarpals_iii_tunnel_ii"/>
+    <site site="hand1_adductor_pollicis_insertion"/>
+  </spatial>
+  <spatial name="hand1_flexor_pollicis_brevis" width="0.0004">
+    <site site="hand1_flexor_pollicis_brevis_origin"/>
+    <geom geom="hand1_rc_joint_wrap" sidesite="hand1_rc_joint_wrap_palmar_sidesite"/>
+    <site site="hand1_flexor_pollicis_brevis_carpals_tunnel"/>
+    <geom geom="hand1_distal_cm_i_wrap" sidesite="hand1_distal_cm_i_wrap_dorsal_sidesite"/>
+    <site site="hand1_flexor_pollicis_brevis_insertion"/>
+  </spatial>
+  <spatial name="hand1_opponens_pollicis" width="0.0004">
+    <site site="hand1_opponens_pollicis_origin"/>
+    <geom geom="hand1_rc_joint_wrap" sidesite="hand1_rc_joint_wrap_palmar_sidesite"/>
+    <site site="hand1_opponens_pollicis_carpals_tunnel"/>
+    <geom geom="hand1_distal_cm_i_wrap" sidesite="hand1_distal_cm_i_wrap_dorsal_sidesite"/>
+    <site site="hand1_opponens_pollicis_insertion_i"/>
+    <site site="hand1_opponens_pollicis_insertion_ii"/>
+  </spatial>
+  <spatial name="hand1_flexor_pollicis_longus" width="0.0004">
+    <site site="hand1_flexor_pollicis_longus_origin"/>
+    <geom geom="hand1_rc_joint_wrap" sidesite="hand1_rc_joint_wrap_palmar_sidesite"/>
+    <site site="hand1_flexor_pollicis_brevis_carpals_tunnel"/>
+    <geom geom="hand1_proximal_cm_i_wrap" sidesite="hand1_proximal_cm_i_wrap_palmar_sidesite"/>
+    <site site="hand1_metacarpal_i_flexor_tunnel"/>
+    <geom geom="hand1_mcp_i_wrap" sidesite="hand1_mcp_i_wrap_palmar_sidesite"/>
+    <site site="hand1_pp_i_flexor_tunnel"/>
+    <geom geom="hand1_ip_i_wrap" sidesite="hand1_ip_i_wrap_palmar_sidesite"/>
+    <site site="hand1_flexor_pollicis_longus_insertion"/>
+  </spatial>
+  <spatial name="hand1_extensor_pollicis_brevis" width="0.0004">
+    <site site="hand1_extensor_pollicis_brevis_origin"/>
+    <geom geom="hand1_rc_joint_wrap" sidesite="hand1_rc_joint_wrap_dorsal_sidesite"/>
+    <site site="hand1_extensor_pollicis_carpals_tunnel"/>
+    <geom geom="hand1_proximal_cm_i_wrap" sidesite="hand1_proximal_cm_i_wrap_dorsal_sidesite"/>
+    <site site="hand1_metacarpal_i_extensor_tunnel"/>
+    <geom geom="hand1_mcp_i_wrap" sidesite="hand1_mcp_i_wrap_dorsal_sidesite"/>
+    <site site="hand1_extensor_pollicis_brevis_insertion"/>
+  </spatial>
+  <spatial name="hand1_extensor_pollicis_longus" width="0.0004">
+    <site site="hand1_extensor_pollicis_longus_origin"/>
+    <geom geom="hand1_rc_joint_wrap" sidesite="hand1_rc_joint_wrap_dorsal_sidesite"/>
+    <site site="hand1_extensor_pollicis_carpals_tunnel"/>
+    <geom geom="hand1_proximal_cm_i_wrap" sidesite="hand1_proximal_cm_i_wrap_dorsal_sidesite"/>
+    <site site="hand1_metacarpal_i_extensor_tunnel"/>
+    <geom geom="hand1_mcp_i_wrap" sidesite="hand1_mcp_i_wrap_dorsal_sidesite"/>
+    <site site="hand1_pp_i_extensor_tunnel"/>
+    <geom geom="hand1_ip_i_wrap" sidesite="hand1_ip_i_wrap_dorsal_sidesite"/>
+    <site site="hand1_extensor_pollicis_longus_insertion"/>
+  </spatial>
+  <spatial name="hand1_abductor_pollicis_longus" width="0.0004">
+    <site site="hand1_abductor_pollicis_longus_origin"/>
+    <geom geom="hand1_rc_joint_wrap" sidesite="hand1_rc_joint_wrap_dorsal_sidesite"/>
+    <site site="hand1_extensor_pollicis_carpals_tunnel"/>
+    <geom geom="hand1_distal_cm_i_wrap" sidesite="hand1_distal_cm_i_wrap_dorsal_sidesite"/>
+    <site site="hand1_abductor_pollicis_longus_insertion"/>
+  </spatial>
+  <!-- Digiti Minimi -->
+  <spatial name="hand1_opponens_digiti_minimi" width="0.0004">
+    <site site="hand1_opponens_digiti_minimi_origin"/>
+    <geom geom="hand1_rc_joint_wrap" sidesite="hand1_rc_joint_wrap_palmar_sidesite"/>
+    <site site="hand1_opponens_digiti_minimi_carpals_tunnel"/>
+    <geom geom="hand1_proximal_cm_v_wrap" sidesite="hand1_proximal_cm_v_wrap_palmar_sidesite"/>
+    <site site="hand1_opponens_digiti_minimi_distal_cm_v_tunnel"/>
+    <geom geom="hand1_distal_cm_v_wrap" sidesite="hand1_distal_cm_v_wrap_ulnar_sidesite"/>
+    <site site="hand1_opponens_digiti_minimi_insertion_i"/>
+    <site site="hand1_opponens_digiti_minimi_insertion_ii"/>
+  </spatial>
   <!-- <xacro:hand_tendons hand_name="hand2" />
     <xacro:hand_tendons hand_name="hand3" /> -->
 </tendon>
