{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d4e9afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8a73cad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'/home/daniel/DARM/darm_mujoco'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"DARM_MUJOCO_PATH\"] = \"/home/daniel/DARM/darm_mujoco\"\n",
    "os.getenv('DARM_MUJOCO_PATH')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08c0e0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd ../mujoco_env\n",
    "bash generate_darm_xml.sh false true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fee3abe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from darm_gym_env import DARMEnv\n",
    "from stable_baselines3 import SAC\n",
    "from stable_baselines3.common.vec_env.subproc_vec_env import SubprocVecEnv\n",
    "from stable_baselines3.common.vec_env.vec_monitor import VecMonitor\n",
    "from stable_baselines3.common.vec_env.vec_normalize import VecNormalize\n",
    "from stable_baselines3.common.vec_env.dummy_vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "import wandb\n",
    "from wandb.integration.sb3 import WandbCallback\n",
    "from stable_baselines3.common.callbacks import CallbackList, EvalCallback, StopTrainingOnRewardThreshold, StopTrainingOnNoModelImprovement\n",
    "\n",
    "\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f78d0f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_name = \"test1_MF_SB3_SAC\"\n",
    "\n",
    "config = {\n",
    "    \"env_id\": \"darm/DarmHand-v0\", # changed from darm/DarmHand-v0\n",
    "    \"single_finger_env\": False,\n",
    "    \"algo\": \"SAC\",\n",
    "    \"rl_lib\": \"SB3\",\n",
    "    \n",
    "    \"seed\": 0,\n",
    "    \"mean_reward_thresh\": 1_300,\n",
    "    \"total_timesteps\": 10_000_000,\n",
    "    \"pi_net_arch\": [128, 256, 256, 128],\n",
    "    \"qf_net_arch\": [128, 256, 256, 128],\n",
    "    \"learning_starts\": 40_000,\n",
    "    \"num_cpu\": 6,\n",
    "    \n",
    "    \"eval_freq\": 2_000, # 5_000\n",
    "    \"max_no_improvement_evals\": 10,\n",
    "    \"no_improvement_min_evals\": 20,\n",
    "    \n",
    "    \"log_interval\": 20, # episodes\n",
    "    \"wandb_model_save_freq\": 2_000, #5_000 timesteps?\n",
    "    \n",
    "    \"run_local_dir\": f\"{os.getenv('DARM_MUJOCO_PATH')}/darm_training/results/darm_mf_hand/{run_name}\" \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a11daee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/daniel/DARM/darm_mujoco/darm_training/wandb/run-20230302_150402-6occ3zcz</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/danieladejumo/DARM/runs/6occ3zcz' target=\"_blank\">test1_MF_SB3_SAC</a></strong> to <a href='https://wandb.ai/danieladejumo/DARM' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/danieladejumo/DARM' target=\"_blank\">https://wandb.ai/danieladejumo/DARM</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/danieladejumo/DARM/runs/6occ3zcz' target=\"_blank\">https://wandb.ai/danieladejumo/DARM/runs/6occ3zcz</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "notes = \"\"\"\n",
    "- The environment was updated such that the target is within a range from the start point\n",
    "- Velocity penalty was removed and only effort penalty was used\n",
    "- The reward function was updated according to the reach task reward used in facebookresearch/myosuite [https://github.com/facebookresearch/myosuite/blob/main/myosuite/envs/myo/reach_v0.py]\n",
    "- The done signal is trigerred only when the fingertip goes beyond a threshold. The episode continues to the maximum timestep otherwise.\n",
    "- The friction and damping coefficient of the environment is updated. Values are inspired from Deepmind's Mujoco Menagerie [https://github.com/deepmind/mujoco_menagerie/blob/main/shadow_hand/right_hand.xml]\n",
    "- The range of action from the model was changed to [-1, 1]. This action is mapped to the actual action sent to mujoco e.g [0, 2]]. This change is inspired from values used in OpenAI's Gym Mujoco environments.\n",
    "- max_episode_steps was updated to 200.\n",
    "- Velocity vector (size [3,]) was added to observation. Observation size is now (9,)\n",
    "- Action range was increased to [0, 5]\n",
    "- Observation warpper to scale observation from m and m/s to cm and cm/s was applied\n",
    "- Max Tension for Digitorum Extensor Communis was increased to 10\n",
    "- FIXED: Velocity Observation from (prev_pos - new_pos)/time to (new_pos - prev_pos)/time\n",
    "- FIXED: Removed weight of 1 from 'sparse', 'solved', and 'done' in reward weighting\n",
    "- Reduced max_target_th to 5*0.004, 20 mm\n",
    "\n",
    "- Five-Fingers; No Wrist Environment\n",
    "- This run was trained on vast_ai using SB3's SAC algo.\n",
    "\"\"\"\n",
    "\n",
    "tags = [\"five_fingers\", \"sac\", \"sb3\", \"vast_ai\"]\n",
    "\n",
    "run = wandb.init(\n",
    "    project=\"DARM\",\n",
    "    name=run_name,\n",
    "    tags=tags,\n",
    "    notes=notes,\n",
    "    config=config,\n",
    "    sync_tensorboard=True,  # auto-upload sb3's tensorboard metrics\n",
    "    # monitor_gym=True,  # auto-upload the videos of agents playing the game\n",
    "    save_code=True,  # optional\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c0c5e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym.wrappers import TransformObservation\n",
    "# from gym.wrappers import RescaleAction\n",
    "\n",
    "create_env = lambda: TransformObservation(gym.make(config[\"env_id\"], single_finger_env=config[\"single_finger_env\"]), lambda obs: obs*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "55a1a604",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CPU = config[\"num_cpu\"]\n",
    "\n",
    "env = make_vec_env(create_env, n_envs=NUM_CPU, seed=config[\"seed\"])\n",
    "# env = VecNormalize(env)   #FIXME: Remember to save norm params if using VecNorm env\n",
    "# env = VecMonitor(env)\n",
    "\n",
    "policy_kwargs = dict(net_arch=dict(pi=config[\"pi_net_arch\"], qf=config[\"qf_net_arch\"]))\n",
    "\n",
    "model = SAC(\"MlpPolicy\", env, verbose=1,\n",
    "            learning_starts=config[\"learning_starts\"],\n",
    "            gradient_steps=NUM_CPU, # num of envs\n",
    "            policy_kwargs=policy_kwargs,\n",
    "            tensorboard_log=config['run_local_dir'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e8472162",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<stable_baselines3.common.callbacks.CallbackList at 0x7f218449c7f0>"
     ]
    }
   ],
   "source": [
    "eval_env = make_vec_env(create_env, n_envs=1, seed=config[\"seed\"])\n",
    "\n",
    "# Stop training when the model reaches the reward threshold\n",
    "# reward_thresh_callback = StopTrainingOnRewardThreshold(reward_threshold=config[\"mean_reward_thresh\"], verbose=1)\n",
    "\n",
    "# Stop training if there is no improvement after more than N evaluations\n",
    "# stop_train_callback = StopTrainingOnNoModelImprovement(\n",
    "#     max_no_improvement_evals=config[\"max_no_improvement_evals\"], \n",
    "#     min_evals=config[\"no_improvement_min_evals\"], \n",
    "#     verbose=1)\n",
    "\n",
    "eval_callback = EvalCallback(eval_env, \n",
    "                             best_model_save_path=f\"{config['run_local_dir']}/models/best\",\n",
    "                             log_path=f\"{config['run_local_dir']}/models/best/logs\", \n",
    "                             eval_freq=config[\"eval_freq\"],\n",
    "                             # callback_on_new_best=reward_thresh_callback,\n",
    "                             # callback_after_eval=stop_train_callback,\n",
    "                             deterministic=True, render=False, verbose=1)\n",
    "\n",
    "wandb_callback=WandbCallback(model_save_path=f\"{config['run_local_dir']}/models\",\n",
    "                             model_save_freq=config[\"wandb_model_save_freq\"],\n",
    "                             verbose=2)\n",
    "\n",
    "# Create the callback list\n",
    "callback = CallbackList([wandb_callback, eval_callback])\n",
    "callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2bc47cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    model.learn(total_timesteps=config[\"total_timesteps\"], \n",
    "                log_interval=config[\"log_interval\"], \n",
    "                tb_log_name=run_name,\n",
    "                callback=callback)\n",
    "except Exception as e:\n",
    "    print(\"Exception caught:\")\n",
    "    print(e)\n",
    "finally:\n",
    "    # timestamp = f\"{datetime.now().date()}__{datetime.now().time()}\"\n",
    "    print(\"Saving last checkpoint\")\n",
    "    model_name = f\"{config['run_local_dir']}/models/last_model\"\n",
    "    model.save(model_name)\n",
    "    print(f\"Last checkpoint saved in: {model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ad79ac82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finish the run if it's final\n",
    "run.finish()\n",
    "print(f\"Finished run {run_name}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
